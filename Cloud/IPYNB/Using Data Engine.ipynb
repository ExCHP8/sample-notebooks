{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n",
    "from project_lib import Project\n",
    "project = Project(spark.sparkContext, 'e051c915-6375-45b6-b123-fc23288c03ab', 'p-07df629a784e2029523667fe384c159b3a37b69a')\n",
    "pc = project.project_context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using IBM Data Engine (f.k.a. IBM SQL Query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"pull-left\"><left><img style=\"float: right;\" src=\"https://s3.eu-de.cloud-object-storage.appdomain.cloud/dataengine-downloads/dataengine.svg\" width=\"100\" margin=50></left></div>\n",
    "<div style=\"text-align:left\">\n",
    "IBM Data Engine is IBM's core service for serverless data lakes. It provides both, data lake processing and data lake metadata management. \n",
    "<h4>1. Processing</h4>\n",
    "It allows to ingest, prepare, index, curate and query data on Cloud Object Storage (COS). It exposes and ANSI SQL interface to work on Parquet, CSV, JSON, ORC and AVRO data sets. You can use it to run your analytic queries, and you can use it to conduct complex transformations and write the result in any desired data format, partitioning and layout. Data Engine also allows to continuously ready data from Kafka topics and write to COS. Data Engine is based on Apache Spark SQL as the scale-out processing engine in the background. You do not have to provision any Apache Spark instance or service. For more background information, check out the Data Engine <a href=\"https://console.bluemix.net/docs/services/sql-query/getting-started.html#getting-started-tutorial\" target=\"_blank\">documentation</a>.\n",
    "As mentioned in that documentation, you can craft and run a direct SQL job using the Data Engine Web UI or using a simple Python client tool (like the IBM Watson Studio Notebook) is sufficient.\n",
    "<h4>2. Meta Data</h4>\n",
    "Data Engine comes with a built-in table catalog that is compatible with Hive Metastore. It can be accessed directly in Data Engine via DDL statements. But it can also be used by other big data runtimes and services, such as your custom deployment of Apache Spark by configuring it to connect to Data Engine as Hive Metastore to use for Spark SQL statements.\n",
    "<br><br></div>\n",
    "<div>\n",
    "This notebook is meant to be a generic starter to use the Data Engine API in order to run SQL statements in a programmatic way. It uses the <a href=\"https://github.com/IBM-Cloud/sql-query-clients/tree/master/Python\" target=\"_blank\">ibmcloudsql</a> Python library for this purpose. In the first section it is demonstrated how to setup the appropriate libraries, how to edit respectively generate SQL statements using the so-called sql-magic module, how to execute the SQL statements and finally how to work with the resulte. \n",
    "The later sections deals with topcis for the more advanced user. \n",
    "For information about using ibmcloudsql Python library, check out the <a href=\"https://ibm-cloud.github.io/sql-query-clients/intro.html#ibmcloudsql\" target=\"_blank\">documentation</a>. \n",
    "</div><br>\n",
    "At the very end the notebook also includes a section that demonstrates the usage of a custom Spark runtime with the Hive Metastore of Data Engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "## Table of contents\n",
    "1. [Setup libraries](#setup)<br>\n",
    "2. [Configure the Data Engine client](#configure)<br>\n",
    "    2.1 [Using the project bucket](#projectbucket)<br>\n",
    "    2.2 [Setting Data Engine parameters](#parameters)<br>\n",
    "3. [Understand the schema of your data](#schema)<br>\n",
    "4. [Create your SQL statement](#sql)<br>\n",
    "5. [Run your SQL statement](#run)<br>\n",
    "    5.1 [Synchronous Execution](#synchronous)<br>\n",
    "    5.2 [Asynchronous Execution](#lowlevel)<br>\n",
    "    5.3 [Synchronous execution with optional result dataframe](#execute_sql)<br>\n",
    "    5.4 [Use Paginated SQL](#paginated)<br>\n",
    "    5.5 [get_job](#get_job)<br>\n",
    "6.  [Running ETL SQLs](#etl)<br>\n",
    "7. [Work with result objects](#results)<br>\n",
    "    7.1 [list_results](#list_results)<br>\n",
    "    7.2 [delete_result](#delete_result)<br>\n",
    "    7.3 [Enforce exact target object name](#rename_result_objects)<br>      \n",
    "8. [Manage SQL jobs](#manage_jobs)<br>\n",
    "    8.1 [Work with Job History](#joblist)<br>\n",
    "    8.2 [Batch automation of SQL jobs](#many_queries)<br>\n",
    "9. [Work with hive tables](#catalog)<br>\n",
    "10. [Custom Spark with Hive Metastore in Data Engine](#hms)<br>\n",
    "    10.1 [Set up Data Engine libraries](#hms_setup)<br>\n",
    "    10.2 [Run Custom Spark job with Data Engine as Table Catalog](#hms_usage)<br>\n",
    "11. [Next steps](#next)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"setup\"></a> 1. Setup libraries\n",
    "[Home](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following sequence of cells at least once in your notebook environment in order to install required packages: \n",
    "    - pyarrow - provides rich, powerful features for working with columnar data\n",
    "    - sqlparse - a non-validating SQL parser \n",
    "    - ibmcloudsql - Data Engine client library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (5.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from pyarrow) (1.20.3)\n",
      "Collecting sqlparse\n",
      "  Downloading sqlparse-0.4.2-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 503 kB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sqlparse\n",
      "Successfully installed sqlparse-0.4.2\n",
      "Found existing installation: autoai-libs 1.13.1\n",
      "Uninstalling autoai-libs-1.13.1:\n",
      "  Successfully uninstalled autoai-libs-1.13.1\n",
      "Found existing installation: tensorflow-text 2.7.3\n",
      "Uninstalling tensorflow-text-2.7.3:\n",
      "  Successfully uninstalled tensorflow-text-2.7.3\n",
      "Found existing installation: numba 0.54.1\n",
      "Uninstalling numba-0.54.1:\n",
      "  Successfully uninstalled numba-0.54.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow \n",
    "!pip install sqlparse\n",
    "!pip uninstall --yes autoai-libs tensorflow-text numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ibmcloudsql\n",
      "  Downloading ibmcloudsql-0.5.10.tar.gz (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 4.0 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sqlparse>=0.4.2\n",
      "  Downloading sqlparse-0.4.3-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting backoff==1.10.0\n",
      "  Downloading backoff-1.10.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from ibmcloudsql) (4.1.1)\n",
      "Collecting deprecated\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: importlib-metadata in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from ibmcloudsql) (4.8.2)\n",
      "Requirement already satisfied: requests>=2.2.0 in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from ibmcloudsql) (2.26.0)\n",
      "Requirement already satisfied: pyarrow in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from ibmcloudsql) (5.0.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from ibmcloudsql) (1.20.3)\n",
      "Collecting pre-commit\n",
      "  Downloading pre_commit-2.20.0-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 67.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ibm-cos-sdk>=2.10.0 in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from ibmcloudsql) (2.11.0)\n",
      "Requirement already satisfied: packaging in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from ibmcloudsql) (21.3)\n",
      "Requirement already satisfied: ibm-cos-sdk-core>=2.10.0 in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from ibmcloudsql) (2.11.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from ibmcloudsql) (2.8.2)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from ibmcloudsql) (1.3.4)\n",
      "Collecting isodate\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 489 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ibm-cos-sdk-s3transfer==2.11.0 in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from ibm-cos-sdk>=2.10.0->ibmcloudsql) (2.11.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from ibm-cos-sdk>=2.10.0->ibmcloudsql) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.26.7 in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from ibm-cos-sdk-core>=2.10.0->ibmcloudsql) (1.26.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from pandas>=1.1.0->ibmcloudsql) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from python-dateutil->ibmcloudsql) (1.15.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from requests>=2.2.0->ibmcloudsql) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from requests>=2.2.0->ibmcloudsql) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from requests>=2.2.0->ibmcloudsql) (2.0.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from deprecated->ibmcloudsql) (1.12.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from importlib-metadata->ibmcloudsql) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from packaging->ibmcloudsql) (3.0.4)\n",
      "Collecting virtualenv>=20.0.8\n",
      "  Downloading virtualenv-20.16.5-py3-none-any.whl (8.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.8 MB 74.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: toml in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from pre-commit->ibmcloudsql) (0.10.2)\n",
      "Collecting nodeenv>=0.11.1\n",
      "  Downloading nodeenv-1.7.0-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from pre-commit->ibmcloudsql) (5.4.1)\n",
      "Collecting identify>=1.0.0\n",
      "  Downloading identify-2.5.5-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 7.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cfgv>=2.0.0\n",
      "  Downloading cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\n",
      "Requirement already satisfied: setuptools in /opt/ibm/conda/miniconda3.9/lib/python3.9/site-packages (from nodeenv>=0.11.1->pre-commit->ibmcloudsql) (58.0.4)\n",
      "Collecting platformdirs<3,>=2.4\n",
      "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
      "Collecting filelock<4,>=3.4.1\n",
      "  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Collecting distlib<1,>=0.3.5\n",
      "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
      "\u001b[K     |████████████████████████████████| 468 kB 48.9 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: ibmcloudsql\n",
      "  Building wheel for ibmcloudsql (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ibmcloudsql: filename=ibmcloudsql-0.5.10-py3-none-any.whl size=59142 sha256=f1988663a36aaa3baa92ec30ff59d5f59f8e843db321ce529052291eb587819d\n",
      "  Stored in directory: /home/spark/shared/.cache/pip/wheels/05/2c/f5/0a787ef991cdd05edc4b66b54b251cd5cc6b19202e83ec197d\n",
      "Successfully built ibmcloudsql\n",
      "Installing collected packages: platformdirs, filelock, distlib, virtualenv, nodeenv, identify, cfgv, sqlparse, pre-commit, isodate, deprecated, backoff, ibmcloudsql\n",
      "Successfully installed backoff-1.10.0 cfgv-3.3.1 deprecated-1.2.13 distlib-0.3.6 filelock-3.8.0 ibmcloudsql-0.5.10 identify-2.5.5 isodate-0.6.1 nodeenv-1.7.0 platformdirs-2.5.2 pre-commit-2.20.0 sqlparse-0.4.3 virtualenv-20.16.5\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade ibmcloudsql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import (load) the following packages:\n",
    "\n",
    "    - ibmcloudsql - IBM Data Engine python client\n",
    "    - sqlparse - a non-validating SQL parser \n",
    "    - pandas - an open source data manipulation and analysis tool \n",
    "    - pygments - a syntax highlighter library used to pretty print the SQL statements\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibmcloudsql\n",
    "import sqlparse \n",
    "import pandas as pd\n",
    "import getpass\n",
    "import pprint\n",
    "from pygments import highlight\n",
    "from pygments.lexers import get_lexer_by_name\n",
    "from pygments.formatters import HtmlFormatter, Terminal256Formatter\n",
    "lexer = get_lexer_by_name(\"sql\", stripall=True)\n",
    "formatter = Terminal256Formatter(style='vim')\n",
    "apikey=''\n",
    "instancecrn=''\n",
    "targeturl=''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"configure\"></a> 2. Configure the Data Engine client\n",
    "[Home](#toc)\n",
    "\n",
    "1. You need an **API key** for an IBM cloud identity. This single key provides you accesses to both your Cloud Object Storage (COS) bucket for writing SQL results and to your Data Engine instance. To create API keys log on to the IBM Cloud console and go to <a href=\"https://console.bluemix.net/iam/#/apikeys\" target=\"_blank\">Manage->Access (IAM): then select API Keys</a>, click the `Create an IBM Cloud API key` button, give the key a custom name and click `Create`. In the next dialog click `Show` and copy the key to your clipboard and paste it below in this notebook.\n",
    "\n",
    "2. You need the **instance CRN** for the Data Engine instance. If you don't have an Data Engine instance created yet, <a href=\"https://console.bluemix.net/catalog/services/sql-query\" target=\"_blank\">create one</a> first. If you already have one, you can find it in the <a href=\"https://console.bluemix.net/dashboard/apps\" target=\"_blank\">IBM Cloud console dashboard</a>. Make sure you select the right `Resource Groups` for **Group**. In the section `Services` you can see different types of services (created for the selected Group), and Data Engine instances have the icon like the one at the top of this notebook. Select the instance of Data Engine that you want to use. In the Data Engine dashboard page that opens up you find a section titled **Overview** with *Deployment Details* and copy the text after **CRN**. Click the button to copy the CRN into your clipboard and paste it here into the notebook. \n",
    "\n",
    "3. You need to specify the location on COS where your **query results** should be written.This is because Data Engine instance, to process an ETL SQL statement, needs to store queried data on COS.\n",
    "This COS location comprises three parts of information that you can find in the Cloud Object Storage UI for your instance in the IBM Cloud console. You need to provide it as a **target_cos_url** using the format `cos://<endpoint>/<bucket>/[<prefix>]`. \n",
    "\n",
    "In case you want to use the **cloud object storage bucket that is associated with your Watson Studio project** as target location for your query results you can make use of the project token for access and construct the target_cos_url as follows:  \n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"projectbucket\"></a> 2.1 Using the project bucket\n",
    "**Only** follow the instructions in this section when you want to write your SQL query results to the bucket that has been created for the project for which you have created this notebook. In any other case proceed directly with section **2.2**.\n",
    "<br><br>\n",
    "__Inserting the project token__:  \n",
    "Click the `More` option in the toolbar above (the three stacked dots) and select `Insert project token`.\n",
    " * If you haven't created an access token for this project before, you will see a dialog that asks you to create one first. Follow the link to open your project settings, scroll down to `Access tokens` and click `New token`. Give the token a custom name and make sure you select `Editor` as `Access role for project`. After you created your access token you can come back to this notebook, select the empty cell below and again select `Insert project token` from the toolbar at the top.\n",
    "[//]: # \n",
    "This will add a new cell at the top of your notebook with content that looks like this:\n",
    "```\n",
    "# @hidden_cell\n",
    "# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n",
    "from project_lib import Project\n",
    "project = Project(project_id='<some id>', project_access_token='<some access token>')\n",
    "pc = project.project_context\n",
    "```\n",
    "Leave that cell content as inserted and run the cell. Then then proceed with the following cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_bucket = project.get_metadata()['entity']['storage']['properties']\n",
    "targeturl=\"cos://\" + cos_bucket['bucket_region'] + \"/\" + cos_bucket['bucket_name'] + \"/\"\n",
    "targeturl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"parameters\"></a> 2.2 Setting the Data Engine parameters\n",
    "Now let's instantiate and configure an Data Engine Client. During instantiation the client accesses the Data Engine instance you selected and returns the link to the Data Engine service instance web console. You can use this link later to author an SQL statement interactively. \n",
    "\n",
    "In addition to the parameters entered interactively here, there is two more optional parameters **max_concurrent_jobs** (from version 0.4) and  **max_tries**  \n",
    "    For details see  <a href=\"https://ibm-cloud.github.io/sql-query-clients/ibmcloudsql.html#ibmcloudsql.SQLQuery.SQLQuery\">use</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter IBM Cloud API Key: ········\n",
      "Enter Data Engine instance CRN to use: crn:v1:bluemix:public:sql-query:us-south:a/d86af7367f70fba4f306d3c19c938f2f:d1b2c005-e3d8-48c0-9247-e9726a7ed510::\n",
      "Enter target URL for SQL results: cos://us-south/sqltempregional/\n"
     ]
    }
   ],
   "source": [
    "if apikey == '':\n",
    "    apikey=getpass.getpass('Enter IBM Cloud API Key: ')\n",
    "else:\n",
    "    apikey=getpass.getpass('Enter a new IBM Cloud API Key or leave empty to use the previous one: ') or apikey\n",
    "if instancecrn == '':\n",
    "    instancecrn=input('Enter Data Engine instance CRN to use: ')\n",
    "else:\n",
    "    instancecrn=input('Enter new Data Engine instance CRN to use (leave empty to use ' + instancecrn + '): ') or instancecrn\n",
    "if targeturl == '':\n",
    "    targeturl=input('Enter target URL for SQL results: ')\n",
    "else:\n",
    "    targeturl=input('Enter new target URL for SQL results (leave empty to use ' + targeturl + '): ') or targeturl\n",
    " \n",
    " \n",
    "sqlClient = ibmcloudsql.SQLQuery(apikey, instancecrn, client_info='Data Engine Starter Notebook', target_cos_url=targeturl, max_concurrent_jobs=4, max_tries=3 )\n",
    "#sqlClient.configure()  # use this if you want to change the API key or Data Engine CRN later\n",
    "    \n",
    "sqlClient.logon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your Data Engine web console link:\n",
      "\n",
      "https://dataengine.cloud.ibm.com/sqlquery/?instance_crn=crn:v1:bluemix:public:sql-query:us-south:a/d86af7367f70fba4f306d3c19c938f2f:d1b2c005-e3d8-48c0-9247-e9726a7ed510::\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://dataengine.cloud.ibm.com/sqlquery/?instance_crn=crn:v1:bluemix:public:sql-query:us-south:a/d86af7367f70fba4f306d3c19c938f2f:d1b2c005-e3d8-48c0-9247-e9726a7ed510::'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nYour Data Engine web console link:\\n')\n",
    "sqlClient.sql_ui_link()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"schema\"></a> 3. Get the schema of your data\n",
    "[Home](#toc)\n",
    "\n",
    "In order to work with your data using Data Engine you need to know the schema of your data. Since `ibmcloudsql` version 0.4, you can directly query the <a href=\"https://ibm-cloud.github.io/sql-query-clients/ibmcloudsql.html#ibmcloudsql.SQLQuery.SQLQuery.get_schema_data\">schema of the data</a>. Let's have a look at the employees.parquet dataset provided with Data Engine as a sample dataset. Note, that the first invocation uses the parameter `dry_run=True` which is done here to demonstrate you can get a preview of the statement being executed.\n",
    "\n",
    "**NOTE**: If you want to work with [hive table](#catalog), there is another API: `describe_table()` that returns the schema of your defined table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlClient.get_schema_data(\"cos://us-geo/sql/employees.parquet\", type=\"parquet\", dry_run=True)\n",
    "\n",
    "sqlClient.get_schema_data(\"cos://us-geo/sql/employees.parquet\", type=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sql\"></a> 4. Create your SQL statement \n",
    "[Home](#toc)\n",
    "\n",
    "Now that we know the schema of our data sets we can create an SQL statement to be executed on these data. You provide the SQL statement in the form of a **string**. In the subsequent cell a sample SQL statement on the above listed example datasets provided with Data Engine is given. You can either use that sample statement for the subsequent steps or copy and paste your own statement, which you may have authored using the Data Engine Web Console (**link above**) of your Data Engine service instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=input('Enter your SQL statement (leave empty to use a simple sample SQL)')\n",
    "\n",
    "if sql == '':\n",
    "    sql='SELECT o.OrderID, c.CompanyName, e.FirstName, e.LastName FROM cos://us-geo/sql/orders.parquet STORED AS PARQUET o, \\\n",
    "         cos://us-geo/sql/employees.parquet STORED AS PARQUET e, cos://us-geo/sql/customers.parquet STORED AS PARQUET c \\\n",
    "         WHERE e.EmployeeID = o.EmployeeID AND c.CustomerID = o.CustomerID AND o.ShippedDate > o.RequiredDate AND o.OrderDate > \"1998-01-01\" \\\n",
    "         ORDER BY c.CompanyName'\n",
    "if \" INTO \" not in sql:\n",
    "    sql += ' INTO {}myQueryResult STORED AS CSV'.format(targeturl)\n",
    "formatted_sql = sqlparse.format(sql, reindent=True, indent_tabs=True, keyword_case='upper')\n",
    "lexer = get_lexer_by_name(\"sql\", stripall=True)\n",
    "formatter = Terminal256Formatter(style='tango')\n",
    "result = highlight(formatted_sql, lexer, formatter)\n",
    "from IPython.core.display import display, HTML\n",
    "print('\\nYour SQL statement is:\\n')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since version 0.4, *ibmcloudsql* provides a new alternartive way to construct your SQL statement, using the functionality called <a href=\"https://github.com/IBM-Cloud/sql-query-clients/blob/master/Python/ibmcloudsql/sql_magic.py\">*sql_magic*</a>. The documentation is available <a href=\"https://ibm-cloud.github.io/sql-query-clients/sql_magic.html#sqlmagic\">here</a>. The same example SQL statement as above, can be generated with the sql_magic api as demonstrated in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlClient.reset_()\n",
    "(sqlClient.select_(\"o.OrderID, c.CompanyName, e.FirstName, e.LastName\")\n",
    "        .from_cos_(\"cos://us-geo/sql/orders.parquet\", format_type=\"parquet\", alias=\"o\")\n",
    "        .from_cos_(\"cos://us-geo/sql/employees.parquet\", format_type=\"parquet\", alias=\"e\")\n",
    "        .from_cos_(\"cos://us-geo/sql/customers.parquet\", alias=\"c\")\n",
    "        .where_('e.EmployeeID = o.EmployeeID AND c.CustomerID = o.CustomerID AND o.ShippedDate > o.RequiredDate AND o.OrderDate > \"1998-01-01\"')\n",
    "        .order_by_(\"c.CompanyName\")\n",
    "        .store_at_(targeturl + \"myResult\", format_type=\"csv\")\n",
    ")\n",
    "\n",
    "sqlClient.print_sql()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: *sql_magic* stores the constructed SQL string in the SQL client object so that you can use it (e.g. to submit() it for execution) in subsequent method calls on the client object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"run\"></a> 5. Run your SQL statement\n",
    "[Home](#toc)\n",
    "\n",
    "There are three options to execute an SQL statement, as described in the <a href=\"https://ibm-cloud.github.io/sql-query-clients/sql_query.html#b-submit-sql-jobs\">documentation</a>.    \n",
    "\n",
    "1. [**Synchronously with result dataframe**](#synchronous) - submit, wait (until the query is completed), and return the queried data as dataframe: `run_sql()`\n",
    "\n",
    "2. [**Asynchronously**](#lowlevel) - submit, and return the control immediately (along with job_id) : `submit_sql()` (`submit()` when using *sql_magic*)\n",
    "    \n",
    "3. [**Synchronously with optional result dataframe**](#execute_sql) - submit, wait (until the query is completed), and return a tuple (data, job_id), where data is the optional result dataframe: `execute_sql()`  (`run()` when using *sql_magic*)\n",
    "\n",
    "The last option is useful to help avoiding Python runtime memory overload. Another alternative to deal with big result sets is to paginate the result i.e. use the `pagesize` option, for details see [**paginated result**](#paginated). \n",
    "\n",
    "A Data Engine instance with free Lite plan can process one query at a time. A standard plan instance can by default process 5 concurrent queries, which can be increased via support ticket. When the maximum has been reached the Python SDK will retry (with progressive backup timing) as many times as has been specified in the **max_tries** when initializing the client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"synchronous\"></a> 5.1 Synchronous Execution() \n",
    "\n",
    "This method provides a synchronous SQL execution mechanism. In the subsequent cell the above SQL statement is submitted. The method waits for the excution to be finish. The queried data are returned synchronously as a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = sqlClient.run_sql(sql)\n",
    "if isinstance(result_df, str):\n",
    "    print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the subsequent call you see how easy it can be to visualize your result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "chartsize": "89",
      "clusterby": "LastName",
      "handlerId": "barChart",
      "keyFields": "FirstName",
      "orientation": "horizontal",
      "rendererId": "brunel",
      "rowCount": "20",
      "sortby": "Values ASC",
      "title": "Orders per Employee"
     }
    }
   },
   "outputs": [],
   "source": [
    "ax = result_df.FirstName.value_counts().plot(kind='bar', title=\"Orders per Employee\")\n",
    "ax.set_xlabel(\"First Name\")\n",
    "ax.set_ylabel(\"Order Count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"lowlevel\"></a> 5.2 Asynchronous Execution\n",
    "\n",
    "In the subsequent cell the earlier created SQL statement is submitted. The method returns right away without waiting for the completion of the job execution. \n",
    "\n",
    "`submit_sql()` runs the given SQL string,  while `submit()` runs the internally via `sql_magic` module generated SQL string ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobId = sqlClient.submit_sql(sql)\n",
    "print(\"SQL job submitted and running in the background. jobId = \" + jobId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Job status for \" + jobId + \": \" + sqlClient.get_job(jobId)['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `wait_for_job()` method as a blocking call until your job has finished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job_status = sqlClient.wait_for_job(jobId)\n",
    "print(\"Job \" + jobId + \" terminated with status: \" + job_status)\n",
    "if job_status == 'failed':\n",
    "    details = sqlClient.get_job(jobId)\n",
    "    print(\"Error: {}\\nError Message: {}\".format(details['error'], details['error_message']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `get_result()` method to retrieve a dataframe for the SQL result set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = sqlClient.get_result(jobId)\n",
    "print(\"OK, we have a dataframe for the SQL result that has been stored by Data Engine in \" + sqlClient.get_job(jobId)['resultset_location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative method for asynchronous SQL submission using the internal statement created earlier using *sql_magic* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Internal SQL string created earlier using sql_magic:\\n \")\n",
    "sqlClient.print_sql()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobId= sqlClient.submit()\n",
    "print(\"\\nSQL job submitted and running in the background. jobId = \" + jobId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_status = sqlClient.wait_for_job(jobId)\n",
    "print(\"Job \" + jobId + \" terminated with status: \" + job_status)\n",
    "if job_status == 'failed':\n",
    "    details = sqlClient.get_job(jobId)\n",
    "    print(\"Error: {}\\nError Message: {}\".format(details['error'], details['error_message']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"execute_sql\"></a> 5.3 Synchronous execution with optional result dataframe\n",
    "\n",
    "The synchronuous execution of an SQL statement with run_sql() can cause Python runtime memory overload when the result set is big. Therefore a synchronous execution method is provided which allows to control whether the result set is to be returned as dataframe or only stored on cloud object storage.   \n",
    "\n",
    "`execute_sql()` submits the sql statement, waits (until the query is completed), and returns a named tuple (data, job_id), with data being a dataframe optionally filled with the queried data. The option `get_result` controls if data are returned in the dataframe `data` or only stored on cloud object storage.  Setting get_result = \"false\"  is recommended if the result set is expected to be big. An alternative to deal with big result sets is to use pagination which is possible using the the `pagesize` option as described later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = sqlClient.execute_sql(sql, get_result=True) \n",
    "display(result.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent method to run a SQL statement that was generated with *sql_magic* is `run()`.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"paginated\"></a>5.4 Use paginated SQL \n",
    "\n",
    "There is another alternative to deal with large result sets to avoid Python runtime memory overload when reading the result as a dataframe. You can return the result in small enough  \"chunks\" or \"pages\". All of the SQL execution methods previously introduced provide the optional paramemter `pagesize`. When set the result set is written in multiple objects with each having as many rows as specified in `pagesize`. Since this is implemented using the Data Engine SQL syntax clause of `PARTITIONED EVERY <num> ROW` your query must not already contain another `PARTITIONED` clause when you set the `pagesize` parameter.\n",
    "    \n",
    "The following cells demonstrate the usage of the optional `pagesize` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagination_sql='SELECT OrderID, c.CustomerID CustomerID, CompanyName, City, Region, PostalCode \\\n",
    "                FROM cos://us-geo/sql/orders.parquet STORED AS PARQUET o, \\\n",
    "                     cos://us-geo/sql/customers.parquet STORED AS PARQUET c \\\n",
    "                WHERE c.CustomerID = o.CustomerID \\\n",
    "                INTO {}paginated_orders STORED AS PARQUET'.format(targeturl)\n",
    "\n",
    "formatted_etl_sql = sqlparse.format(pagination_sql, reindent=True, indent_tabs=True, keyword_case='upper')\n",
    "result = highlight(formatted_etl_sql, lexer, formatter)\n",
    "print('\\nExample Statement is:\\n')\n",
    "print(result)\n",
    "\n",
    "jobId = sqlClient.submit_sql(pagination_sql, pagesize=10)\n",
    "job_status = sqlClient.wait_for_job(jobId)\n",
    "print(\"Job \" + jobId + \" terminated with status: \" + job_status)\n",
    "job_details = sqlClient.get_job(jobId)\n",
    "if job_status == 'failed':\n",
    "    print(\"Error: {}\\nError Message: {}\".format(job_details['error'], job_details['error_message']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many pages with each 10 rows have been written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of pages written by job {}: {}\".format(jobId, len(sqlClient.list_results(jobId))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell retrieves the first page of the result as a data frame. The desired page is specified as the optional parameter **`pagenumber`** to the `get_result()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagenumber=1\n",
    "sqlClient.get_result(jobId, pagenumber=pagenumber).head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell gets the next page. Run it multiple times in order to retrieve the subsequent pages, one page after the another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pagenumber+=1\n",
    "sqlClient.get_result(jobId, pagenumber).head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"get_job\"></a>5.5 get_job()\n",
    "\n",
    "The method get_job() provides you with some execution related details. This way you can figure out the status of the job, the `result_location` on COS, the query start`start_time` and `end_time`, and the key performance metrics such as `bytes_read`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job_details = sqlClient.get_job(jobId)\n",
    "pprint.pprint(job_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"etl\"></a> 6. Running ETL SQLs\n",
    "[Home](#toc)\n",
    "\n",
    "The following ETL SQL statement joins two data sets from COS and writes the result to COS using **hive style partitioning** with two columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_sql='SELECT OrderID, c.CustomerID CustomerID, CompanyName, ContactName, ContactTitle, Address, City, Region, PostalCode, Country, Phone, Fax \\\n",
    "         EmployeeID, OrderDate, RequiredDate, ShippedDate, ShipVia, Freight, ShipName, ShipAddress, \\\n",
    "         ShipCity, ShipRegion, ShipPostalCode, ShipCountry FROM cos://us-geo/sql/orders.parquet STORED AS PARQUET o, \\\n",
    "         cos://us-geo/sql/customers.parquet STORED AS PARQUET c \\\n",
    "         WHERE c.CustomerID = o.CustomerID \\\n",
    "         INTO {}customer_orders STORED AS PARQUET PARTITIONED BY (ShipCountry, ShipCity)'.format(targeturl)\n",
    "formatted_etl_sql = sqlparse.format(etl_sql, reindent=True, indent_tabs=True, keyword_case='upper')\n",
    "result = highlight(formatted_etl_sql, lexer, formatter)\n",
    "print('\\nExample ETL Statement is:\\n')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobId = sqlClient.submit_sql(etl_sql)\n",
    "print(\"SQL job submitted and running in the background. jobId = \" + jobId)\n",
    "job_status = sqlClient.wait_for_job(jobId)\n",
    "print(\"Job \" + jobId + \" terminated with status: \" + job_status)\n",
    "job_details = sqlClient.get_job(jobId)\n",
    "if job_status == 'failed':\n",
    "    print(\"Error: {}\\nError Message: {}\".format(job_details['error'], job_details['error_message']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at the cloud object storage location where the result objects are stored. The following cell uses the `get_cos_summary()` method to print a summary of the objects that have been written by the previous ETL SQL statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultset_location = job_details['resultset_location']\n",
    "sqlClient.get_cos_summary(resultset_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the **total_volume** value. We will reference it for comparison in the next steps. Also note the number in `total_objects`, which indicates that the ETL produced 69 partitions (71 objects minus two bookkeeping objects).\n",
    "\n",
    "In the following cell we use the `list_results()` method to print a list of these 71 objects that have been written by the above ETL SQL statement. Note the partition columns and their values being part of the object names now. When you have a closer look at the cloud object storage URL of the objects you will notice that it contains the values of the partionting columns, e.g. `ShipCountry=Argentina/ShipCity=Buenos Aires` . All records in this result partition do have the value `Argentina` for column `ShipCountry`and `Buenos Aires` for `ChipCity`.  \n",
    "\n",
    "The fact that the result data are partitioned this way and that this fact is made externally visible by making it part of the object name can be leveraged for example by a query engine to optimize the execution performance. This type of partitioning called **hive-style-partitioning**  is the basis for optimizing the execution of an SQL statement using predicates that match with the partitioning columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "result_objects_df = sqlClient.list_results(jobId)\n",
    "print(\"List of objects written by ETL SQL:\")\n",
    "result_objects_df.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the result data with the `get_result()` method. Note that the result dataframe contains the two partitioning columns. The values for these have been put together by `get_result()` from the object names above because in hive style partitioning the partition column values are not stored in the objects but rather in the object names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlClient.get_result(jobId).head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell runs an SQL query against the **partitioned data** that has been produced by the previous ETL SQL statement. The query uses `WHERE` predicates on the columns the dataset is partitioned by. This allows for **performance optimization** during query execution. The query engine will physically only read the objects that match these predicate values exploiting the fact that the predicate columns match the partitioning columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_sql='SELECT * FROM {} STORED AS PARQUET WHERE ShipCountry = \"Austria\" AND ShipCity=\"Graz\" \\\n",
    "               INTO {} STORED AS PARQUET'.format(resultset_location, targeturl)\n",
    "formatted_optimized_sql = sqlparse.format(optimized_sql, reindent=True, indent_tabs=True, keyword_case='upper')\n",
    "result = highlight(formatted_optimized_sql, lexer, formatter)\n",
    "print('\\nRunning SQL against the previously produced hive style partitioned objects as input:\\n')\n",
    "print(result)\n",
    "\n",
    "jobId = sqlClient.submit_sql(optimized_sql)\n",
    "job_status = sqlClient.wait_for_job(jobId)\n",
    "print(\"Job \" + jobId + \" terminated with status: \" + job_status)\n",
    "job_details = sqlClient.get_job(jobId)\n",
    "if job_status == 'failed':\n",
    "    print(\"Error: {}\\nError Message: {}\".format(job_details['error'], job_details['error_message']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we use `get_job()` to verify in the job details of the just run optimized SQL that hive style partitioning has been leveraged. Note the `bytes_read` value that is significantly lower than the `total_volume` value of the data in the queries data set. The number is 6408 bytes - remember that the total volume of input data is 408 KB. The number of `bytes_read` matches the size of the partion customer_orders/jobid=6ff7b1d0-b69c-4d1b-8ebf-968d69436f56/**ShipCountry=Argentina/ShipCity=Buenos Aires**/ . \n",
    "This reveals the fact that the execution engine leverages the hive style partitioning to optimze performance by only reading the partitions which match the filter predicate. This I/O avoidance is the reason for the increase of query performance and the lower the query cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlClient.get_job(jobId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning data is also a worth a consideration when writing big result sets as this enables parallel writing and avoids memory problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"results\"></a> 7. Work with result objects\n",
    "[Home](#toc)\n",
    "\n",
    "The result set, apart from being returned to the client as pandas.Dataframe, is also stored permanently on cloud object storage in the form of cloud object storage objects. There is a set of methods that allow you to work with the results on an object level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"list_results\"></a> 7.1 list_results() \n",
    "\n",
    "The result objects written to cloud object storage by default with a virtual sub folder structure in the following naming convention: \n",
    "    \n",
    "        cos://endpoint/bucketname/resultSetName/jobid=<JOB_ID>/_SUCCESS \n",
    "        \n",
    "        cos://endpoint/bucketname/resultSetName/jobid=<JOB_ID>\n",
    "        \n",
    "        cos://endpoint/bucketname/resultSetName/jobid=<JOB_ID>/part-<xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx>.<format>\n",
    "    \n",
    "where  `cos://endpoint/bucketname` is the target URL and resultSetName the user picked result set name as specified in the SQL INTO clause. The first two objects are of zero byte size and are merely used for bookkeepung purposes. The first `_SUCCESS` object indicates that all objects of the given data set have been successfully written. The second object is empty as well and denotes the name prefix of all objects belonging to a specific data set. You may think of it as the \"root\" in a naming hierachy for all result set related objects. The third object can in fact exist more than once if a `PARTIONED INTO` or `PARTITIONED EVERY` clause has been used. \n",
    "\n",
    "The following cell runs a simple SQL and lists the result objects for it. Since it uses a `PARTITIONED INTO 3 OBJECT` clause you can see three data objects being written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "sql=\"SELECT * FROM cos://us-geo/sql/orders.parquet STORED AS PARQUET INTO {} STORED AS PARQUET PARTITIONED INTO 3 OBJECTS\".format(targeturl)\n",
    "jobId = sqlClient.submit_sql(sql)\n",
    "sqlClient.wait_for_job(jobId)\n",
    "sqlClient.list_results(jobId).head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"delete_result\"></a> 7.2 delete_result()\n",
    "\n",
    "You can delete the result set from Cloud Object Storage using the `delete_result()` method which deletes all cloud object storage objects related to the specified result set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlClient.delete_result(jobId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"rename_result_objects\"></a> 7.3 Enforce exact target object name\n",
    "\n",
    "As described above an SQL job always writes a folder structure with three (or more when partitioned) objects to the cloud object store. The fact that the jobId is generated into the name of the objects ensures that in case the same SQL statement is executed multiple times the result set is not overwritten but stored in execution specific objects. If this is not desired, but the result is supposed to be overwritten each time the SQL statement is executed `JOBPREFIX NONE` clause can be used.  In the subsequent sample the optional clause `JOBPREFIX NONE` implies that the jobId is no longer part of the object name. \n",
    "\n",
    "But note, that this in turn implies each time the same target path - as specified in the INTO clause - will be used and therefore the result data set objects be \"overwritten\". The following sample demonstrates the effect of the `JOBPREFIX NONE` clause in an SQL statement on the naming of the result objects. Note, that the jobID is no longer part of the object names.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=\"SELECT * FROM cos://us-geo/sql/orders.parquet STORED AS PARQUET LIMIT 100 INTO {}first100orders.parquet JOBPREFIX NONE STORED AS PARQUET\".format(targeturl)\n",
    "jobId = sqlClient.submit_sql(sql)\n",
    "sqlClient.wait_for_job(jobId)\n",
    "sqlClient.list_results(jobId).head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this your result object name is closer your actually specified path in the `INTO` clause, but it is still not EXACTLY the same. When you write a single partitioned result there is only one object with the data and you may want to have a single result object without any virtual folder structure. For this case you can use the method `rename_exact_result()` on a jobId for a job that has a single partitoned result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlClient.rename_exact_result(jobId)\n",
    "sqlClient.list_results(jobId).head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying this method to the jobs' result the `list_results()` method shows exactly one object - the one containing the result data - with the result set name as specified in the `INTO` clause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"manage_jobs\"></a> 8. Manage SQL jobs \n",
    "\n",
    "[Home](#toc)\n",
    "\n",
    "### <a id='joblist'></a> 8.1. Work with Job History\n",
    "\n",
    "The set of APIs that allow you to interact with jobs is provided via <a href=\"https://ibm-cloud.github.io/sql-query-clients/sql_query.html#d-manage-jobs\">this link</a>. \n",
    "\n",
    "Many of them are useful when you are launching **many** SQL statements e.g.\n",
    "\n",
    "1. get the list of running jobs: `get_jobs_with_status()`\n",
    "2. get the list of jobs that have been launched in the given session: `myjobs()`\n",
    "3. get up-to-30 most recent jobs: `get_jobs()`\n",
    "4. export the list of jobs: `export_job_history()`\n",
    "\n",
    "The next cell lists the most recent 30 jobs in your instance of Data Engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None) \n",
    "#You can change the value -1 for display.max_colwidth to a positive integer if you want to truncate the cell content to shrink the overall table display size.\n",
    "\n",
    "job_history_df = sqlClient.get_jobs()\n",
    "job_history_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need a longer history of your jobs then you need to periodically export the current job history to a location on COS. This can be achieved conveniently with the method `export_job_history()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlClient.export_job_history(targeturl + \"my_job_history/\",   \"job_export_\" , \".parquet\")  \n",
    "# sqlClient.export_job_history(targeturl + \"my_job_history/\" , \"job_export_\" , \".parquet\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time when you run this function it performs a delta check of the currently available recent history of 30 jobs with the already stored jobs on the specified location and writes out an additional parquet partition with only the new jobs that hadn't been exported previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run another SQL:\n",
    "sql=\"SELECT * FROM cos://us-geo/sql/orders.parquet STORED AS PARQUET LIMIT 100 INTO {} STORED AS PARQUET\".format(targeturl)\n",
    "jobId = sqlClient.submit_sql(sql)\n",
    "sqlClient.wait_for_job(jobId)\n",
    "\n",
    "#Export job history again:\n",
    "sqlClient.export_job_history(targeturl + \"my_job_history/\",   \"job_export_\" , \".parquet\")\n",
    "\n",
    "# Query exported job history:\n",
    "pd.set_option('display.max_colwidth', 20)\n",
    "sql = \"SELECT * FROM {}my_job_history/ STORED AS PARQUET INTO {} STORED AS PARQUET\".format(targeturl, targeturl)\n",
    "sqlClient.run_sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"many_queries\"></a> 8.2. Batch automation of SQL jobs\n",
    "[Home](#toc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you want to automate your ETL process and there is a situation where you have to submit many SQL statements at once. Doing so you have to be aware of the fact that there is a limit to the number of queries that can be served by a single Data Engine instance at a time. This is 1 for Lite Plan; and 5 for Standard Plan. If some of your jobs are long running it may happen that some jobs are not yet  completed when the session ends.  \n",
    "\n",
    "Since `ibmcloudsql` 0.4, if you're using Watson Studio,there is also the possibility to retry jobs which did not complete within the session they have been started in. At a later point in time when you start a new session you may want to  restart those jobs which did not complete. You do so by connecting to the [Project-Lib](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/project-lib-python.html), and save the list of jobs submitted in a file in the project bucket. When you re-run the session, the status of the execution of all jobs is updated and the jobs which did not complete in the previous session are  re-executed. \n",
    "\n",
    "The following code in the following cell demonstrates this behaviour. Re-run the cell a couple of times and check the file \"tracked_jobs_new.json\" in the project bucket to observe the behavior.  \n",
    "\n",
    "This can be used in Watson Studio with ProjectLib activated using a file stored as an asset in the project, or it can also be used using a file from a local machine.\n",
    "\n",
    "REF: The API docs is provided via <a href=\"https://ibm-cloud.github.io/sql-query-clients/ibmcloudsql.html#ibmcloudsql.SQLQuery.SQLQuery.submit_and_track_sql\">this link</a>. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "file_name=\"tracked_jobs1.json\"\n",
    "sqlClient.connect_project_lib(project, file_name)\n",
    "\n",
    "sql_stmt1='SELECT o.OrderID  FROM cos://us-geo/sql/orders.parquet STORED AS PARQUET o LIMIT 5 ' \n",
    "sql_stmt2='SELECT o.OrderID  FROM cos://us-geo/sql/orders.parquet STORED AS PARQUET o LIMIT 10 ' \n",
    "sql_stmt3='SELECT o.OrderID  FROM cos://us-geo/sql/orders.parquet STORED AS PARQUET o LIMIT 15 '\n",
    "sql_stmt4='SELECT o.OrderID  FROM cos://us-geo/sql/orders.parquet STORED AS PARQUET o LIMIT 20'\n",
    " \n",
    "\n",
    "jobs = [ ]\n",
    "\n",
    "jobs.append(sqlClient.submit_and_track_sql(sql_stmt1))\n",
    "jobs.append(sqlClient.submit_and_track_sql(sql_stmt2))\n",
    "jobs.append(sqlClient.submit_and_track_sql(sql_stmt3))  \n",
    "jobs.append(sqlClient.submit_and_track_sql(sql_stmt4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, if you are given a list of job_id, you can run them all, and each will be checked with those saved in the `file_name` stored in ProjectLib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlClient.process_failed_jobs_until_all_completed(jobs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"catalog\"></a>9 Work with hive tables\n",
    "[Home](#toc)\n",
    "\n",
    "So far all SQL statements used cloud object storage URLs to specify the input data of an SQL statement. This can be cumbersome. That's why ibmcloudsql has dedicated functions for hive meta store support of Data Engine. This enables the usage of table names in SQL statements rather than cloud object storage URLs. The hive meta store is a catalog  which holds for each table all related metadata including the table name, the schema, the backing cloud object storage objects, partitioning information etc.\n",
    "\n",
    "NOTE: Using this requires Standard Plan for Data Engine.\n",
    "\n",
    "One can create tables, list tables, drop tables etc. The APIs for catalog management can be found <a href=\"https://ibm-cloud.github.io/sql-query-clients/sql_query.html#f-manage-table-catalog\"> here </a>.    \n",
    "\n",
    "Let's  first create table for a simple sample dataset customers.csv provided by Data Engine. The `create_table()` method creates an object in the hive metastore containing the table name and the schema of the table. When not specifying the optional paramter `schema` the schema is automatically discovered from the data on COS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlClient.create_table(\"customers\", cos_url=\"cos://us-geo/sql/customers.csv\", format_type=\"csv\", force_recreate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `describe_table()` you can retrieve the schema information in the catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    }
   },
   "outputs": [],
   "source": [
    "customers_schema = sqlClient.describe_table(\"customers\")\n",
    "customers_schema.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we run the `create_table()` again but this time we specify the schema explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlClient.create_table(\"customers\", cos_url=\"cos://us-geo/sql/customers.csv\", format_type=\"csv\", force_recreate=True,\n",
    "                       schema=\"(customerID string, companyName string, contactName string, contact_Title string, address string, city string)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have hive-partitioned dataset on COS (i.e. the sub folder hierarchy adhere to the hive naming convention) you can create a partitioned table using the `create_partitioned_table()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = sqlClient.show_tables()\n",
    "try:\n",
    "    found = df[df[\"tableName\"].str.contains(\"customers_partitioned\")]\n",
    "except Exception:\n",
    "    found = []\n",
    "if len(found) > 0:\n",
    "    sqlClient.drop_table(\"customers_partitioned\")\n",
    "sqlClient.create_partitioned_table(\"customers_partitioned\", cos_url=\"cos://us-geo/sql/customers_partitioned.csv\", format_type=\"csv\")\n",
    "customers_partitioned_schema = sqlClient.describe_table(\"customers_partitioned\")\n",
    "customers_partitioned_schema.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: After creatung a partitioned table it is always initially emply until you have added the partitions to it. A convenient method to do this is via the `recover_table_partitions()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlClient.recover_table_partitions(\"customers_partitioned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a list of created tables with `show_tables()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " sqlClient.show_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively to the above convenience methods for managing hive tables you can use [DDL statements](https://cloud.ibm.com/docs/sql-query?topic=sql-query-sql-reference#chapterHiveCatalog) and run them via `submit_sql()`. This gives you the full set of hive table management commands and options, which goes beyond what the above convenience methods cover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"hms\"></a> 10. Custom Spark with Hive Metastore in Data Engine\n",
    "\n",
    "[Home](#toc)\n",
    "\n",
    "In this section we use a custom Spark application context that we configure with Data Engine as the table catalog. You have to run the **Notebook with a Spark runtime** environment to exercise this section. The section is divided into two parts:\n",
    "<ol>\n",
    "    <li>How to <b>set up</b> the Data Engine extensions</li>\n",
    "    <li>How to <b>configure</b> and <b>use</b> your Spark session with Data Engine as Hive Metastore</li>\n",
    "</ol>\n",
    "\n",
    "## <a id=\"hms_setup\"></a> 10.1. Set up Data Engine libraries (when using your own Spark runtimes)\n",
    "**Note** The steps in this sub section are only required if you're not using one of Watson Studio Spark environment or Analytics Engine serverless Spark. In those services the Data Engine libraries are already configured out of the box and you can immediately proceed with [section 10.2](#hms_usage) below.\n",
    "\n",
    "**Download Hive client** library for Spark to connect to Hive Metastore in Data Engine. The download directory needs to be specified to Data Engine's Spark session builder later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-26 09:18:23--  https://us.sql-query.cloud.ibm.com/download/catalog/hive-metastore-standalone-client-3.1.2-sqlquery.jar\n",
      "Resolving us.sql-query.cloud.ibm.com (us.sql-query.cloud.ibm.com)... 104.17.204.65, 104.16.171.24, 2606:4700::6811:cc41, ...\n",
      "Connecting to us.sql-query.cloud.ibm.com (us.sql-query.cloud.ibm.com)|104.17.204.65|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 48692070 (46M) [application/java-archive]\n",
      "Saving to: ‘/tmp/dataengine_jars/dataengine-hive-client.jar’\n",
      "\n",
      "/tmp/dataengine_jar 100%[===================>]  46.44M  30.2MB/s    in 1.5s    \n",
      "\n",
      "2022-09-26 09:18:26 (30.2 MB/s) - ‘/tmp/dataengine_jars/dataengine-hive-client.jar’ saved [48692070/48692070]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hive_client_dir=\"/tmp/dataengine_jars\"\n",
    "!mkdir -p {hive_client_dir}\n",
    "!wget https://us.sql-query.cloud.ibm.com/download/catalog/hive-metastore-standalone-client-3.1.2-sqlquery.jar -O {hive_client_dir}/dataengine-hive-client.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download** and install Data Engine **session builder** libraries that allow to create a Spark session readily configured with Data Engine with a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-26 09:19:04--  https://us.sql-query.cloud.ibm.com/download/catalog/dataengine-spark-integration-1.0.10.jar\n",
      "Resolving us.sql-query.cloud.ibm.com (us.sql-query.cloud.ibm.com)... 104.17.204.65, 104.16.171.24, 2606:4700::6810:ab18, ...\n",
      "Connecting to us.sql-query.cloud.ibm.com (us.sql-query.cloud.ibm.com)|104.17.204.65|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8266 (8.1K) [application/java-archive]\n",
      "Saving to: ‘user-libs/spark2/dataengine-spark.jar’\n",
      "\n",
      "user-libs/spark2/da 100%[===================>]   8.07K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-09-26 09:19:05 (57.2 MB/s) - ‘user-libs/spark2/dataengine-spark.jar’ saved [8266/8266]\n",
      "\n",
      "--2022-09-26 09:19:05--  https://us.sql-query.cloud.ibm.com/download/catalog/dataengine_spark-1.0.10-py3-none-any.whl\n",
      "Resolving us.sql-query.cloud.ibm.com (us.sql-query.cloud.ibm.com)... 104.17.204.65, 104.16.171.24, 2606:4700::6811:cc41, ...\n",
      "Connecting to us.sql-query.cloud.ibm.com (us.sql-query.cloud.ibm.com)|104.17.204.65|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3614 (3.5K) [application/octet-stream]\n",
      "Saving to: ‘/tmp/dataengine_spark-1.0.10-py3-none-any.whl’\n",
      "\n",
      "/tmp/dataengine_spa 100%[===================>]   3.53K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-09-26 09:19:06 (19.0 MB/s) - ‘/tmp/dataengine_spark-1.0.10-py3-none-any.whl’ saved [3614/3614]\n",
      "\n",
      "Processing /tmp/dataengine_spark-1.0.10-py3-none-any.whl\n",
      "Installing collected packages: dataengine-spark\n",
      "Successfully installed dataengine-spark-1.0.10\n"
     ]
    }
   ],
   "source": [
    "dataengine_spark_version=\"1.0.10\"\n",
    "!wget https://us.sql-query.cloud.ibm.com/download/catalog/dataengine-spark-integration-{dataengine_spark_version}.jar -O user-libs/spark2/dataengine-spark.jar\n",
    "!wget https://us.sql-query.cloud.ibm.com/download/catalog/dataengine_spark-{dataengine_spark_version}-py3-none-any.whl -O /tmp/dataengine_spark-{dataengine_spark_version}-py3-none-any.whl\n",
    "!pip install --user --force-reinstall /tmp/dataengine_spark-{dataengine_spark_version}-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you continue in the next sub section: <font color='red'>Restart Kernel now! </font> Select: Kernel -> Restart\n",
    "Also make sure that you run the cells to set `instancecrn` and `apikey` in section [2.2 Setting the Data Engine parameters](#parameters) again after restarting the kernel.\n",
    "<br><br>\n",
    "Now set up a Spark session with the hive client library that you just downloaded above into the in `hive_client_dir` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataengine import SparkSessionWithDataengine\n",
    "session_builder = SparkSessionWithDataengine.enableDataengine(instancecrn, apikey, \"public\", hive_client_dir)\n",
    "spark = session_builder.appName(\"Spark DataEngine integration\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"hms_usage\"></a> 10.2. Run Custom Spark job with Data Engine as Table Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"row\">\n",
    "<img style=\"float: center;\" src=\"https://s3.eu-de.cloud-object-storage.appdomain.cloud/dataengine-downloads/spark.png\" width=\"120\" margin=50 align=\"left\">\n",
    "</div>\n",
    "<div class=\"row\">\n",
    "Use a <b>custom Spark</b> application kernel with Jupyter notebook to run SQL queries on the data lake with the metadata defined in <b>Data Engine</b>.\n",
    "</div>\n",
    "<div class=\"row\">\n",
    "<br>\n",
    "\n",
    "In case you want to run only this noebook section here with a fresh notebook runtime, you must make sure that you first run the cells to set `instancecrn` and `apikey` in section [2.2 Setting the Data Engine parameters](#parameters) because we need these variables to configure the Hive client with your Data Engine instance.\n",
    "    \n",
    "Set up a **Data Eninge session** for Spark (don't run this cell if you went through sectioon 10.1. using your own Spark runtime because there we already configured your Spark session):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataengine import SparkSessionWithDataengine\n",
    "session_builder = SparkSessionWithDataengine.enableDataengine(instancecrn, apikey, \"public\", hive_client_dir)\n",
    "spark = session_builder.appName(\"Spark DataEngine integration\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only for informational purposes you can now introspect **Hive Metastore parameters** of the Data Engine Spark session. As you can see it is configured to connect to Data Engine's URI as Hive Metastore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.hive.metastore.truststore.password changeit\n",
      "spark.hive.metastore.client.plain.username crn:v1:bluemix:public:sql-query:us-south:a/d86af7367f70fba4f306d3c19c938f2f:d1b2c005-e3d8-48c0-9247-e9726a7ed510::\n",
      "spark.hive.metastore.uris thrift://catalog.us.dataengine.cloud.ibm.com:9083\n",
      "spark.hive.metastore.client.auth.mode PLAIN\n",
      "spark.hive.metastore.client.plain.password ***\n",
      "spark.hive.metastore.use.SSL true\n",
      "spark.hive.metastore.truststore.path file:////opt/ibm/jdk/jre/lib/security/cacerts\n"
     ]
    }
   ],
   "source": [
    "for conf in spark.sparkContext.getConf().getAll():\n",
    "    key = conf[0]\n",
    "    value = \"***\" if apikey == conf[1] else conf[1]\n",
    "    if key.startswith(\"spark.hive.metastore\"):\n",
    "        print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare a table `my_customers`** in your own bucket from the customers sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not targeturl.endswith('/'):\n",
    "    targeturl+=\"/\"\n",
    "jobId = sqlClient.submit_sql(\"SELECT * FROM cos://us-geo/sql/customers.csv INTO {}my_customers.parquet JOBPREFIX NONE STORED AS PARQUET\".format(targeturl))\n",
    "sqlClient.wait_for_job(jobId)\n",
    "sqlClient.rename_exact_result(jobId)\n",
    "sqlClient.create_table(\"my_customers\", cos_url=\"{}my_customers.parquet\".format(targeturl), format_type=\"parquet\", force_recreate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List** all **tables** in **Data Engine catalog**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------+-----------+\n",
      "|namespace|tableName                     |isTemporary|\n",
      "+---------+------------------------------+-----------+\n",
      "|default  |counties                      |false      |\n",
      "|default  |customer_statistics           |false      |\n",
      "|default  |customers                     |false      |\n",
      "|default  |customers_partitioned         |false      |\n",
      "|default  |eu_demographic                |false      |\n",
      "|default  |geographic                    |false      |\n",
      "|default  |geographic_full               |false      |\n",
      "|default  |jdbc_test_table               |false      |\n",
      "|default  |metergen                      |false      |\n",
      "|default  |my_customers                  |false      |\n",
      "|default  |mytimeseties                  |false      |\n",
      "|default  |plumpf                        |false      |\n",
      "|default  |position_data                 |false      |\n",
      "|default  |position_data2                |false      |\n",
      "|default  |position_data3                |false      |\n",
      "|default  |provinces                     |false      |\n",
      "|default  |struct_test                   |false      |\n",
      "|default  |tab_desktopq6rk8_1616421318103|false      |\n",
      "|default  |tab_test_snoop                |false      |\n",
      "|default  |tdec                          |false      |\n",
      "+---------+------------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a **Spark SQL** on a table in **Data Engine catalog**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|count(1)|    country|\n",
      "+--------+-----------+\n",
      "|       2|     Sweden|\n",
      "|      11|    Germany|\n",
      "|      11|     France|\n",
      "|       3|  Argentina|\n",
      "|       2|    Belgium|\n",
      "|       2|    Finland|\n",
      "|       3|      Italy|\n",
      "|       1|     Norway|\n",
      "|       5|      Spain|\n",
      "|       2|    Denmark|\n",
      "|       1|    Ireland|\n",
      "|       4|  Venezuela|\n",
      "|      13|        USA|\n",
      "|       5|     Mexico|\n",
      "|       7|         UK|\n",
      "|       2|Switzerland|\n",
      "|       3|     Canada|\n",
      "|       9|     Brazil|\n",
      "|       1|     Poland|\n",
      "|       2|   Portugal|\n",
      "+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.sql('select count(*), country from my_customers group by country')\n",
    "spark_df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"next\"></a> 11. Next steps\n",
    "\n",
    "[Home](#toc)\n",
    "\n",
    "In this notebook you learned how you can use the `ibmcloudsql` library in a Python notebook to submit SQL queries on data in IBM Cloud Object Storage and how you can interact with the query results. If you want to automate such an SQL job execution as part of your cloud solution, apart from the solution using ProjectLib, you can use the <a href=\"https://console.bluemix.net/openwhisk/\" target=\"_blank\">IBM Cloud Functions</a> framework. There is a dedicated SQL function available that lets you set up a cloud function to run SQL statements with IBM Data Engine. You can find the documentation for doing this <a href=\"https://hub.docker.com/r/ibmfunctions/sqlquery/\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"authors\"></a>Authors\n",
    "\n",
    "**Torsten Steinbach**, Torsten is IBM's CTO for Big Data in Cloud.\n",
    "\n",
    "**Tuan M. HoangTrong**, Tuan is the research staff member in the Distributed AI, TimeSeries Group. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corp. 2020-2022. This notebook and its source code are released under the terms of the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n",
    "<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n",
    "<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n",
    "<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 with Spark",
   "language": "python3",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
