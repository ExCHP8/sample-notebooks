{"cells": [{"metadata": {"id": "0309efd3-23a8-4215-9560-55680cdb4df9"}, "cell_type": "markdown", "source": "# IBM Federated Learning with Homomorphic Encryption"}, {"metadata": {}, "cell_type": "markdown", "source": "This notebook demonstrates how to run Federated Learning training experiments with Homomorphic Encryption."}, {"metadata": {}, "cell_type": "markdown", "source": "## Learning Goals"}, {"metadata": {}, "cell_type": "markdown", "source": "The learning goals of this notebook are:\n\n- Create and use the WML Python Client to setup and run Federated Learning training jobs.\n- Create WML assets (initial models, remote training systems), data sets and handlers, and cryptographic files (certificates, keys) - that are required for running a Federated Learning training job with encryption. All local files are stored in a single directory tree, whose root location can be specified using a setup parameter.\n- Launch a Federated Learning training job, by launching the aggergator and multiple parties. The aggregator runs in a cluster, and the parties run in the machine that is running this notebook. The number of parties can be specified using a setup parameter.\n- Monitor the training job.\n- Cleanup of the WML assets and the local files and directories created by this notebook. You will also be able to reuse assets and files that were created in previous sessions of this notebook and not removed."}, {"metadata": {}, "cell_type": "markdown", "source": "## Table of Contents"}, {"metadata": {}, "cell_type": "markdown", "source": "- [Introduction](#introduction)\n- [Prerequisites](#Prerequisites)\n- [Basic setup](#basic_setup)\n- [Create a WML client](#create_wml_client)\n- [Create WML assets](#create_wml_assets)\n- [Create parties data](#create_parties_data)\n- [Create parties cryptographic elements](#create_parties_crypto)\n- [Launch aggregator](#launch_aggregator)\n- [Launch parties](#launch_parties)\n- [Monitor execution status of the training](#monitor_execution)\n- [Cleanup](#cleanup)\n- [Next steps](#next_steps)"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"introduction\"></a>\n## Introduction"}, {"metadata": {}, "cell_type": "markdown", "source": "### IBM Federated Learning"}, {"metadata": {}, "cell_type": "markdown", "source": "IBM Federated Learning enables you to train a machine learning model across multiple decentralized parties holding local data sets, without sharing the local data sets. Such parties can be for example within an enterprise, within a consortium of enterprises, within multiple data centers or multiple clouds, or on edge devices. This allows to build a collective machine learning model without sharing data between the nodes, therefore addressing data security, privacy, and regulatory compliance requirements, as well as eliminating data movement and its associated costs.\n\nIn the federated learning training process, the parties build locally trained machine learning models and send these local models to an aggregator. The aggregator fuses the local models into an aggregated model and sends this model back to the parties to continue with the next round of training.\n\nFor additional details see [IBM Federated Learning documentation](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fed-lea.html)."}, {"metadata": {}, "cell_type": "markdown", "source": "### Homomorphic encryption support in IBM Federated Learning"}, {"metadata": {}, "cell_type": "markdown", "source": "IBM Federated Learning uses SSL secured connections between the parties and the aggregator for communicating the machine learning models. In this setting, the aggregator can still see the unencrypted local and aggregated models. \n\nIBM Federated Learning further includes homomorphic encryption capabilities, to enhance the parties\u2019 data privacy and security in settings where the aggregator operates in an environment which is less trusted, and the parties wish to avoid revealing the local models and the aggregated models to the aggregator.\n\nHomomorphic encryption (HE) is a form of encryption that enables performing computations on the encrypted data without decrypting it. The results of the computations remain in encrypted form which, when decrypted, results in an output that is the same as the output produced had the computations been performed on the unencrypted data.\n\nIn federated learning, homomorphic encryption enables the parties to homomorphically encrypt their local model updates before sending them to the aggregator. The aggregator sees only the homomorphically encrypted local model updates, and therefore cannot learn anything from this information. Specifically, the aggregator is not able to reverse-engineer the local model updates to discover information on local training data. The aggregator fuses the local model updates in their encrypted form, obtaining an encrypted aggregated model. Then the aggregator sends the encrypted aggregated model to the parties, which decrypt it and continue with the next round of training.\n\n<img src=\"https://raw.githubusercontent.com/IBMDataScience/sample-notebooks/master/Files/fhe_1.jpg\" width=\"500\"/>\n\nHomomorphic encryption is a form of public key cryptography. It uses a public key for encryption and a private key for decryption.\nIn IBM Federated Learning with homomorphic encryption, the parties (also named \u201cremote training systems\u201d) share the private HE key, and the aggregator has only the public HE key. Each party encrypts its local model update using the public HE key, and sends its encrypted local model update to the aggregator. Since the aggregator does not have the private HE key, it cannot decrypt the encrypted local model updates.\n\n<img src=\"https://raw.githubusercontent.com/IBMDataScience/sample-notebooks/master/Files/fhe_2.jpg\" width=\"500\"/>\n\nThe aggregator uses its public HE key to fuse the encrypted local model updates into a new encrypted aggregated model. This encrypted aggregated model is sent to the parties, which decrypt it using their private HE key, and continue the model training process.\n\n<img src=\"https://raw.githubusercontent.com/IBMDataScience/sample-notebooks/master/Files/fhe_3.jpg\" width=\"500\"/>\n\nIBM Federated Learning makes it easy to use homomorphic encryption in model training, by specifying simple parameters in the configurations of the aggregator and the parties. IBM Federated Learning includes a mechanism that generates and distributes automatically and securely homomorphic encryption keys among the parties participating in a training experiment."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"prerequisites\"></a>\n## Prerequisites"}, {"metadata": {}, "cell_type": "markdown", "source": "1. The currently supported operating system and architecture for running parties in Federated Learning experiments with homomorphic encryption is Linux x86. Therefore, this notebook must run on a Linux x86 platform.\n<br/><br/>\n2. Install the IBM Watson Machine Learning Python client package with homomorphic encryption support, within the Python environment in which this notebook runs. Use the following command within your Python environment:  \n```pip install 'ibm_watson_machine_learning[fl-rt22.2-py3.10,fl-crypto]'```.  \nYou can use the installation cell in the next section of this notebook to perform this installation.  \nThis installation is required for any Python environment that will be used for running parties in Federated Learning experiments with homomorphic encryption.\n<br/><br/>\n3. In your [IBM cloud account](https://cloud.ibm.com/):\n   1. Obtain your cloud user *IAM ID* by accessing the [Users](https://cloud.ibm.com/iam/users) page, clicking on the relevant user, clicking on *Details* and copying the *IAM ID*. The format of the *IAM ID* is *IBMid\\-\\<aaa\\>*. \n   2. Generate and obtain an API key from the [API keys](https://cloud.ibm.com/iam/apikeys) page.\n<br/><br/>\n4. Create a [Watson Machine Learning service instance](https://cloud.ibm.com/catalog/services/watson-machine-learning). A free plan is offered. In your Watson Machine Learning service instance:\n   1. Create a project or use an existing project, for use with this notebook. Access the [Projects](https://dataplatform.cloud.ibm.com/projects) page, and click *New project* to create a new project. For additional details see: [Creating a project](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/projects.html). Obtain the ID of the project to be used from *Projects > Specific project > Manage > Project ID*.\n   2. Associate your project with the Watson Machine Learning service, by accessing *Projects > Specific project > Manage > Services & integrations > Associate service*. For additional details see: [Associating services](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/assoc-services.html).\n   3. Obtain the location name of your Watson Machine Learning service instance, by accessing the location drop-down menu in the top toolbar. Example location names: *us-south*, *eu-gb*, *eu-de*, *jp-tok*."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"basic_setup\"></a>\n## Basic setup"}, {"metadata": {}, "cell_type": "markdown", "source": "Install the IBM Watson Machine Learning Python client package with homomorphic encryption support, within the Python environment in which this notebook runs, if this package is not yet installed in this environment."}, {"metadata": {}, "cell_type": "code", "source": "%pip install --upgrade 'ibm_watson_machine_learning[fl-rt22.2-py3.10,fl-crypto]'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The following cell applies base definitions for the notebook.\n \n**User action:** Before running the following cell, replace the mandatory TBDs in the cell with your information and review the optional TBDs."}, {"metadata": {"id": "de644d5d-3933-4df6-b4f5-9aa130efde28"}, "cell_type": "code", "source": "import os\nimport subprocess\nimport urllib3\nimport requests\nurllib3.disable_warnings()\n\ncmd = subprocess.Popen(\"pip list | grep 'ibm-watson-machine-learning'\", \n    shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\nwml_installed = len(cmd.communicate()[0]) > 0\nif not wml_installed:\n    raise Exception('ibm-watson-machine-learning package must be installed in the environment')\n\nbase_dir = os.getcwd() # TBD [optional] A base directory under which the notebook work directory will be created. Default is the current work directory.\nnb_dir = os.path.join(base_dir, 'fl_fhe_nb')\ndata_path = os.path.join(nb_dir, 'data')\nmodel_path = os.path.join(nb_dir, 'model')\ncrypto_path = os.path.join(nb_dir, 'crypto')\nexec_path = os.path.join(nb_dir, 'exec')\nif not os.path.exists(data_path):\n    os.makedirs(data_path)\nif not os.path.exists(model_path):\n    os.makedirs(model_path)\nif not os.path.exists(crypto_path):\n    os.makedirs(crypto_path)\nif not os.path.exists(exec_path):\n    os.makedirs(exec_path)\nos.chdir(exec_path)\n\nPROJECT_ID = '' # TBD [mandatory] See the prerequisites section for details.\nCLOUD_USERID = '' # TBD [mandatory] See the prerequisites section for details.\nIAM_APIKEY = '' # TBD [mandatory] See the prerequisites section for details.\nWML_SERVICES_LOCATION = '' # TBD [mandatory] See the prerequisites section for details.\nWML_SERVICES_URL = 'https://' + WML_SERVICES_LOCATION + '.ml.cloud.ibm.com'\nNUM_RTS = int(3) # TBD [optional] This parameter enables to specify the number of parties for a training experiment.\nSW_SPEC_NAME = 'runtime-22.2-py3.10'\nHW_SPEC_NAME = 'S'\nRSC_TAGS = ['wml_fl_fhe_nb_example']\nTIMEOUT_TRAINING_SEC = 600\ncrypto_file_ext = 'v1'\nasym_file_is = crypto_path + \"/is_asym_\" + crypto_file_ext + \".pem\"\ncert_file_is = crypto_path + \"/is_cert_\" + crypto_file_ext + \".pem\"\nasym_file_sb = crypto_path + \"/sb_asym_\" + crypto_file_ext + \"_\"\ncsr_file_sb = crypto_path + \"/sb_csr_\" + crypto_file_ext + \"_\"\ncert_file_sb = crypto_path + \"/sb_cert_\" + crypto_file_ext + \"_\"\nprt_data_file_prefix = 'data_party_'\nNUM_MODELS = int(1)\nMODEL_NAME = 'pytorch'\nMODEL_TYPE = 'pytorch-onnx_1.12'\nINIT_MODEL_FILE_NAME = 'pt_mnist_init_model.zip'\nINIT_MODEL_URL = 'https://github.com/IBMDataScience/sample-notebooks/raw/master/Files/pt_mnist_init_model.zip'\nDATA_HANDLER_FILE_NAME = 'mnist_pytorch_data_handler.py'\nDATA_HANDLER_CLASS_NAME = 'MnistPytorchDataHandler'\nDATASET_FILE_NAME = 'mnist.npz'\nDATASET_URL = 'https://api.dataplatform.cloud.ibm.com/v2/gallery-assets/entries/85ae67d0cf85df6cf114d0664194dc3b/data'\n\nhearbeat_resp = requests.get(WML_SERVICES_URL + \"/wml_services/training/heartbeat\", verify=False)\nprint(\"Heartbeat response %s\" % hearbeat_resp.content.decode(\"utf-8\"))", "execution_count": null, "outputs": []}, {"metadata": {"id": "4b23df51-63bb-4f80-a633-c259419db3ea"}, "cell_type": "markdown", "source": "<a id=\"create_wml_client\"></a>\n## Create a WML client"}, {"metadata": {}, "cell_type": "markdown", "source": "This section creates and activates a WML client, which enables to interact with your WML instance."}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient\n\nwml_credentials = {\n    \"url\": WML_SERVICES_URL,\n    \"apikey\": IAM_APIKEY\n}\nwml_client = APIClient(wml_credentials)\nwml_client.set.default_project(PROJECT_ID)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"create_wml_assets\"></a>\n## Create WML assets"}, {"metadata": {}, "cell_type": "markdown", "source": "The WML assets created in this notebook are initial models and remote training systems.  \nIn this section you can either create new assets, or reuse assets that were created in a previous session of this notebook and not removed."}, {"metadata": {}, "cell_type": "markdown", "source": "### Create new assets"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Store initial model assets in the cluster"}, {"metadata": {}, "cell_type": "markdown", "source": "Initial untrained model assets are required for Federated Learning.  \nIn this notebook, an untrained Pytorch model is used.  \nFor additional details see the documentation on [creating initial models](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fl-models.html).\n\nFirst, we download a pre-built initial model."}, {"metadata": {}, "cell_type": "code", "source": "import shutil\nprint(\"Downloading initial model\")\ninit_model_file_path = os.path.join(model_path, INIT_MODEL_FILE_NAME)\nwith requests.get(INIT_MODEL_URL, stream=True) as r:\n    with open(init_model_file_path, 'wb') as f:\n        shutil.copyfileobj(r.raw, f)\nprint('Model stored in: ' + str(init_model_file_path))\nprint(\"Done\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Next, we upload the initial model as an asset into the cluster."}, {"metadata": {}, "cell_type": "code", "source": "print(\"Storing initial model\")\nsw_spec_id = wml_client.software_specifications.get_id_by_name(SW_SPEC_NAME)\nuntrained_model_ids = {}\nmodel_metadata = {\n    wml_client.repository.ModelMetaNames.NAME: MODEL_NAME,\n    wml_client.repository.ModelMetaNames.TYPE: MODEL_TYPE,\n    wml_client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sw_spec_id,\n    wml_client.repository.ModelMetaNames.TAGS: RSC_TAGS\n}\nuntrained_model_details = wml_client.repository.store_model(os.path.join(model_path, INIT_MODEL_FILE_NAME), model_metadata)\nuntrained_model_ids[MODEL_NAME] = wml_client.repository.get_model_id(untrained_model_details)\nprint('Model id: ' + str(untrained_model_ids[MODEL_NAME]))\nprint('Done')", "execution_count": null, "outputs": []}, {"metadata": {"id": "aee02c1c-cc0b-4880-914d-da3ac4c04afb"}, "cell_type": "markdown", "source": "#### Create remote training systems in the cluster"}, {"metadata": {}, "cell_type": "markdown", "source": "A Remote Training System (RTS) asset defines a party that connects to the aggregator for a training experiment.  \nFor additional details see the corresponding [documentation](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fl-agg.html) page."}, {"metadata": {}, "cell_type": "code", "source": "print(\"Creating Remote Training Systems\")\nremote_training_systems = []\nfor i in range(NUM_RTS):\n    rts_metadata = {\n        wml_client.remote_training_systems.ConfigurationMetaNames.NAME: \"Party_\"+str(i),\n        wml_client.remote_training_systems.ConfigurationMetaNames.TAGS: RSC_TAGS,\n        wml_client.remote_training_systems.ConfigurationMetaNames.ORGANIZATION: {\"name\" : \"IBM\", \"region\": \"US\"},\n        wml_client.remote_training_systems.ConfigurationMetaNames.ALLOWED_IDENTITIES: [{\"id\": CLOUD_USERID, \"type\": \"user\"}],\n        wml_client.remote_training_systems.ConfigurationMetaNames.REMOTE_ADMIN: {\"id\": CLOUD_USERID, \"type\":\"user\"}\n    }\n    rts = wml_client.remote_training_systems.store(rts_metadata)\n    rts_id = wml_client.remote_training_systems.get_id(rts)\n    print('Remote training system Party_' + str(i) + ' id: ' + str(rts_id))\n    remote_training_systems.append({'id': rts_id, 'required': True})\nprint('Done')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Reuse existing assets"}, {"metadata": {}, "cell_type": "markdown", "source": "Run the following cell if you are reusing WML assets that were created in previous sessions.  \nThis code enables to build internal notebook lists from existing assets. These lists are used in later operations of this notebook."}, {"metadata": {}, "cell_type": "code", "source": "import json\n\nFORCE_REBUILD_DS = False\n\nprint(\"Models:\")\nif 'untrained_model_ids' not in globals() or FORCE_REBUILD_DS or \\\n    len(untrained_model_ids) != NUM_MODELS:\n    untrained_model_ids = {}\n    load_models_dict = True\nelse:\n    load_models_dict = False\nmodels = wml_client.repository.get_model_details(get_all=True)\nfor m in models['resources']:\n    md = m['metadata']\n    if not 'tags' in md or md['tags'] != RSC_TAGS:\n        continue\n    if load_models_dict:\n        untrained_model_ids[md['name']] = md['id']\n    print('{}: {}'.format(md['name'],md['id']))\n\nprint(\"Remote Training Systems:\")\nif 'remote_training_systems' not in globals() or FORCE_REBUILD_DS or \\\n    len(remote_training_systems) != NUM_RTS:\n    remote_training_systems = []\n    load_rts_lst = True\nelse:\n    load_rts_lst = False\nrts = wml_client.remote_training_systems.get_details()\nfor r in rts['resources']:\n    md = r['metadata']\n    if not 'tags' in md or md['tags'] != RSC_TAGS:\n        continue\n    if load_rts_lst:\n        remote_training_systems.append({'id': md['id'], 'required': True})\n    print('{}: {}'.format(md['name'],md['id']))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"create_parties_data\"></a>\n## Create parties data"}, {"metadata": {}, "cell_type": "markdown", "source": "This section downloads the MNIST data set and splits it into subsets for the parties.  \nThen, it defines and stores a data handler."}, {"metadata": {}, "cell_type": "markdown", "source": "### Download data set and split it for the parties"}, {"metadata": {}, "cell_type": "code", "source": "import os\nimport requests\nimport numpy as np\nimport shutil\n\ndef load_mnist(normalize=True, download_dir=''):\n    \"\"\"\n    Download MNIST training data from source used in `keras.datasets.load_mnist`\n    :param normalize: whether or not to normalize data\n    :type normalize: bool\n    :param download_dir: directory to download data\n    :type download_dir: `str`\n    :return: 2 tuples containing training and testing data respectively\n    :rtype (`np.ndarray`, `np.ndarray`), (`np.ndarray`, `np.ndarray`)\n    \"\"\"\n    local_file = os.path.join(download_dir, DATASET_FILE_NAME)\n    if not os.path.isfile(local_file):\n        with requests.get(DATASET_URL, stream=True) as r:\n            with open(local_file, 'wb') as f:\n                shutil.copyfileobj(r.raw, f)\n        with np.load(local_file, allow_pickle=True) as mnist:\n            x_train, y_train = mnist['x_train'], mnist['y_train']\n            x_test, y_test = mnist['x_test'], mnist['y_test']\n            if normalize:\n                x_train = x_train.astype('float32')\n                x_test = x_test.astype('float32')\n                x_train /= 255\n                x_test /= 255\n        np.savez(local_file, x_train=x_train, y_train=y_train,\n                 x_test=x_test, y_test=y_test)\n    else:\n        with np.load(local_file, allow_pickle=True) as mnist:\n            x_train, y_train = mnist['x_train'], mnist['y_train']\n            x_test, y_test = mnist['x_test'], mnist['y_test']\n    return (x_train, y_train), (x_test, y_test)\n\ndef save_mnist_party_data(nb_dp_per_party, should_stratify, party_folder, dataset_folder):\n    \"\"\"\n    Saves MNIST party data\n    :param nb_dp_per_party: the number of data points each party should have\n    :type nb_dp_per_party: `list[int]`\n    :param should_stratify: True if data should be assigned proportional to source class distributions\n    :type should_stratify: `bool`\n    :param party_folder: folder to save party data\n    :type party_folder: `str`\n    :param dataset_folder: folder to save dataset\n    :type data_path: `str`\n    :param dataset_folder: folder to save dataset\n    :type dataset_folder: `str`\n    \"\"\"\n    if not os.path.exists(dataset_folder):\n        os.makedirs(dataset_folder)\n    (x_train, y_train), (x_test, y_test) = load_mnist(download_dir=dataset_folder)\n    labels, train_counts = np.unique(y_train, return_counts=True)\n    te_labels, test_counts = np.unique(y_test, return_counts=True)\n    diff_labels = np.all(np.isin(labels, te_labels))\n    num_train = np.shape(y_train)[0]\n    num_test = np.shape(y_test)[0]\n    num_labels = np.shape(np.unique(y_test))[0]\n    nb_parties = len(nb_dp_per_party)\n    if should_stratify:\n        train_probs = {\n            label: train_counts[label] / float(num_train) for label in labels}\n        test_probs = {label: test_counts[label] /\n                        float(num_test) for label in te_labels}\n    else:\n        train_probs = {label: 1.0 / len(labels) for label in labels}\n        test_probs = {label: 1.0 / len(te_labels) for label in te_labels}\n    for idx, dp in enumerate(nb_dp_per_party):\n        train_p = np.array([train_probs[y_train[idx]]\n                            for idx in range(num_train)])\n        train_p /= np.sum(train_p)\n        train_indices = np.random.choice(num_train, dp, p=train_p)\n        test_p = np.array([test_probs[y_test[idx]] for idx in range(num_test)])\n        test_p /= np.sum(test_p)\n        test_indices = np.random.choice(\n            num_test, int(num_test / nb_parties), p=test_p)\n        x_train_pi = x_train[train_indices]\n        y_train_pi = y_train[train_indices]\n        x_test_pi = x_test[test_indices]\n        y_test_pi = y_test[test_indices]\n        name_file = prt_data_file_prefix + str(idx) + '.npz'\n        name_file = os.path.join(party_folder, name_file)\n        np.savez(name_file, x_train=x_train_pi, y_train=y_train_pi,\n                 x_test=x_test_pi, y_test=y_test_pi)\n    print('Data saved in ' + party_folder)\n    return\n\nsave_mnist_party_data(nb_dp_per_party=[200 for _ in range(NUM_RTS)], should_stratify=False, \n    party_folder=data_path, dataset_folder=data_path)\nprint('Done')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Define and store a data handler"}, {"metadata": {}, "cell_type": "markdown", "source": "This section creates a data handler Python file for the MNIST dataset to train using PyTorch.  \nFor additional details see the corresponding [documentation](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fl-handler.html) page."}, {"metadata": {}, "cell_type": "code", "source": "%%writefile mnist_pytorch_data_handler.py\nimport numpy as np\nfrom ibmfl.data.data_handler import DataHandler\n\nclass MnistPytorchDataHandler(DataHandler):\n    \"\"\"\n    Data handler for the MNIST dataset to train using PyTorch.\n    \"\"\"\n\n    def __init__(self, data_config=None):\n        super().__init__()\n        self.file_name = None\n        if data_config is not None:\n            if 'npz_file' in data_config:\n                self.file_name = data_config['npz_file']\n        # Load the datasets.\n        (self.x_train, self.y_train), (self.x_test, self.y_test) = self.load_dataset()\n        # Pre-process the datasets.\n        self.preprocess()\n\n    def get_data(self):\n        \"\"\"\n        Gets pre-process mnist training and testing data.\n\n        :return: training data\n        :rtype: `tuple`\n        \"\"\"\n        return (self.x_train, self.y_train), (self.x_test, self.y_test)\n\n    def load_dataset(self, nb_points=500):\n        \"\"\"\n        Loads the training and testing datasets from a given local path.\n        If no local path is provided, it will download the original MNIST \\\n        dataset online, and reduce the dataset size to contain \\\n        500 data points per training and testing dataset.\n        Because this method\n        is for testing it takes as input the number of datapoints, nb_points,\n        to be included in the training and testing set.\n\n        :param nb_points: Number of data points to be included in each set if\n        no local dataset is provided.\n        :type nb_points: `int`\n        :return: training and testing datasets\n        :rtype: `tuple`\n        \"\"\"\n        try:\n            data_train = np.load(self.file_name)\n            x_train = data_train['x_train']\n            y_train = data_train['y_train']\n            x_test = data_train['x_test']\n            y_test = data_train['y_test']\n        except Exception:\n            raise IOError('Unable to load training data from path '\n                            'provided in config file: ' +\n                            self.file_name)\n        return (x_train, y_train), (x_test, y_test)\n\n    def preprocess(self):\n        \"\"\"\n        Preprocesses the training and testing dataset, \\\n        e.g., reshape the images according to self.channels_first; \\\n        convert the labels to binary class matrices.\n\n        :return: None\n        \"\"\"\n        img_rows, img_cols = 28, 28\n        self.x_train = self.x_train.astype('float32').reshape(self.x_train.shape[0], 1, img_rows, img_cols)\n        self.x_test = self.x_test.astype('float32').reshape(self.x_test.shape[0], 1,img_rows, img_cols)\n        self.y_train = self.y_train.astype('int64')\n        self.y_test = self.y_test.astype('int64')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import shutil\nshutil.move(os.path.join('.', DATA_HANDLER_FILE_NAME), os.path.join(data_path, DATA_HANDLER_FILE_NAME))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"create_parties_crypto\"></a>\n## Create parties cryptographic elements"}, {"metadata": {}, "cell_type": "markdown", "source": "This section creates the certificate and key files required for running a Federated Learning training experiment with encryption.  \nTwo methods are provided in this section for creating the cryptographic files - using the Python *cryptography* package, or using *openssl*. Use either one of these methods.\n\nHomomorphic encryption keys are generated and distributed automatically and securely among the parties for each experiment. Only the parties participating in an experiment have access to the homomorphic encryption private key generated for the experiment.  \nTo facilitate this generation and distribution process, the following steps must be performed before an experiment:\n- All the parties participating in the experiment must agree on a single Certificate Authority.\n- Each party must be provisioned with a certificate from the agreed Certificate Authority.\n- Each party must be provisioned with an *RSA* key pair. The *RSA* public key must be included in the aforementioned party certificate.\n\nAn *RSA* key pair and certificate for a party must be generated using the following parameters and guidelines:\n- Key type: *RSA*.\n- Key size: 4096 bit.\n- Public exponent: 65537.\n- No password for the *RSA* key file.\n- Hash algorithm: *SHA256*.\n- The key and certificate files must be in *PEM* format.\n\nEach party must be configured with paths to the following files:  \n- Certificate of the Certificate Authority.\n- Certificate of the party issued by the Certificate Authority (includes the *RSA* public key of the party).\n- *RSA* private key of the party. \n\nFurther details on this configuration are provided in the notebook section [Launch parties](#launch-parties).\n\nIn this notebook, we generate and provision self-signed certificates. "}, {"metadata": {}, "cell_type": "markdown", "source": "### Method 1: Using Python Cryptography package"}, {"metadata": {}, "cell_type": "code", "source": "import os\nimport datetime\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom cryptography import x509\nfrom cryptography.x509.oid import NameOID\n\nclass CryptoRsa():\n\n    KEY_SIZE = 4096\n    PUBLIC_EXPONENT = 65537\n    CRYPTO_HASH = hashes.SHA256()\n\n    def __init__(self):\n        self.private_key = CryptoRsa.generate_key()\n\n    def generate_key():\n        private_key = rsa.generate_private_key(\n            public_exponent=CryptoRsa.PUBLIC_EXPONENT,\n            key_size=CryptoRsa.KEY_SIZE,\n        )\n        return private_key\n\n    def get_public_key(self, type: str = \"obj\"):\n        if self.private_key is None:\n            raise Exception(\"self.private_key is None\")\n        if type == \"obj\":\n            ret = self.private_key.public_key()\n        elif type == \"pem\":\n            ret = self.private_key.public_key().public_bytes(\n                encoding=serialization.Encoding.PEM,\n                format=serialization.PublicFormat.SubjectPublicKeyInfo\n            )\n        else:\n            raise Exception(\"Invalid type=\" + repr(type))\n        return ret\n\n    def write_key_file(self, file_path: str):\n        if self.private_key is None:\n            raise Exception(\"self.private_key is None\")\n        pem = self.private_key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.NoEncryption()\n        )\n        with open(file_path, \"wb\") as key_file:\n            key_file.write(pem)\n        return\n\nif not os.path.exists(crypto_path):\n    os.makedirs(crypto_path)\n\nissuer = x509.Name([\n    x509.NameAttribute(NameOID.COUNTRY_NAME, u\"US\"),\n    x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME, u\"California\"),\n    x509.NameAttribute(NameOID.LOCALITY_NAME, u\"San Francisco\"),\n    x509.NameAttribute(NameOID.ORGANIZATION_NAME, u\"Issuer Company\"),\n    x509.NameAttribute(NameOID.COMMON_NAME, u\"mysite.com\"),\n])\nsubject = x509.Name([\n    x509.NameAttribute(NameOID.COUNTRY_NAME, u\"US\"),\n    x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME, u\"California\"),\n    x509.NameAttribute(NameOID.LOCALITY_NAME, u\"San Francisco\"),\n    x509.NameAttribute(NameOID.ORGANIZATION_NAME, u\"Subject Company\"),\n    x509.NameAttribute(NameOID.COMMON_NAME, u\"mysite.com\"),\n])\n\nissuer_key = CryptoRsa()\nissuer_key.write_key_file(asym_file_is)\n\ncert_is = x509.CertificateBuilder().subject_name(\n    issuer\n).issuer_name(\n    issuer\n).public_key(\n    issuer_key.get_public_key()\n).serial_number(\n    x509.random_serial_number()\n).not_valid_before(\n    datetime.datetime.utcnow()\n).not_valid_after(\n    datetime.datetime.utcnow() + datetime.timedelta(days=1000)\n).add_extension(\n    x509.SubjectAlternativeName([x509.DNSName(u\"localhost\")]),\n    critical=False,\n).sign(issuer_key.private_key, CryptoRsa.CRYPTO_HASH)\nwith open(cert_file_is, \"wb\") as f:\n    f.write(cert_is.public_bytes(serialization.Encoding.PEM))\n\nfor idx in range(NUM_RTS):\n    asym_file_path = asym_file_sb+str(idx)+\".pem\"\n    cert_file_path = cert_file_sb+str(idx)+\".pem\"\n    subject_key = CryptoRsa()\n    subject_key.write_key_file(asym_file_path)\n    cert_sb = x509.CertificateBuilder().subject_name(\n        subject\n    ).issuer_name(\n        issuer\n    ).public_key(\n        subject_key.get_public_key()\n    ).serial_number(\n        x509.random_serial_number()\n    ).not_valid_before(\n        datetime.datetime.utcnow()\n    ).not_valid_after(\n        datetime.datetime.utcnow() + datetime.timedelta(days=1000)\n    ).add_extension(\n        x509.SubjectAlternativeName([x509.DNSName(u\"localhost\")]),\n        critical=False,\n    ).sign(issuer_key.private_key, CryptoRsa.CRYPTO_HASH)\n    with open(cert_file_path, \"wb\") as f:\n        f.write(cert_sb.public_bytes(serialization.Encoding.PEM))\n\nprint('Done')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Method 2: Using openssl"}, {"metadata": {}, "cell_type": "code", "source": "import os\n\nif not os.path.exists(cert_file_is):\n    ret = os.system(\"openssl req -x509 -newkey rsa:4096 -sha256 -days 365 -nodes \"\n        \"-subj \\\"/C=US/ST=California/L=San Francisco/O=Issuer Company/OU=Org/CN=www.iscompany.com\\\" -keyout \" + \n        str(asym_file_is) + \" -out \" + str(cert_file_is))\n    if ret != 0:\n        raise Exception(\"openssl for issuer failed: {}\".format(ret))\n\nfor idx in range(NUM_RTS):\n    asym_file_path = asym_file_sb+str(idx)+\".pem\"\n    csr_file_path = csr_file_sb+str(idx)+\".pem\"\n    cert_file_path = cert_file_sb+str(idx)+\".pem\"\n    if not os.path.exists(cert_file_path):\n        ret = os.system(\"openssl req -newkey rsa:4096 -nodes -subj \"\n            \"\\\"/C=US/ST=California/L=San Francisco/O=SB Company/OU=Org/CN=www.sbcompany.com\\\" -keyout \" +\n            str(asym_file_path) + \" -out \" + str(csr_file_path))\n        if ret != 0:\n            raise Exception(\"openssl for subject step 1 failed: {}\".format(ret))\n        ret = os.system(\"openssl x509 -req -CAcreateserial -CA \" + str(cert_file_is) + \" -CAkey \" + str(asym_file_is) +\n            \" -sha256 -days 365 -in \" + str(csr_file_path) + \" -out \" + str(cert_file_path))\n        if ret != 0:\n            raise Exception(\"openssl for subject step 2 failed: {}\".format(ret))\n\nprint('Done')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"launch_aggregator\"></a>\n## Launch aggregator"}, {"metadata": {}, "cell_type": "markdown", "source": "This section launches the Federated Learning aggregator for the experiment.\n\nTo run the experiment with homomorphic encryption, the aggregator\u2019s configuration must specify the following fusion type:  \n`\"fusion_type\": \"crypto_iter_avg\"`.\n\nThe aggregator\u2019s configuration may also include a `crypto` object, that specifies the required encryption level. For example:  \n```\n\"crypto\": {\n\t\"cipher_spec\": \"encryption_level_1\"\n}\n```  \nIf this object is not specified then the default of `encryption_level_1` is used.\n\nThere are four possible encryption levels, ranging from level 1 to level 4. Higher encryption levels increase security and precision, and require higher resource consumption (e.g. computation, memory, network bandwidth). The security level corresponds to the strength of the encryption system, typically measured by the number of operations that an attacker must perform to break the system. The precision level corresponds to the precision of the encryption system's outcomes. Higher precision level means that cryptographic operations are accurate up to a larger number of digits before and after the floating point. Higher precision levels reduce loss of accuracy of the model due to the encryption operations.  \nFollowing is a description of the encryption levels:\n- Encryption level 1 provides high security and good precision, and is the default level.\n- Encryption level 2 provides high security and high precision, and requires more resources than level 1.\n- Encryption level 3 provides extra high security and good precision, and requires more resources than level 2.\n- Encryption level 4 provides extra high security and high precision, and requires more resources than level 3.\n\nFor additional details on launching the aggregator see the corresponding [documentation](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fl-agg.html) page."}, {"metadata": {}, "cell_type": "code", "source": "fl_conf = {\n    \"model\": {\n      \"type\": MODEL_NAME,\n      \"spec\": {\n        \"id\": untrained_model_ids[MODEL_NAME]\n      },\n      \"model_file\": \"pytorch_sequence.pt\"\n    },\n    \"fusion_type\": \"crypto_iter_avg\",\n    \"crypto\": {\n      \"cipher_spec\": \"encryption_level_1\"\n    },\n    \"epochs\": 1,\n    \"rounds\": 2,\n    \"metrics\": \"accuracy\",\n    \"remote_training\": {\n      \"max_timeout\": TIMEOUT_TRAINING_SEC,\n      \"quorum\": 1,\n      \"remote_training_systems\": remote_training_systems,\n    },\n    \"software_spec\": {\n      \"name\": SW_SPEC_NAME\n    },\n    \"hardware_spec\": {\n      \"name\": HW_SPEC_NAME\n    }\n}\naggregator_metadata = {\n    wml_client.training.ConfigurationMetaNames.NAME: 'aggregator_he',\n    wml_client.training.ConfigurationMetaNames.DESCRIPTION: '',\n    wml_client.training.ConfigurationMetaNames.TAGS: RSC_TAGS,\n    wml_client.training.ConfigurationMetaNames.TRAINING_DATA_REFERENCES: [],\n    wml_client.training.ConfigurationMetaNames.TRAINING_RESULTS_REFERENCE: {\n        \"type\": \"container\",\n        \"name\": \"outputData\",\n        \"connection\": {},\n        \"location\": {\n          \"path\": \".\"\n        }\n    },\n    wml_client.training.ConfigurationMetaNames.FEDERATED_LEARNING: fl_conf\n}\nprint(\"Prepared config for aggregator with model type {}\".format(MODEL_NAME))\naggregator = wml_client.training.run(aggregator_metadata, asynchronous=True)\nprint(\"Created Aggregator\")\ntraining_id = wml_client.training.get_id(aggregator)\nprint(\"Training id: \" + str(training_id))\nprint (\"RTS: \" + str(remote_training_systems))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"launch_parties\"></a>\n## Launch parties"}, {"metadata": {}, "cell_type": "markdown", "source": "This section launches the Federated Learning parties for the experiment.\n\nTo run the experiment with homomorphic encryption, the parties\u2019 configuration must include a `crypto` object inside the `local_training` object, which specifies the required certificate and key files for the party.\n\nFor additional details on launching the parties see the corresponding [documentation](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fl-conn.html) page."}, {"metadata": {}, "cell_type": "code", "source": "import os\n\nfor idx, prt in enumerate(remote_training_systems):\n    party_metadata = {\n        wml_client.remote_training_systems.ConfigurationMetaNames.LOCAL_TRAINING: {\n            \"info\": {\n                \"crypto\": {\n                    \"key_manager\": {\n                        \"key_mgr_info\": {\n                            \"distribution\": {\n                \t\t\t\t\"ca_cert_file_path\": cert_file_is,\n                \t\t\t\t\"my_cert_file_path\": cert_file_sb+str(idx)+'.pem',\n                \t\t\t\t\"asym_key_file_path\": asym_file_sb+str(idx)+'.pem'\n                            }\n                        }\n                    }\n                }\n            }\n        },\n        wml_client.remote_training_systems.ConfigurationMetaNames.DATA_HANDLER: {\n            \"info\": {\n                \"npz_file\": os.path.join(data_path, prt_data_file_prefix+str(idx)+'.npz')\n            },\n            \"name\": DATA_HANDLER_CLASS_NAME,\n            \"path\": os.path.join(data_path, DATA_HANDLER_FILE_NAME)\n        }\n    }\n    print(\"Connecting party id {} to aggregator id {}, model type {}\".format(prt['id'], training_id, MODEL_NAME))\n    party = wml_client.remote_training_systems.create_party(prt['id'], party_metadata)\n    party.monitor_logs(\"ERROR\")\n    party.run(aggregator_id=training_id, asynchronous=True, verify=False)\n    print(\"Party {} is running\".format(prt['id']))\nprint('Done')", "execution_count": null, "outputs": []}, {"metadata": {"id": "475b62e9-fa52-4fb9-b6fd-91ab10df94c5"}, "cell_type": "markdown", "source": "<a id=\"monitor_execution\"></a>\n## Monitor execution status of the training"}, {"metadata": {}, "cell_type": "markdown", "source": "This section enables to monitor the execution status of the training experiment.\n\nFor additional details on monitoring the experiment see the corresponding [documentation](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fl-mon.html) page."}, {"metadata": {"id": "7d882c29-0837-4aa5-b1da-c20b311ac014"}, "cell_type": "code", "source": "import time\nimport json\n\ndef monitor_training(training_id):\n    print('Monitoring training id: {}'.format(training_id))\n    MAX_ITER = 240\n    SLP_TIME_SEC = 10\n    aggregator_status = wml_client.training.get_status(training_id)\n    aggregator_state = aggregator_status['state']\n    iter = 0\n    while iter < MAX_ITER and 'completed' != aggregator_state and 'failed' != aggregator_state and 'canceled' != aggregator_state:\n        print(\"Elapsed time: {} seconds, State: {}\".format(iter*SLP_TIME_SEC, aggregator_state))\n        time.sleep(SLP_TIME_SEC)\n        aggregator_status = wml_client.training.get_status(training_id)\n        aggregator_state = aggregator_status['state']\n        iter += 1\n    if iter >= MAX_ITER:\n        raise Exception(\"Training did not finish after {} seconds\".format(iter*SLP_TIME_SEC))\n    print(\"Final status: \" + json.dumps(aggregator_status, indent=4))\n\nif 'training_id' in globals():\n    monitor_training(training_id)\nelse:\n    trn = wml_client.training.get_details(get_all=True)\n    for t in trn['resources']:\n        md = t['metadata']\n        if 'tags' in md and md['tags'] == RSC_TAGS:\n            monitor_training(md['id'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"cleanup\"></a>\n## Cleanup"}, {"metadata": {}, "cell_type": "markdown", "source": "Use this section to delete the training jobs, assets, and local files created using this notebook."}, {"metadata": {}, "cell_type": "markdown", "source": "### Remove WML assets"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Remove training jobs"}, {"metadata": {}, "cell_type": "code", "source": "print('Removing training jobs')\ntrn = wml_client.training.get_details(get_all=True)\nfor t in trn['resources']:\n    md = t['metadata']\n    if 'tags' in md and md['tags'] == RSC_TAGS:\n        wml_client.training.cancel(md['id'], hard_delete=True)\n        print('Deleted {}: {}'.format(md['name'],md['id']))\nprint('Done')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Remove remote training systems and models"}, {"metadata": {}, "cell_type": "code", "source": "print('Removing remote training systems')\nrts = wml_client.remote_training_systems.get_details(get_all=True)\nfor r in rts['resources']:\n    md = r['metadata']\n    if 'tags' in md and md['tags'] == RSC_TAGS:\n        wml_client.repository.delete(md['id'])\n        print('Deleted {}: {}'.format(md['name'],md['id']))\n\nprint('Removing models')\nmodels = wml_client.repository.get_model_details(get_all=True)\nfor m in models['resources']:\n    md = m['metadata']\n    if 'tags' in md and md['tags'] == RSC_TAGS:\n        wml_client.repository.delete(md['id'])\n        print('Deleted {}: {}'.format(md['name'],md['id']))\n\nprint('Done')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Remove local files"}, {"metadata": {}, "cell_type": "code", "source": "import shutil\nshutil.rmtree(nb_dir)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"next_steps\"></a>\n## Next steps"}, {"metadata": {}, "cell_type": "markdown", "source": "You successfully completed this notebook!\n\nCheck out our [online documentation](https://dataplatform.cloud.ibm.com/docs) and [IBM Federated Learning documentation](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fed-lea.html) for more tutorials, samples and documentation."}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>\n\nCopyright &copy; IBM Corp. 2022. This notebook and its source code are released under the terms of the MIT License."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "vscode": {"interpreter": {"hash": "f3a806cb5f2c8e96de4e8a5e6dd38ad784866d379583e393d16cba069cf6e1b5"}}}, "nbformat": 4, "nbformat_minor": 2}