{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Monitor german credit risk model with Watson OpenScale"}, {"metadata": {}, "cell_type": "markdown", "source": "This notebook should be run in a Watson Studio project, using **Default Python 3** runtime environment. It requires service credentials for the following Cloud services:\n  * Watson Machine Learning V4\n  * Watson OpenScale\n  \nIf you have a paid Cloud account, you may also provision a **Databases for PostgreSQL** or **Db2 Warehouse** service to take full advantage of integration with Watson Studio and continuous learning services. If you choose not to provision this paid service, you can use the free internal PostgreSQL storage with OpenScale, but will not be able to configure continuous learning for your model.\n\nThe notebook will configure OpenScale to monitor that deployment, and inject seven days' worth of historical records and measurements for viewing in the OpenScale Insights dashboard."}, {"metadata": {}, "cell_type": "markdown", "source": "### Prerequisite"}, {"metadata": {}, "cell_type": "markdown", "source": "In order to execute this notebook you will need a deployed model. You can perform it with following [notebook](German%20credit%20risk%20prediction%20with%20Scikit%20for%20model%20monitoring.ipynb). Then you will need to copy `deployment_uid` to this notebook."}, {"metadata": {}, "cell_type": "markdown", "source": "### Learning goals\n\nIn this notebook, you will learn how to:\n\n- Retrieve existing deployment in Watson OpenScale\n-  Make a WatsonOpenscale subscription\n-  Prepare and run model monitoring and feedback logging\n-  Run fairness monitor\n-  Run drift monitor\n-  Run custom monitors and metrics\n\n### Contents\n\n- [Setup](#setup)\n- [OpenScale configuration](#openscale)\n- [Quality monitor and feedback logging](#quality)\n- [Fairness monitoring and explanations](#fairness)\n- [Drift monitoring](#drift)\n- [Custom monitors and metrics](#custom)\n- [Summary](#summary)"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"setup\"></a>\n## 1. Set up the environment\n\nBefore you use the sample code in this notebook, you must perform the following setup tasks:\n\n-  Create a <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/admin/create-services.html?context=wx&audience=wdp\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)."}, {"metadata": {}, "cell_type": "markdown", "source": "### Connection to WML\n\nAuthenticate the Watson Machine Learning service on IBM Cloud. You need to provide platform `api_key` and instance `location`.\n\nYou can use [IBM Cloud CLI](https://cloud.ibm.com/docs/cli/index.html) to retrieve platform API Key and instance location.\n\nAPI Key can be generated in the following way:\n```\nibmcloud login\nibmcloud iam api-key-create API_KEY_NAME\n```\n\nIn result, get the value of `api_key` from the output.\n\n\nLocation of your WML instance can be retrieved in the following way:\n```\nibmcloud login --apikey API_KEY -a https://cloud.ibm.com\nibmcloud resource service-instance WML_INSTANCE_NAME\n```\n\nIn result, get the value of `location` from the output."}, {"metadata": {}, "cell_type": "markdown", "source": "**Tip**: Your `Cloud API key` can be generated by going to the [**Users** section of the Cloud console](https://cloud.ibm.com/iam#/users). From that page, click your name, scroll down to the **API Keys** section, and click **Create an IBM Cloud API key**. Give your key a name and click **Create**, then copy the created key and paste it below. You can also get a service specific url by going to the [**Endpoint URLs** section of the Watson Machine Learning docs](https://cloud.ibm.com/apidocs/machine-learning).  You can check your instance location in your  <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance details.\n\nYou can also get service specific apikey by going to the [**Service IDs** section of the Cloud Console](https://cloud.ibm.com/iam/serviceids).  From that page, click **Create**, then copy the created key and paste it below.\n\n**Action**: Enter your `api_key` and `location` in the following cell."}, {"metadata": {}, "cell_type": "code", "source": "apikey = 'PASTE YOUR PLATFORM API KEY HERE'\nlocation = 'PASTE YOUR INSTANCE LOCATION HERE'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_credentials = {\n    \"apikey\": apikey,\n    \"url\": 'https://' + location + '.ml.cloud.ibm.com'\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Install and import the `ibm-watson-machine-learning` package\n**Note:** `ibm-watson-machine-learning` documentation can be found <a href=\"http://ibm-wml-api-pyclient.mybluemix.net/\" target=\"_blank\" rel=\"noopener no referrer\">here</a>."}, {"metadata": {}, "cell_type": "code", "source": "!pip install -U ibm-watson-machine-learning", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient\n\nclient = APIClient(wml_credentials)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Working with spaces\n\nFirst of all, you need to create a space that will be used for your work. If you do not have space already created, you can use [Deployment Spaces Dashboard](https://dataplatform.cloud.ibm.com/ml-runtime/spaces?context=cpdaas) to create one.\n\n- Click New Deployment Space\n- Create an empty space\n- Select Cloud Object Storage\n- Select Watson Machine Learning instance and press Create\n- Copy `space_id` and paste it below\n\n**Tip**: You can also use SDK to prepare the space for your work. More information can be found [here](https://github.com/IBM/watson-machine-learning-samples/blob/master/cloud/notebooks/python_sdk/instance-management/Space%20management.ipynb).\n\n**Action**: Assign space ID below"}, {"metadata": {}, "cell_type": "code", "source": "space_id = 'PASTE YOUR SPACE ID HERE'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You can use `list` method to print all existing spaces."}, {"metadata": {}, "cell_type": "code", "source": "client.spaces.list(limit=10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "To be able to interact with all resources available in Watson Machine Learning, you need to set **space** which you will be using."}, {"metadata": {}, "cell_type": "code", "source": "client.set.default_space(space_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "List deployments."}, {"metadata": {}, "cell_type": "code", "source": "client.deployments.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Put `deployment_id` of model deployed in [prerequisite](#prereq) notebook in the next cell."}, {"metadata": {}, "cell_type": "code", "source": "deployment_uid = \"PUT_YOUR_DEPLOYMENT_ID_HERE\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Get `deployment_url` and `model_uid` from deployment details."}, {"metadata": {}, "cell_type": "code", "source": "deployment_url = client.deployments.get_details(deployment_uid)['entity']['status']['online_url']['url']\nmodel_uid = client.deployments.get_details(deployment_uid)['entity']['asset']['id']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Install and import the `ibm-watson-openscale` package\n**Note:** `ibm-watson-openscale` documentation can be found <a href=\"http://ai-openscale-python-client.mybluemix.net/\" target=\"_blank\" rel=\"noopener no referrer\">here</a>."}, {"metadata": {}, "cell_type": "code", "source": "!pip install -U ibm-watson-openscale", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!pip install -U ibm-cloud-sdk-core", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "This tutorial can use Databases for PostgreSQL, Db2 Warehouse, or a free internal verison of PostgreSQL to create a datamart for OpenScale.\n\nIf you have previously configured OpenScale, it will use your existing datamart, and not interfere with any models you are currently monitoring. Do not update the cell below.\n\nIf you do not have a paid Cloud account or would prefer not to provision this paid service, you may use the free internal PostgreSQL service with OpenScale. Do not update the cell below.\n\nTo provision a new instance of Db2 Warehouse, locate [Db2 Warehouse in the Cloud catalog](https://cloud.ibm.com/catalog/services/db2-warehouse), give your service a name, and click **Create**. Once your instance is created, click the **Service Credentials** link on the left side of the screen. Click the **New credential** button, give your credentials a name, and click **Add**. Your new credentials can be accessed by clicking the **View credentials** button. Copy and paste your Db2 Warehouse credentials into the cell below.\n\nTo provision a new instance of Databases for PostgreSQL, locate [Databases for PostgreSQL in the Cloud catalog](https://cloud.ibm.com/catalog/services/databases-for-postgresql), give your service a name, and click **Create**. Once your instance is created, click the **Service Credentials** link on the left side of the screen. Click the **New credential** button, give your credentials a name, and click **Add**. Your new credentials can be accessed by clicking the **View credentials** button. Copy and paste your Databases for PostgreSQL credentials into the cell below."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "DB_CREDENTIALS = None\nSCHEMA_NAME = None", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "__If you previously configured OpenScale to use the free internal version of PostgreSQL, you can switch to a new datamart using a paid database service.__ If you would like to delete the internal PostgreSQL configuration and create a new one using service credentials supplied in the cell above, set the __KEEP_MY_INTERNAL_POSTGRES__ variable below to __False__ below. In this case, the notebook will remove your existing internal PostgreSQL datamart and create a new one with the supplied credentials. __*NO DATA MIGRATION WILL OCCUR.*__"}, {"metadata": {}, "cell_type": "code", "source": "KEEP_MY_INTERNAL_POSTGRES = True", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "In next cells, you will need to paste some credentials to Cloud Object Storage. If you haven't worked with COS yet please visit [getting started with COS tutorial](https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-getting-started-cloud-object-storage). \nYou can find `COS_API_KEY_ID` and `COS_RESOURCE_CRN` variables in **_Service Credentials_** in menu of your COS instance. Used COS Service Credentials must be created with _Role_ parameter set as Writer. Later training data file will be loaded to the bucket of your instance and used as training refecence in subsription.  \n`COS_ENDPOINT` variable can be found in **_Endpoint_** field of the menu."}, {"metadata": {}, "cell_type": "code", "source": "COS_API_KEY_ID = \"***\"\nCOS_RESOURCE_CRN = \"***\" \nCOS_ENDPOINT = \"***\" \nIAM_AUTH_ENDPOINT = \"***\"\nBUCKET_NAME = \"***\" ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Run the notebook\n\nAt this point, the notebook is ready to run. You can either run the cells one at a time, or click the **Kernel** option above and select **Restart and Run All** to run all the cells."}, {"metadata": {}, "cell_type": "markdown", "source": "### Load the training data from github"}, {"metadata": {"tags": []}, "cell_type": "code", "source": "!rm german_credit_data_biased_training.csv\n!wget https://raw.githubusercontent.com/pmservice/ai-openscale-tutorials/master/assets/historical_data/german_credit_risk/wml/german_credit_data_biased_training.csv", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Retrieve data filename, model name and deployment name."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "training_data_file_name = \"german_credit_data_biased_training.csv\"\nMODEL_NAME = \"Scikit German Risk Model WML V4\"\n\nDEPLOYMENT_NAME = \"Scikit German Risk Deployment WML V4\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Configure OpenScale <a name=\"openscale\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "The notebook will now import the necessary libraries and set up a Python OpenScale client."}, {"metadata": {"scrolled": true, "tags": []}, "cell_type": "code", "source": "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n\nfrom ibm_watson_openscale import APIClient\nfrom ibm_watson_openscale.supporting_classes.enums import *\nfrom ibm_watson_openscale.supporting_classes import *\n\n\nauthenticator = IAMAuthenticator(apikey=wml_credentials[\"apikey\"])\nwos_client = APIClient(authenticator=authenticator)\nwos_client.version", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Create schema and datamart"}, {"metadata": {}, "cell_type": "markdown", "source": "### Set up datamart"}, {"metadata": {}, "cell_type": "markdown", "source": "Watson OpenScale uses a database to store payload logs and calculated metrics. If database credentials were **not** supplied above, the notebook will use the free, internal lite database. If database credentials were supplied, the datamart will be created there **unless** there is an existing datamart **and** the **KEEP_MY_INTERNAL_POSTGRES** variable is set to **True**. If an OpenScale datamart exists in Db2 or PostgreSQL, the existing datamart will be used and no data will be overwritten.\n\nPrior instances of the German Credit model will be removed from OpenScale monitoring."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "wos_client.data_marts.show()", "execution_count": null, "outputs": []}, {"metadata": {"tags": []}, "cell_type": "code", "source": "data_marts = wos_client.data_marts.list().result.data_marts\nif len(data_marts) == 0:\n    if DB_CREDENTIALS is not None:\n        if SCHEMA_NAME is None: \n            print(\"Please specify the SCHEMA_NAME and rerun the cell\")\n\n        print(\"Setting up external datamart\")\n        added_data_mart_result = wos_client.data_marts.add(\n                background_mode=False,\n                name=\"WOS Data Mart\",\n                description=\"Data Mart created by WOS tutorial notebook\",\n                database_configuration=DatabaseConfigurationRequest(\n                  database_type=DatabaseType.POSTGRESQL,\n                    credentials=PrimaryStorageCredentialsLong(\n                        hostname=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"hosts\"][0][\"hostname\"],\n                        username=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"authentication\"][\"username\"],\n                        password=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"authentication\"][\"password\"],\n                        db=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"database\"],\n                        port=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"hosts\"][0][\"port\"],\n                        ssl=True,\n                        sslmode=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"query_options\"][\"sslmode\"],\n                        certificate_base64=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"certificate\"][\"certificate_base64\"]\n                    ),\n                    location=LocationSchemaName(\n                        schema_name= SCHEMA_NAME\n                    )\n                )\n             ).result\n    else:\n        print(\"Setting up internal datamart\")\n        added_data_mart_result = wos_client.data_marts.add(\n                background_mode=False,\n                name=\"WOS Data Mart\",\n                description=\"Data Mart created by WOS tutorial notebook\", \n                internal_database = True).result\n        \n    data_mart_id = added_data_mart_result.metadata.id\n    \nelse:\n    data_mart_id=data_marts[0].metadata.id\n    print(\"Using existing datamart {}\".format(data_mart_id))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Remove existing service provider connected with used  WML instance. "}, {"metadata": {}, "cell_type": "markdown", "source": "Multiple service providers for the same engine instance are avaiable in Watson OpenScale. To avoid multiple service providers of used WML instance in the tutorial notebook the following code deletes existing service provder(s) and then adds new one. "}, {"metadata": {}, "cell_type": "code", "source": "SERVICE_PROVIDER_NAME = \"Watson Machine Learning V2\"\nSERVICE_PROVIDER_DESCRIPTION = \"Added by tutorial WOS notebook\"", "execution_count": null, "outputs": []}, {"metadata": {"tags": []}, "cell_type": "code", "source": "service_providers = wos_client.service_providers.list().result.service_providers\nfor service_provider in service_providers:\n    service_instance_name = service_provider.entity.name\n    if service_instance_name == SERVICE_PROVIDER_NAME:\n        service_provider_id = service_provider.metadata.id\n        wos_client.service_providers.delete(service_provider_id)\n        print(\"Deleted existing service_provider for WML instance: {}\".format(service_provider_id))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Add service provider"}, {"metadata": {}, "cell_type": "markdown", "source": "Watson OpenScale needs to be bound to the Watson Machine Learning instance to capture payload data into and out of the model."}, {"metadata": {}, "cell_type": "markdown", "source": "**Note:** You can bind more than one engine instance if needed by calling `wos_client.service_providers.add` method. Next, you can refer to particular service provider using `service_provider_id`."}, {"metadata": {"tags": []}, "cell_type": "code", "source": "added_service_provider_result = wos_client.service_providers.add(\n        name=SERVICE_PROVIDER_NAME,\n        description=SERVICE_PROVIDER_DESCRIPTION,\n        service_type=ServiceTypes.WATSON_MACHINE_LEARNING,\n        deployment_space_id = space_id,\n        operational_space_id = \"production\",\n        credentials=WMLCredentialsCloud(\n            apikey=wml_credentials[\"apikey\"],\n            url=wml_credentials[\"url\"],\n            instance_id=None\n        ),\n        background_mode=False\n    ).result\nservice_provider_id = added_service_provider_result.metadata.id", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wos_client.service_providers.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Subscriptions"}, {"metadata": {}, "cell_type": "markdown", "source": "### Remove existing credit risk subscriptions"}, {"metadata": {}, "cell_type": "code", "source": "wos_client.subscriptions.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "This code removes previous subscriptions to the German Credit model to refresh the monitors with the new model and new data."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "subscriptions = wos_client.subscriptions.list().result.subscriptions\nfor subscription in subscriptions:\n    sub_model_id = subscription.entity.asset.asset_id\n    if sub_model_id == model_uid:\n        wos_client.subscriptions.delete(subscription.metadata.id)\n        print(\"Deleted existing subscription for model\", model_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Following cells create the model subscription in OpenScale using the Python client API. Note that we need to provide the model unique identifier, and some information about the model itself."}, {"metadata": {"tags": []}, "cell_type": "code", "source": "asset = Asset(\n    asset_id=model_uid,\n    url=deployment_url,\n    asset_type=AssetTypes.MODEL,\n    input_data_type=InputDataType.STRUCTURED,\n    problem_type=ProblemType.BINARY_CLASSIFICATION\n)\nasset_deployment = AssetDeploymentRequest(\n    deployment_id=deployment_uid,\n    name=DEPLOYMENT_NAME,\n    deployment_type=DeploymentTypes.ONLINE,\n    url=deployment_url\n)\ntraining_data_reference = TrainingDataReference(\n    type=\"cos\",\n    location=COSTrainingDataReferenceLocation(\n        bucket=BUCKET_NAME,\n        file_name=training_data_file_name\n    ),\n    connection=COSTrainingDataReferenceConnection.from_dict(\n        {\n            \"resource_instance_id\": COS_RESOURCE_CRN,\n            \"url\": COS_ENDPOINT,\n            \"api_key\": COS_API_KEY_ID,\n            \"iam_url\": \"https://iam.bluemix.net/oidc/token\"\n        }\n    )\n)\nasset_properties_request = AssetPropertiesRequest(\n    label_column=\"Risk\",\n    probability_fields=[\"probability\"],\n    prediction_field=\"prediction\",\n    feature_fields=[\"CheckingStatus\",\"LoanDuration\",\"CreditHistory\",\"LoanPurpose\",\"LoanAmount\",\"ExistingSavings\",\"EmploymentDuration\",\"InstallmentPercent\",\"Sex\",\"OthersOnLoan\",\"CurrentResidenceDuration\",\"OwnsProperty\",\"Age\",\"InstallmentPlans\",\"Housing\",\"ExistingCreditsCount\",\"Job\",\"Dependents\",\"Telephone\",\"ForeignWorker\"],\n    categorical_fields=[\"CheckingStatus\",\"CreditHistory\",\"LoanPurpose\",\"ExistingSavings\",\"EmploymentDuration\",\"Sex\",\"OthersOnLoan\",\"OwnsProperty\",\"InstallmentPlans\",\"Housing\",\"Job\",\"Telephone\",\"ForeignWorker\"],\n    training_data_reference=training_data_reference\n)", "execution_count": null, "outputs": []}, {"metadata": {"tags": []}, "cell_type": "code", "source": "subscription_details = wos_client.subscriptions.add(\n        data_mart_id=data_mart_id,\n        service_provider_id=service_provider_id,\n        asset=asset,\n        deployment=asset_deployment,\n        asset_properties=asset_properties_request).result\nsubscription_id = subscription_details.metadata.id\nprint(subscription_details)", "execution_count": null, "outputs": []}, {"metadata": {"tags": []}, "cell_type": "code", "source": "import time\n\ntime.sleep(5)\npayload_data_set_id = None\npayload_data_set_id = wos_client.data_sets.list(type=DataSetTypes.PAYLOAD_LOGGING, \n                                                target_target_id=subscription_id, \n                                                target_target_type=TargetTypes.SUBSCRIPTION).result.data_sets[0].metadata.id\nif payload_data_set_id is None:\n    print(\"Payload data set not found. Please check subscription status.\")\nelse:\n    print(\"Payload data set id:\", payload_data_set_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wos_client.data_sets.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\n\nGet subscription list"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "wos_client.subscriptions.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Score the model so we can configure monitors"}, {"metadata": {}, "cell_type": "markdown", "source": "Now that the WML service has been bound and the subscription has been created, we need to send a request to the model before we configure OpenScale. This allows OpenScale to create a payload log in the datamart with the correct schema, so it can capture data coming into and out of the model. The sends a few records for predictions."}, {"metadata": {"scrolled": true, "tags": []}, "cell_type": "code", "source": "fields = [\"CheckingStatus\",\"LoanDuration\",\"CreditHistory\",\"LoanPurpose\",\"LoanAmount\",\"ExistingSavings\",\"EmploymentDuration\",\"InstallmentPercent\",\"Sex\",\"OthersOnLoan\",\"CurrentResidenceDuration\",\"OwnsProperty\",\"Age\",\"InstallmentPlans\",\"Housing\",\"ExistingCreditsCount\",\"Job\",\"Dependents\",\"Telephone\",\"ForeignWorker\"]\nvalues = [\n  [\"no_checking\",10,\"credits_paid_to_date\",\"car_new\",1343,\"100_to_500\",\"1_to_4\",2,\"female\",\"none\",3,\"savings_insurance\",46,\"none\",\"own\",2,\"skilled\",1,\"none\",\"yes\"],\n  [\"no_checking\",20,\"prior_payments_delayed\",\"furniture\",4567,\"500_to_1000\",\"1_to_4\",4,\"male\",\"none\",4,\"savings_insurance\",36,\"none\",\"free\",2,\"management_self-employed\",1,\"none\",\"yes\"],\n  [\"0_to_200\",24,\"all_credits_paid_back\",\"car_new\",863,\"less_100\",\"less_1\",2,\"female\",\"co-applicant\",2,\"real_estate\",38,\"none\",\"own\",1,\"skilled\",1,\"none\",\"yes\"],\n  [\"0_to_200\",17,\"no_credits\",\"car_new\",2368,\"less_100\",\"1_to_4\",3,\"female\",\"none\",3,\"real_estate\",29,\"none\",\"own\",1,\"skilled\",1,\"none\",\"yes\"],\n  [\"0_to_200\",66,\"no_credits\",\"car_new\",250,\"less_100\",\"unemployed\",2,\"female\",\"none\",3,\"real_estate\",23,\"none\",\"rent\",1,\"management_self-employed\",1,\"none\",\"yes\"],\n  [\"no_checking\",14,\"credits_paid_to_date\",\"car_new\",832,\"100_to_500\",\"1_to_4\",2,\"male\",\"none\",2,\"real_estate\",42,\"none\",\"own\",1,\"skilled\",1,\"none\",\"yes\"],\n  [\"no_checking\",35,\"outstanding_credit\",\"appliances\",5696,\"unknown\",\"greater_7\",4,\"male\",\"co-applicant\",4,\"unknown\",54,\"none\",\"free\",2,\"skilled\",1,\"yes\",\"yes\"],\n  [\"0_to_200\",23,\"prior_payments_delayed\",\"retraining\",1375,\"100_to_500\",\"4_to_7\",3,\"male\",\"none\",3,\"real_estate\",37,\"none\",\"own\",2,\"management_self-employed\",1,\"none\",\"yes\"]\n]\n\npayload_scoring = {\"input_data\": [{\"fields\": fields, \"values\": values}]}\npredictions = client.deployments.score(deployment_uid, payload_scoring)\n\nprint(\"Single record scoring result:\", \"\\n fields:\", predictions[\"predictions\"][0][\"fields\"], \"\\n values: \", predictions[\"predictions\"][0][\"values\"][0])", "execution_count": null, "outputs": []}, {"metadata": {"tags": []}, "cell_type": "code", "source": "import uuid\nfrom ibm_watson_openscale.supporting_classes.payload_record import PayloadRecord\ntime.sleep(5)\npl_records_count = wos_client.data_sets.get_records_count(payload_data_set_id)\nprint(\"Number of records in the payload logging table: {}\".format(pl_records_count))\nif pl_records_count == 0:\n    print(\"Payload logging did not happen, performing explicit payload logging.\")\n    wos_client.data_sets.store_records(data_set_id=payload_data_set_id, request_body=[PayloadRecord(\n                   scoring_id=str(uuid.uuid4()),\n                   request=payload_scoring,\n                   response=predictions,\n                   response_time=460\n               )])\n    time.sleep(5)\n    pl_records_count = wos_client.data_sets.get_records_count(payload_data_set_id)\n    print(\"Number of records in the payload logging table: {}\".format(pl_records_count))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Quality monitoring and feedback logging <a name=\"quality\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "## Enable quality monitoring"}, {"metadata": {}, "cell_type": "markdown", "source": "The code below waits ten seconds to allow the payload logging table to be set up before it begins enabling monitors. First, it turns on the quality (accuracy) monitor and sets an alert threshold of 70%. OpenScale will show an alert on the dashboard if the model accuracy measurement (area under the curve, in the case of a binary classifier) falls below this threshold.\n\nThe second paramater supplied, min_records, specifies the minimum number of feedback records OpenScale needs before it calculates a new measurement. The quality monitor runs hourly, but the accuracy reading in the dashboard will not change until an additional 50 feedback records have been added, via the user interface, the Python client, or the supplied feedback endpoint."}, {"metadata": {"scrolled": true, "tags": []}, "cell_type": "code", "source": "import time\n\ntime.sleep(10)\ntarget = Target(\n        target_type=TargetTypes.SUBSCRIPTION,\n        target_id=subscription_id\n)\nparameters = {\n    \"min_feedback_data_size\": 50\n}\nquality_monitor_details = wos_client.monitor_instances.create(\n    data_mart_id=data_mart_id,\n    background_mode=False,\n    monitor_definition_id=wos_client.monitor_definitions.MONITORS.QUALITY.ID,\n    target=target,\n    parameters=parameters\n).result", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "quality_monitor_instance_id = quality_monitor_details.metadata.id", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Feedback logging"}, {"metadata": {}, "cell_type": "markdown", "source": "The code below downloads and stores enough feedback data to meet the minimum threshold so that OpenScale can calculate a new accuracy measurement. It then kicks off the accuracy monitor. The monitors run hourly, or can be initiated via the Python API, the REST API, or the graphical user interface."}, {"metadata": {"tags": []}, "cell_type": "code", "source": "!rm additional_feedback_data_v2.json\n!wget https://raw.githubusercontent.com/pmservice/ai-openscale-tutorials/master/assets/historical_data/german_credit_risk/wml/additional_feedback_data_v2.json", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Get feedback logging dataset ID"}, {"metadata": {"tags": []}, "cell_type": "code", "source": "feedback_dataset_id = None\nfeedback_dataset = wos_client.data_sets.list(type=DataSetTypes.FEEDBACK, \n                                                target_target_id=subscription_id, \n                                                target_target_type=TargetTypes.SUBSCRIPTION).result\nprint(feedback_dataset)\nfeedback_dataset_id = feedback_dataset.data_sets[0].metadata.id\nif feedback_dataset_id is None:\n    print(\"Feedback data set not found. Please check quality monitor status.\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import json\nwith open('additional_feedback_data_v2.json') as feedback_file:\n    additional_feedback_data = json.load(feedback_file)", "execution_count": null, "outputs": []}, {"metadata": {"tags": []}, "cell_type": "code", "source": "wos_client.data_sets.store_records(feedback_dataset_id, request_body=additional_feedback_data, background_mode=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wos_client.data_sets.get_records_count(data_set_id=feedback_dataset_id)", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true, "tags": []}, "cell_type": "code", "source": "run_details = wos_client.monitor_instances.run(monitor_instance_id=quality_monitor_instance_id, background_mode=False).result", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wos_client.monitor_instances.show_metrics(monitor_instance_id=quality_monitor_instance_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Fairness monitoring and explanations <a name=\"fairness\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "### Fairness configuration\n\nThe code below configures fairness monitoring for our model. It turns on monitoring for two features, Sex and Age. In each case, we must specify:\n  * Which model feature to monitor\n  * One or more **majority** groups, which are values of that feature that we expect to receive a higher percentage of favorable outcomes\n  * One or more **minority** groups, which are values of that feature that we expect to receive a higher percentage of unfavorable outcomes\n  * The threshold at which we would like OpenScale to display an alert if the fairness measurement falls below (in this case, 95%)\n\nAdditionally, we must specify which outcomes from the model are favourable outcomes, and which are unfavourable. We must also provide the number of records OpenScale will use to calculate the fairness score. In this case, OpenScale's fairness monitor will run hourly, but will not calculate a new fairness rating until at least 200 records have been added. Finally, to calculate fairness, OpenScale must perform some calculations on the training data, so we provide the dataframe containing the data."}, {"metadata": {}, "cell_type": "code", "source": "wos_client.monitor_instances.show()", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true, "tags": []}, "cell_type": "code", "source": "target = Target(\n    target_type=TargetTypes.SUBSCRIPTION,\n    target_id=subscription_id\n\n)\nparameters = {\n    \"features\": [\n        {\"feature\": \"Sex\",\n         \"majority\": ['male'],\n         \"minority\": ['female'],\n         \"threshold\": 0.95\n         },\n        {\"feature\": \"Age\",\n         \"majority\": [[26, 75]],\n         \"minority\": [[18, 25]],\n         \"threshold\": 0.95\n         }\n    ],\n    \"favourable_class\": [\"No Risk\"],\n    \"unfavourable_class\": [\"Risk\"],\n    \"min_records\": 4\n}\n\nfairness_monitor_details = wos_client.monitor_instances.create(\n    data_mart_id=data_mart_id,\n    background_mode=False,\n    monitor_definition_id=wos_client.monitor_definitions.MONITORS.FAIRNESS.ID,\n    target=target,\n    parameters=parameters).result\nfairness_monitor_instance_id =fairness_monitor_details.metadata.id", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Drift configuration"}, {"metadata": {"tags": []}, "cell_type": "code", "source": "monitor_instances = wos_client.monitor_instances.list().result.monitor_instances\nfor monitor_instance in monitor_instances:\n    monitor_def_id=monitor_instance.entity.monitor_definition_id\n    if monitor_def_id == \"drift\" and monitor_instance.entity.target.target_id == subscription_id:\n        wos_client.monitor_instances.delete(monitor_instance.metadata.id)\n        print('Deleted existing drift monitor instance with id: ', monitor_instance.metadata.id)\n\n\ntarget = Target(\n    target_type=TargetTypes.SUBSCRIPTION,\n    target_id=subscription_id\n\n)\nparameters = {\n    \"min_samples\": 20,\n    \"train_drift_model\": True,\n    \"enable_model_drift\": False,\n    \"enable_data_drift\": True\n}\n\ndrift_monitor_details = wos_client.monitor_instances.create(\n    data_mart_id=data_mart_id,\n    background_mode=False,\n    monitor_definition_id=wos_client.monitor_definitions.MONITORS.DRIFT.ID,\n    target=target,\n    parameters=parameters\n).result\n\ndrift_monitor_instance_id = drift_monitor_details.metadata.id", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Score the model again now that monitoring is configured"}, {"metadata": {}, "cell_type": "markdown", "source": "This next section randomly selects 200 records from the data feed and sends those records to the model for predictions. This is enough to exceed the minimum threshold for records set in the previous section, which allows OpenScale to begin calculating fairness."}, {"metadata": {"tags": []}, "cell_type": "code", "source": "from IPython.utils import io\n!rm german_credit_feed.json\nwith io.capture_output() as captured:\n    !wget https://raw.githubusercontent.com/pmservice/ai-openscale-tutorials/master/assets/historical_data/german_credit_risk/wml/german_credit_feed.json -O german_credit_feed.json\n!ls -lh german_credit_feed.json", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Score 200 randomly chosen records"}, {"metadata": {"tags": []}, "cell_type": "code", "source": "import random\n\nwith open('german_credit_feed.json', 'r') as scoring_file:\n    scoring_data = json.load(scoring_file)\n\nfields = scoring_data['fields']\nvalues = []\nfor _ in range(200):\n    values.append(random.choice(scoring_data['values']))\npayload_scoring = {\"input_data\": [{\"fields\": fields, \"values\": values}]}\n\nscoring_response = client.deployments.score(deployment_uid, payload_scoring)\ntime.sleep(5)\n\nif pl_records_count == 8:\n    print(\"Payload logging did not happen, performing explicit payload logging.\")\n    wos_client.data_sets.store_records(data_set_id=payload_data_set_id, request_body=[PayloadRecord(\n                   scoring_id=str(uuid.uuid4()),\n                   request=payload_scoring,\n                   response=scoring_response,\n                   response_time=460\n               )])\n    time.sleep(5)\n    pl_records_count = wos_client.data_sets.get_records_count(payload_data_set_id)\n    print(\"Number of records in the payload logging table: {}\".format(pl_records_count))", "execution_count": null, "outputs": []}, {"metadata": {"tags": []}, "cell_type": "code", "source": "print(\"Number of records in payload table:\", wos_client.data_sets.get_records_count(data_set_id=payload_data_set_id))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Run fairness monitor"}, {"metadata": {}, "cell_type": "markdown", "source": "Kick off a fairness monitor run on current data. The monitor runs hourly, but can be manually initiated using the Python client, the REST API, or the graphical user interface."}, {"metadata": {"scrolled": true, "tags": []}, "cell_type": "code", "source": "run_details = wos_client.monitor_instances.run(monitor_instance_id=fairness_monitor_instance_id, background_mode=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wos_client.monitor_instances.show_metrics(monitor_instance_id=fairness_monitor_instance_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Run drift monitor\n\n\nKick off a drift monitor run on current data. The monitor runs every hour, but can be manually initiated using the Python client, the REST API."}, {"metadata": {"tags": []}, "cell_type": "code", "source": "drift_run_details = wos_client.monitor_instances.run(monitor_instance_id=drift_monitor_instance_id, background_mode=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wos_client.monitor_instances.show_metrics(monitor_instance_id=drift_monitor_instance_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Configure Explainability"}, {"metadata": {}, "cell_type": "markdown", "source": "Finally, we provide OpenScale with the training data to enable and configure the explainability features."}, {"metadata": {"tags": []}, "cell_type": "code", "source": "target = Target(\n    target_type=TargetTypes.SUBSCRIPTION,\n    target_id=subscription_id\n)\nparameters = {\n    \"enabled\": True\n}\nexplainability_details = wos_client.monitor_instances.create(\n    data_mart_id=data_mart_id,\n    background_mode=False,\n    monitor_definition_id=wos_client.monitor_definitions.MONITORS.EXPLAINABILITY.ID,\n    target=target,\n    parameters=parameters\n).result\n\nexplainability_monitor_id = explainability_details.metadata.id", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Run explanation for sample record"}, {"metadata": {"tags": []}, "cell_type": "code", "source": "pl_records_resp = wos_client.data_sets.get_list_of_records(data_set_id=payload_data_set_id, limit=1, offset=0).result\nscoring_ids = [pl_records_resp[\"records\"][0][\"entity\"][\"values\"][\"scoring_id\"]]\nprint(\"Running explanations on scoring IDs: {}\".format(scoring_ids))\nexplanation_types = [\"lime\", \"contrastive\"]\nresult = wos_client.monitor_instances.explanation_tasks(scoring_ids=scoring_ids, explanation_types=explanation_types).result\nprint(result)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Custom monitors and metrics <a name=\"custom\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "## Register custom monitor"}, {"metadata": {}, "cell_type": "code", "source": "def get_definition(monitor_name):\n    monitor_definitions = wos_client.monitor_definitions.list().result.monitor_definitions\n    \n    for definition in monitor_definitions:\n        if monitor_name == definition.entity.name:\n            return definition\n    \n    return None", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "monitor_name = 'my model performance'\nmetrics = [MonitorMetricRequest(name='sensitivity',\n                                thresholds=[MetricThreshold(type=MetricThresholdTypes.LOWER_LIMIT, default=0.8)]),\n          MonitorMetricRequest(name='specificity',\n                                thresholds=[MetricThreshold(type=MetricThresholdTypes.LOWER_LIMIT, default=0.75)])]\ntags = [MonitorTagRequest(name='region', description='customer geographical region')]\n\nexisting_definition = get_definition(monitor_name)\n\nif existing_definition is None:\n    custom_monitor_details = wos_client.monitor_definitions.add(name=monitor_name, metrics=metrics, tags=tags, background_mode=False).result\nelse:\n    custom_monitor_details = existing_definition", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Show available monitors types"}, {"metadata": {}, "cell_type": "code", "source": "wos_client.monitor_definitions.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Get monitors uids and details"}, {"metadata": {"tags": []}, "cell_type": "code", "source": "custom_monitor_id = custom_monitor_details.metadata.id\n\nprint(custom_monitor_id)", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true, "tags": []}, "cell_type": "code", "source": "custom_monitor_details = wos_client.monitor_definitions.get(monitor_definition_id=custom_monitor_id).result\nprint('Monitor definition details:', custom_monitor_details)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Enable custom monitor for subscription"}, {"metadata": {"tags": []}, "cell_type": "code", "source": "target = Target(\n        target_type=TargetTypes.SUBSCRIPTION,\n        target_id=subscription_id\n    )\n\nthresholds = [MetricThresholdOverride(metric_id='sensitivity', type = MetricThresholdTypes.LOWER_LIMIT, value=0.9)]\n\ncustom_monitor_instance_details = wos_client.monitor_instances.create(\n            data_mart_id=data_mart_id,\n            background_mode=False,\n            monitor_definition_id=custom_monitor_id,\n            target=target\n).result", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Get monitor instance id and configuration details"}, {"metadata": {}, "cell_type": "code", "source": "custom_monitor_instance_id = custom_monitor_instance_details.metadata.id", "execution_count": null, "outputs": []}, {"metadata": {"tags": []}, "cell_type": "code", "source": "custom_monitor_instance_details = wos_client.monitor_instances.get(custom_monitor_instance_id).result\nprint(custom_monitor_instance_details)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Storing custom metrics"}, {"metadata": {"tags": []}, "cell_type": "code", "source": "from datetime import datetime, timezone, timedelta\nfrom ibm_watson_openscale.base_classes.watson_open_scale_v2 import MonitorMeasurementRequest\ncustom_monitoring_run_id = \"11122223333111abc\"\nmeasurement_request = [MonitorMeasurementRequest(timestamp=datetime.now(timezone.utc), \n                                                 metrics=[{\"specificity\": 0.78, \"sensitivity\": 0.67, \"region\": \"us-south\"}], run_id=custom_monitoring_run_id)]\nprint(measurement_request[0])", "execution_count": null, "outputs": []}, {"metadata": {"tags": []}, "cell_type": "code", "source": "published_measurement_response = wos_client.monitor_instances.measurements.add(\n    monitor_instance_id=custom_monitor_instance_id,\n    monitor_measurement_request=measurement_request).result\npublished_measurement_id = published_measurement_response[0][\"measurement_id\"]\nprint(published_measurement_response)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### List and get custom metrics"}, {"metadata": {"scrolled": true, "tags": []}, "cell_type": "code", "source": "time.sleep(5)\npublished_measurement = wos_client.monitor_instances.measurements.get(monitor_instance_id=custom_monitor_instance_id, measurement_id=published_measurement_id).result\nprint(published_measurement)", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true, "tags": []}, "cell_type": "code", "source": "print('Datamart:', data_mart_id)\nprint('Model:', model_uid)\nprint('Deployment:', deployment_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Identify transactions for Explainability"}, {"metadata": {}, "cell_type": "markdown", "source": "Transaction IDs identified by the cells below can be copied and pasted into the Explainability tab of the OpenScale dashboard."}, {"metadata": {}, "cell_type": "code", "source": "wos_client.data_sets.show_records(data_set_id=payload_data_set_id, limit=5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"summary\"></a>\n##  Summary and next steps     "}, {"metadata": {}, "cell_type": "markdown", "source": "You successfully completed this notebook! \n \nYou have finished the hands-on lab for IBM Watson OpenScale. You can now view the [OpenScale Dashboard](https://aiopenscale.cloud.ibm.com/). Click on the tile for the German Credit model to see fairness, accuracy, and performance monitors. Click on the timeseries graph to get detailed information on transactions during a specific time window.\n\n\nOpenScale shows model performance over time. You have two options to keep data flowing to your OpenScale graphs:\n  * Download, configure and schedule the [model feed notebook](https://raw.githubusercontent.com/emartensibm/german-credit/master/german_credit_scoring_feed.ipynb). This notebook can be set up with your WML credentials, and scheduled to provide a consistent flow of scoring requests to your model, which will appear in your OpenScale monitors.\n  * Re-run this notebook. Running this notebook from the beginning will delete and re-create the model and deployment, and re-create the historical data. Please note that the payload and measurement logs for the previous deployment will continue to be stored in your datamart, and can be deleted if necessary. You can use this notebooks cells to delete deployments.\n\n\nCheck out our <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-service-instance.html?context=analytics\" target=\"_blank\" rel=\"noopener noreferrer\">Online Documentation</a> for more samples, tutorials, documentation, how-tos, and blog posts. "}, {"metadata": {}, "cell_type": "markdown", "source": "## Authors\n\nLukasz Cmielowski, PhD, is an Automation Architect and Data Scientist at IBM with a track record of developing enterprise-level applications that substantially increases clients' ability to turn data into actionable knowledge.\n\nSzymon Kucharczyk, Software Engineer at IBM Watson Machine Learning."}, {"metadata": {}, "cell_type": "markdown", "source": "Copyright \u00a9 2020 IBM. This notebook and its source code are released under the terms of the MIT License."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}
