{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "<div style=\"background:#F5F7FA; height:100px; padding: 2em; font-size:14px;\">\n<span style=\"font-size:18px;color:#152935;\">Want to do more?</span><span style=\"border: 1px solid #3d70b2;padding: 15px;float:right;margin-right:40px; color:#3d70b2; \"><a href=\"https://www.ibm.com/products/watson-studio\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n<span style=\"color:#5A6872;\"> Try out this notebook with your free trial of IBM Watson Studio.</span>\n</div>"}, {"metadata": {}, "cell_type": "markdown", "source": "$\\newcommand\\innerprod[2]{\\langle #1, #2 \\rangle}$\n$\\newcommand\\vect[1]{\\mathbf #1}$\n$\\newcommand{\\mR}{\\vect{R}}$\n$\\newcommand{\\vf}{\\vect{f}}$\n$\\newcommand{\\vg}{\\vect{g}}$\n$\\newcommand\\Tex{}$\n$\\newcommand\\norm[2][\\Tnorm]{{\\left\\|#2\\right\\|}_{#1}}$\n$\\newcommand\\PR[1]{\\mathrm{P}\\left[#1\\right]}$\n$\\newcommand{\\reals}{\\mathbb R}$"}, {"metadata": {}, "cell_type": "markdown", "source": "# Overlapping co-CLuster Recommendation algorithm (OCuLaR)\n"}, {"metadata": {}, "cell_type": "markdown", "source": "This notebook explains how the `OCuLaR` algorithm is working and how it can be implemented. It then shows how to prepare a real-world data set to be fed to the algorithm. Last, but not least, we evaluate the performance of `OCuLaR` by computing the recall.\n\nSome familiarity with Python is recommended. This notebook runs on Python."}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "## Table of contents\n\n\n**Part A: [The algorithm](#Part-A:-The-algorithm)**\n  1. [Introduction](#1.-Introduction)<br>\n  2. [Illustrative example](#2.-Illustrative-example)<br>\n  3. [Generative model](#3.-Generative-model)<br>\n  4. [Fitting the model parameters](#4.-Fitting-the-model-parameters)<br>\n    4.1 [Computing the $L$ and $Q$](#4.1-Computing-the-$L$-and-$Q$)<br>\n  5. [Implementation and complexity](#5.-Implementation-and-complexity)<br>\n    5.1 [Alternative: Vectorized DQfi/DQfu](#5.1-Alternative:-vectorized-DQfi/DQfu)<br>\n    5.2 [Unit tests](#5.2-Unit-tests)<br>\n  6. [Ocular fit on the example](#6.-Ocular-fit-on-the-example)\n  \n  \n  \n**Part B: [Ocular with MovieLens](#Part-B:-Ocular-with-MovieLens)**\n  1. [Load the MovieLens data](#1.-Load-the-MovieLens-data)<br>\n    1.1 [Download the data](#1.1-Download-the-data)<br>\n    1.2 [Load the data file to the notebook](#1.2-Load-the-data-file-to-the-notebook)<br>\n  2. [Prepare the data](#2.-Prepare-the-data)<br>\n    2.1 [Select positive ratings](#2.1-Select-positive-ratings)<br>\n    2.2 [Remove \"bad\" users and items](#2.2-Remove-%22bad%22-users-and-items)<br>\n    2.3 [Split data](#2.3-Split-data)<br>\n    2.4 [Get list of active users/items](#2.4-Get-list-of-active-users/items)<br>\n    2.5 [Get user/item history](#2.5-Get-user/item-history)<br>\n  3. [The selection of the step size](#3.-The-selection-of-the-step-size)<br>\n  4. [Recall](#4.-Recall)<br>\n  5. [Summary](#5.-Summary)"}, {"metadata": {}, "cell_type": "markdown", "source": "## Part A: The algorithm\n\nIn this first, part we describe the algorithm and a basic implementation of it.\n\nReferences:\n1. Reinhard Heckel, Vasileios Vasileiadis, Michail Vlachos. Method and system for identifying dependent components, US9524468. <a href= \"https://patents.google.com/patent/US9524468B2/en\" target=\"_blank\" rel=\"noopener noreferrer\">[link]</a>\n2. Reinhard Heckel, Michail Vlachos, Thomas Parnell, Celestine Duenner. Scalable and Interpretable Product Recommendations via Overlapping Co-Clustering, IEEE 33rd International Conference on Data Engineering (ICDE), 2017, pp. 1033-1044. <a href= \"http://ieeexplore.ieee.org/document/7930045/\" target=\"_blank\" rel=\"noopener noreferrer\">[link]</a>\n3. Michail Vlachos, Vassilios G. Vassiliadis, Reinhard Heckel, Abdel Labbi. Toward interpretable predictive models in B2B recommender systems. IBM Journal of Research and Development (60), 2017, pp. 11:1-11:12. <a href= \"http://ieeexplore.ieee.org/document/7580713/\" target=\"_blank\" rel=\"noopener noreferrer\">[link]</a>\n\n## 1. Introduction\nWe assume that we are given a matrix $\\mR$ where the rows correspond, e.g., to users or clients and the columns to items or products. \nIf the $(u,i)$th element of $\\mR$ takes on the value $r_{ui} =1$ this indicates that user $u$ has purchased item $i$ in the past or, more generally, that user $u$ is interested in item $i$. \nWe consider all values $r_{ui}$ that are not positive ($r_{ui} =1$) as unknown ($r_{ui}=0$) because they indicate that user $u$ might be interested in $i$ or not. \nOur goal is to identify those items a user $u$ is likely to be interested in.\nPut differently, we want to find the positives among the unknowns, given only positive examples.\n\nWe assume an underlying model whose parameters are factors associated with the users and items. Those factors are learned, such that the fitted model explains well the given positive examples $r_{ui}=1$."}, {"metadata": {}, "cell_type": "markdown", "source": "## 2. Illustrative example\nIn the next figure, we provide an example of overlapping co-clusters. A dark square describes a product bought by the user in the past. One can visually identify three potential recommendations indicated by white squares inside the co-clusters."}, {"metadata": {}, "cell_type": "code", "source": "%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef plot_input(_r, _users, _items, minor_ticks, cmap=\"hot\", title=\"ratings\"):\n    fig, ax = plt.subplots()\n    cax = ax.imshow(_r, cmap=cmap)\n    ax.set_title(title)\n    ax.set_xlabel('items')\n    ax.set_xticks(minor_ticks, minor=True)\n    ax.set_yticks(minor_ticks, minor=True)\n    ax.set_ylabel('users')\n    ax.set_xticks(range(len(_items)))\n    ax.set_xticklabels(_items)\n    ax.set_yticks(range(len(_users)))\n    ax.set_yticklabels(_users)\n    ax.grid(color='grey', which='minor', linestyle='-', linewidth=0.5)\n    return fig, ax, cax", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# the user ids\nusers = list(range(1, 13))\n# the item ids\nitems = list(range(1, 13))\n\nratings = \\\n    [\n        (1, 4), (1, 5), (1, 6), (1, 7),\n        (2, 4), (2, 5), (2, 6),\n        (3, 4), (3, 5), (3, 6), (3, 7),\n        (5, 2), (5, 3), (5, 4), (5, 5),\n        (6, 2), (6, 3), (6, 4), (6, 5),\n        (7, 2), (7, 3), (7, 4), (7, 6), (7, 7), (7, 8), (7, 9), (7, 10),\n        (8, 5), (8, 6), (8, 7), (8, 8), (8, 9), (8, 10),\n        (9, 5), (9, 6), (9, 7), (9, 8), (9, 10),\n        (10, 5), (10, 6), (10, 7), (10, 8), (10, 9), (10, 10)\n    ]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from scipy.sparse import csr_matrix\nrow = [users.index(user) for (user, item) in ratings]\ncolumn = [items.index(item) for (user, item) in ratings]\ndata = [1] * len(row)\nr_c = csr_matrix((data, (row, column)), shape=(len(users), len(items)))\n\nfig, ax, cax = plot_input(-r_c.toarray(), users, items, np.arange(0.5, 11.5, 1))\nimport matplotlib.patches as mpatches\nrect = mpatches.Rectangle([2.5, -0.5], 4, 3, ec=\"red\", fc=\"none\",  ls = '-', lw=2, fill=False, alpha=1.0)\npp = ax.add_patch(rect)\nrect = mpatches.Rectangle([0.5, 3.5], 4, 3, ec=\"red\", fc=\"none\",  ls = '-', lw=2, fill=False, alpha=1.0)\npp = ax.add_patch(rect)\nrect = mpatches.Rectangle([3.5, 5.5], 6, 4, ec=\"red\", fc=\"none\",  ls = '-', lw=2, fill=False, alpha=1.0)\npp = ax.add_patch(rect)\ndel pp", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We continue with the initial users, items, and ratings that we defined above. For the rest of the analysis, we keep only the active users/items. "}, {"metadata": {}, "cell_type": "code", "source": "active_users = list(set([user for (user, item) in ratings]))\nactive_items = list(set([item for (user, item) in ratings]))\n\nitems_copy = items[:]\nfor item in items_copy:\n    if item not in active_items:\n        items.remove(item)\n\nusers_copy = users[:]\nfor user in users_copy:\n    if user not in active_users:\n        users.remove(user)\n\ndel users_copy, items_copy\n\nrow = [users.index(user) for (user, item) in ratings]\n\ncolumn = [items.index(item) for (user, item) in ratings]\n\ndata = [1] * len(row)\nr_c = csr_matrix((data, (row, column)), shape=(len(users), len(items)))\ndel row, data, column\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 3. Generative model\n\nWe start with the generative model underlying our recommendation approach. \nIt formalizes the following intuition: There exist clusters, groups, or communities of users that are interested in a subset of the items.\n\nAs users can have several interests, and items might satisfy several needs, each user and item can belong to several co-clusters consisting of users and items. \nHowever, a co-cluster must contain at least one user and one item, and can therefore not consist of users or items alone. \n\nSuppose there are $K$ co-clusters ($K$ can be determined from the data, e.g., by cross-validation, as discussed later). \nAffiliation of a user $u$ and item $i$ with a co-cluster is modeled by the $K$-dimensional co-cluster affiliation vectors $\\vf_u$ and $\\vf_i$,  respectively. \nThe entries of $\\vf_u, \\vf_i$ are constrained to be non-negative, and  $[\\vf_u]_c = 0$ signifies that user $u$ does not belong to co-cluster $c$. Here, $[\\vf]_c$ denotes the $c$-th entry of $\\vf$. \nThe absolute value of $[\\vf_u]_c$ corresponds to the affiliation strength of $u$ with co-cluster $c$; the larger it is, the stronger the affiliation. \n\nPositive examples are explained by the co-clusters as follows. If user $u$ and item $i$ both lie in co-cluster $c$, then this co-cluster generates a positive example with probability \n\n$$\n1 - e^{-[\\vf_u]_c [\\vf_i]_c }. \n$$\n\nAssuming that each co-cluster $c=1,...,K$, generates a positive example independently, it follows that \n\n$$ 1 - \\PR{r_{ui} = 1}  = \\prod_c e^{-[\\vf_u]_c [\\vf_i]_c } = e^{- \\innerprod{\\vf_u}{\\vf_i}}, $$\nwhere $\\innerprod{\\vf}{\\vg} = \\sum_c [\\vf]_c [\\vg]_c$ denotes the inner product in $\\reals^K$. Thus \n\n$$\\PR{r_{ui} = 1} = 1 - e^{- \\innerprod{\\vf_u}{\\vf_i}}. $$\n\nA similar generative model also appears in the community detection literature."}, {"metadata": {}, "cell_type": "markdown", "source": "## 4. Fitting the model parameters"}, {"metadata": {}, "cell_type": "markdown", "source": "Given a matrix $\\mR$, we fit the model parameters by finding the most likely factors  $\\vf_u,\\vf_i$ to the matrix $\\mR$ by maximizing the likelihood  (recall that we assume positive examples to be generated independently across co-clusters and across items and users in co-clusters):\n\n$$ L = \n\\prod_{(u,i)\\colon r_{ui}=1}\n( 1 - e^{ -\\innerprod{\\vf_u}{ \\vf_i} } )\n\\prod_{(u,i)\\colon r_{ui}=0}\ne^{ -\\innerprod{\\vf_u}{ \\vf_i} }. \n$$\n\nMaximizing the likelihood is equivalent to minimizing the negative log-likelihood:\n\n$$\n-\\log L \n%= - \\log \\PR{r_{ui} | \\mF}\n= - \\sum_{(u,i)\\colon r_{ui} = 1}\n\\log( 1 - e^{ -\\innerprod{\\vf_u}{ \\vf_i} } )\n+ \\sum_{(u,i) \\colon r_{ui}=0 }  \\innerprod{\\vf_u}{ \\vf_i}.\n$$\n\nTo prevent overfitting, we add an $\\ell_2$ penalty, which results in the following optimization problem:\n\n$$\n\\text{minimize }Q \\text { subject to } [\\vf_{u}]_c, [\\vf_{i}]_c  \\geq 0, \\text{ for all } c,\n$$\n\nwhere \n\n\\begin{align}\nQ = -\\log L + \\lambda \\sum_i \\norm[2]{\\vf_i}^2  + \\lambda \\sum_u \\norm[2]{\\vf_u}^2 \n\\end{align}\n\nand $\\lambda\\geq 0$ is a regularization parameter.\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.1 Computing the $L$ and $Q$\n\nBefore $L$ and $Q$ are computed, the user and item histories are created from the sparse matrix. Both are dictionaries, with the user history listing all index positions of the items that a user has purchased, while the item history lists all index positions of users that have purchased this specific item."}, {"metadata": {}, "cell_type": "code", "source": "indexes_of_ones = r_c.nonzero()\nindexes_of_ones = set(zip(indexes_of_ones[0], indexes_of_ones[1]))\n\ndef input_parser(indexes_of_ones):\n    user_c = dict()\n    item_c = dict()\n    for (user, item) in indexes_of_ones:\n        if user not in user_c:\n            user_c[user] = [item]\n        else: \n            user_c[user].append(item)\n\n        if item not in item_c:\n            item_c[item] = [user]\n        else: \n            item_c[item].append(user)        \n    return user_c, item_c\n\nuser_history, item_history = input_parser(indexes_of_ones)\nprint(user_history, item_history)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# the likehood function\ndef L(indexes_of_ones, fu, fi):\n    import itertools\n    product_ones = 1\n    product_zeros = 1\n\n    for (u, i) in itertools.product(range(fu.shape[0]), range(fi.shape[0])):\n        if (u, i) in indexes_of_ones:\n            product_ones *= (1 - np.exp(-np.inner(fu[u], fi[i])))\n        else:\n            product_zeros *= np.exp(-np.inner(fu[u], fi[i]))\n            \n    return product_ones * product_zeros", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# the Penalized log likehood function\ndef Q(indexes_of_ones, fu, fi, lam):\n    return -np.log(L(indexes_of_ones, fu, fi)) + lam * sum(np.linalg.norm(fu, axis=1)**2) + lam * sum(np.linalg.norm(fi, axis=1)**2)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# the Penalized log likehood function (from the sums)\ndef Q2(indexes_of_ones, fu, fi, lam):\n    import itertools\n    sum_ones = 0\n    sum_zeros = 0\n\n    for (u, i) in itertools.product(range(fu.shape[0]), range(fi.shape[0])):\n        if (u, i) in indexes_of_ones:\n            sum_ones += np.log(1 - np.exp(-np.inner(fu[u], fi[i])))\n        else:\n            sum_zeros += np.inner(fu[u], fi[i])\n            \n    return - sum_ones + sum_zeros + lam * sum(np.linalg.norm(fu, axis=1)**2) + lam * sum(np.linalg.norm(fi, axis=1)**2)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "A common approach to solve an NMF problem is alternating least squares, which iterates between fixing $\\vf_u$, and minimizing with respect to $\\vf_i$, and fixing $\\vf_i$ and minimizing with respect to $\\vf_u$, until convergence. \nThis strategy is known as cyclic block coordinate descent or the non-linear Gauss-Seidel method. \nWhereas $Q$ is non-convex in  $\\vf_i,\\vf_u$, $Q$ is convex in  $\\vf_i$ (with  $\\vf_u$ fixed) and convex in  $\\vf_u$ (with $\\vf_i$ fixed). \nTherefore, a solution to the subproblems of minimizing $Q$ with fixed $\\vf_i$ and minimizing $Q$ with fixed $\\vf_u$ can be found, e.g., via gradient descent or Newton's method. \nAs this optimization problem is non-convex, one cannot in general guarantee convergence to a global minimum; however, convergence to a stationary point can be ensured. \nSpecifically, provided that $\\lambda > 0$, $Q$ is strongly convex in  $\\vf_i$ (with  $\\vf_u$ fixed) and in $\\vf_u$ (with $\\vf_i$ fixed). \nThus, the subproblems have unique solutions, and therefore, if we solve each subproblem exactly, convergence to a stationary point is ensured.\n\nHowever, as noted in the context of matrix factorization, solving the subproblems exactly may slow down convergence. \nSpecifically, when $\\vf_u,\\vf_i$ are far from a stationary point, it is intuitive that there is little reason to allocate computational resources to solve the subproblems exactly. \nIt is therefore often more efficient to solve the subproblem only approximately in each iteration. \n\nFor the above reasons, we will only approximately solve each subproblem by using a single step of projected gradient descent with backtracking line search, and iteratively update $\\vf_i$ and $\\vf_u$ by single projected gradient descent steps until convergence. Convergence is declared if $Q$ stops decreasing. This results in a very efficient algorithm that is essentially linear in the number of positive examples $\\{(u,i)\\colon r_{ui} = 1\\}$, and the number of co-clusters $K$. Our simulations have shown that performing only one gradient descent step significantly speeds up the algorithm. "}, {"metadata": {}, "cell_type": "markdown", "source": "## 5. Implementation and complexity\n\nHere we examine in more detail the projected gradient descent approach we use to solve the subproblems and the complexity of the overall optimization algorithm. It is sufficient to discuss minimization of $Q$ with respect to $\\vf_i$, as minimization with respect to $\\vf_u$ is equivalent. \n\nWe start by noting that, because of  \n\n$$\nQ = \n\\sum_{i} \\left( \n- \\sum_{u \\colon r_{ui} = 1}\n\\log( 1 - e^{- \\innerprod{\\vf_u}{ \\vf_i} }  ) \n+ \\sum_{u \\colon r_{ui} = 0}  \\innerprod{\\vf_u}{ \\vf_i}  \\right) + \\lambda \\sum_{u} \\norm[2]{\\vf_u}^2 + \\lambda \\sum_i \\norm[2]{\\vf_i}^2,\n$$\n\nwe can minimize $Q$ for each $\\vf_i$ individually. The part of $Q$ depending on  $\\vf_i$ is given by\n\n$$\nQ(\\vf_i)\n=\n- \\!\\!\\!\\!\\!\\sum_{u\\colon r_{ui} = 1 }\n\\log( 1 - e^{- \\innerprod{\\vf_u}{ \\vf_i} }  )\n\\!+\\!  \\innerprod{\\vf_i }{  \\sum_{u \\colon r_{ui} = 0 }  \\vf_u }  \\!+\\! \\lambda \\norm[2]{\\vf_i}^2.\n$$\n\n"}, {"metadata": {}, "cell_type": "code", "source": "# the Penalized log likehood function (from the Qfi)\ndef Qfi(i, indexes_of_ones, fu, fi, lam):\n    import itertools\n    sum_ones = 0\n    sum_zeros = 0\n\n    for u in range(fu.shape[0]):\n        if (u, i) in indexes_of_ones:\n            sum_ones += np.log(1 - np.exp(-np.inner(fu[u], fi[i]) - 5e-8))\n        else:\n            sum_zeros += np.inner(fu[u], fi[i])\n            \n    return - sum_ones + sum_zeros + lam * np.linalg.norm(fi[i])**2\n\n\ndef Qfu(u, indexes_of_ones, fu, fi, lam):\n    import itertools\n    sum_ones = 0\n    sum_zeros = 0\n\n    for i in range(fi.shape[0]):\n        if (u, i) in indexes_of_ones:\n            sum_ones += np.log(1 - np.exp(-np.inner(fu[u], fi[i]) - 5e-8))\n        else:\n            sum_zeros += np.inner(fu[u], fi[i])\n            \n    return - sum_ones + sum_zeros + lam * np.linalg.norm(fu[u])**2\n\n\ndef Q3(indexes_of_ones, fu, fi, lam):\n    q = 0 \n    \n    for i in range(fi.shape[0]):\n        q += Qfi(i, indexes_of_ones, fu, fi, lam)\n        \n    return q + lam * sum(np.linalg.norm(fu, axis=1)**2)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "As mentioned, we update the parameter $\\vf_i$ by performing a projected gradient descent step. \nThe projected gradient descent algorithm is initialized with a feasible initial factor $\\vf_i^{0}$ and updates the current solution $\\vf^k_i$ to $\\vf^{k+1}_i$ \naccording to  \n\n$$\n\\vf^{k+1}_i = (\\vf^{k}_i - \\alpha_k   \\nabla Q(\\vf^k_i) )_+,\n$$\nwhere $(\\vf)_+$ projects $\\vf$ on its positive part, $[(\\vf)_+]_c = \\max(0, [\\vf]_c )$, and the gradient is given by\n\n$$\n\\nabla Q( \\vf_i ) \n= \n-\\sum_{u\\colon r_{ui} = 1}  \\vf_u \\frac{e^{- \\innerprod{\\vf_u}{ \\vf_i } }}{1 -e^{-  \\innerprod{\\vf_u}{ \\vf_i} }} \n+ \\sum_{u\\colon r_{ui} = 0} \\vf_u\n+ 2 \\lambda \\vf_i.\n$$\n\nAs the computation of both $\\nabla Q(\\vf_i)$ and $Q(\\vf_i)$ requires $\\sum_{u \\colon r_{ui} = 0} \\vf_u$, and typically, the number of items for which $r_{ui}=1$ is small relative to the total number of items, we precompute $\\sum_u \\vf_u $\nbefore updating all $\\vf_i$, and then compute $\\sum_{u \\colon r_{ui} = 0} \\vf_u$ via \n\n$$\n\\sum_{u \\colon r_{ui} = 0} \\vf_u = \\sum_u \\vf_u  - \\sum_{u \\colon r_{ui} = 1} \\vf_u. \n$$\n\nand this results in\n\n$$\n\\nabla Q( \\vf_i ) \n= \n-\\sum_{u\\colon r_{ui} = 1}  \\vf_u \\frac{1}{1 -e^{-  \\innerprod{\\vf_u}{ \\vf_i} }} \n+ \\sum_{u} \\vf_u\n+ 2 \\lambda \\vf_i.\n$$\n\nSimilarly, \n\n$$\n\\nabla Q( \\vf_u ) \n= \n-\\sum_{i\\colon r_{ui} = 1}  \\vf_i \\frac{1}{1 -e^{-  \\innerprod{\\vf_u}{ \\vf_i} }} \n+ \\sum_{i} \\vf_i\n+ 2 \\lambda \\vf_u.\n$$\n\nand \n\n$$\n\\vf^{k+1}_u = (\\vf^{k}_u - \\alpha_k   \\nabla Q(\\vf^k_u) )_+,\n$$\n\nUsing this approach, a gradient descent step of updating $\\vf_i$ has cost $O(|\\{ u \\colon r_{ui}=1\\}|   K )$. \nThus, updating all $\\vf_i$ and all $\\vf_u$ has cost $O(|\\{(i,u)\\colon r_{ui}=1\\}|  K )$, which means that updating all factors has a cost that is linear in the problem size (i.e., number of positive examples) and linear in the number of co-clusters."}, {"metadata": {}, "cell_type": "code", "source": "def Qfi_ones(item_i_history, fu, fi_i, sfu, lam):\n    sum_ones = 0\n\n    for u in item_i_history:\n        inner = np.inner(fu[u], fi_i)\n        sum_ones += np.log(1 - np.exp(-inner - 5e-8)) + inner\n            \n    return - sum_ones + np.inner(fi_i, sfu) + lam * np.linalg.norm(fi_i)**2\n\n\ndef Qfu_ones(user_u_history, fu_u, fi, sfi, lam):\n    sum_ones = 0\n\n    for i in user_u_history:\n        inner = np.inner(fu_u, fi[i])\n        sum_ones += np.log(1 - np.exp(-inner - 5e-8)) + inner\n            \n    return - sum_ones + np.inner(fu_u, sfi) + lam * np.linalg.norm(fu_u)**2\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def DQfi(item_i_history, fu, fi_i, sfu, lam):\n    sum_ones = np.zeros(sfu.size) \n    \n    for u in item_i_history: \n        sum_ones += (1 / (1 - np.exp(-np.inner(fu[u], fi_i) - 5e-8))) * fu[u]\n\n    return - sum_ones + sfu + 2 * lam * fi_i\n\n\ndef DQfu(user_u_history, fu_u, fi, sfi, lam):\n    sum_ones = np.zeros(sfi.size) \n    \n    for i in user_u_history:\n        sum_ones += (1 / (1 - np.exp(-np.inner(fu_u, fi[i]) - 5e-8)))* fi[i]\n                \n    return - sum_ones + sfi + 2 * lam * fu_u\n\n\ndef fi_next(ifi_i, item_i_history, fu, sfu, lam, a):\n    d = DQfi(item_i_history[ifi_i[0]], fu, ifi_i[1:], sfu, lam)\n    return np.maximum(1e-8, ifi_i[1:] - a * d)\n\n\ndef fu_next(ufu_u, user_u_history, fi, sfi, lam, a):\n    d = DQfu(user_u_history[ufu_u[0]], ufu_u[1:], fi, sfi, lam)\n    return np.maximum(1e-8, ufu_u[1:] - a * d) ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.1 Alternative: vectorized DQfi/DQfu\n\nThe vectorized version avoids the for-loop to compute the `DQfi`/`DQfu` and uses a pure numpy solution instead."}, {"metadata": {}, "cell_type": "code", "source": "def DQfi_v(features, fu, fi_i, sfu, lam):\n    return - (fu[features].T * (1 / (1 - np.exp(-np.dot(fu[features], fi_i) - 5e-8)))).sum(axis=1).T + sfu + 2 * lam * fi_i\n\n\ndef DQfu_v(features, fu_u, fi, sfi, lam):\n    return - (fi[features].T * (1 / (1 - np.exp(-np.dot(fi[features], fu_u) - 5e-8)))).sum(axis=1).T + sfi + 2 * lam * fu_u \n\n\ndef fi_next_v(ifi_i, item_i_history, fu, sfu, lam, a):\n    d = DQfi_v(item_i_history[ifi_i[0]], fu, ifi_i[1:], sfu, lam)\n    return np.maximum(1e-8, ifi_i[1:] - a * d)\n\n\ndef fu_next_v(ufu_u, user_u_history, fi, sfi, lam, a):\n    d = DQfu_v(user_u_history[ufu_u[0]], ufu_u[1:], fi, sfi, lam)\n    return np.maximum(1e-8, ufu_u[1:] - a * d)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# some cleaning before we start\ndef delete_vars(variables):\n    for var in variables:\n        if var in globals():\n            del globals()[var]\n\ndelete_vars(['active_items', 'active_users', 'item', 'user', 'mpatches', 'rect', 'fig', 'ax'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.2 Unit tests\n\nTo ensure that the functions we created so far are working as expected, we created unit tests to test them."}, {"metadata": {}, "cell_type": "code", "source": "import unittest\nclass Tests(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super(Tests, self).__init__(*args, **kwargs)\n        self.lam = 1\n        self.k = 3\n        self.indexes_of_ones = set([                (0, 2), (0, 3), (0, 4), (0, 5), \n                                                    (1, 2), (1, 3), (1, 4), \n                                                    (2, 2), (2, 3), (2, 4), (2, 5),\n                                    (3, 0), (3, 1), (3, 2), (3, 3), \n                                    (4, 0), (4, 1), (4, 2), (4, 3), \n                                    (5, 0), (5, 1), (5, 2),         (5, 4), (5, 5), (5, 6), (5, 7), (5, 8), \n                                                            (6, 3), (6, 4), (6, 5), (6, 6), (6, 7), (6, 8), \n                                                            (7, 3), (7, 4), (7, 5), (7, 6), (7, 8), \n                                                            (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (8, 8)])\n        self.user_history, self.item_history = input_parser(self.indexes_of_ones)\n        self.NU = 9\n        self.NI = 9\n        self.a = 0.5\n        self.fu = np.ones((self.NU, self.k))/2.0\n        self.fi = np.ones((self.NI, self.k))/2.0\n        self.sfu = np.sum(self.fu, axis=0)\n        self.sfi = np.sum(self.fi, axis=0)\n        self.expected_L = (1-np.exp(-3/4.0))**44 * np.exp(-3/4.0)**37\n        self.expected_Q = -np.log(self.expected_L) + self.lam * (3/4.0) * 9 + self.lam * (3/4.0) * 9\n        self.expexted_Qfi_0 = 3*(-np.log((1-np.exp(-3/4.0))) + 7/4.0)\n        self.expected_DQfi_0 = 11/2.0 - (3 / (1 - np.exp(-3/4.0)))/2.0\n        self.expected_DQfi_3 = 11/2.0 - (8 / (1 - np.exp(-3/4.0)))/2.0\n        self.expected_next_fi_0 = max(0, -9/4.0 + (3 / (1 - np.exp(-3/4.0)))/4.0)\n        self.expected_next_fi_3 = max(0, -9/4.0 + (8 / (1 - np.exp(-3/4.0)))/4.0)\n        self.expected_next_fu_5 = max(0, -9/4.0 + (8 / (1 - np.exp(-3/4.0)))/4.0)\n    \n    def test_L(self):\n        L1 = L(self.indexes_of_ones, self.fu, self.fi)\n        self.assertAlmostEqual(L1, self.expected_L, places=5)\n    \n    def test_Qfi(self):\n        Qfi_0 = Qfi(0, self.indexes_of_ones, self.fu, self.fi, self.lam)\n        self.assertAlmostEqual(Qfi_0, self.expexted_Qfi_0, places=5)\n        \n    def test_Qfi_ones(self):\n        Qfi_0 = Qfi_ones(self.item_history[0], self.fu, self.fi[0], self.sfu, self.lam)\n        self.assertAlmostEqual(Qfi_0, self.expexted_Qfi_0, places=5)    \n    \n    def test_Q(self):\n        Q1 = Q(self.indexes_of_ones, self.fu, self.fi, self.lam)\n        self.assertAlmostEqual(Q1, self.expected_Q, places=5)\n\n    def test_Q2(self):      \n        Q1_2 = Q2(self.indexes_of_ones, self.fu, self.fi, self.lam)\n        self.assertAlmostEqual(Q1_2, self.expected_Q, places=5)\n        \n    def test_Q3(self):\n        Q1_3 = Q3(self.indexes_of_ones, self.fu, self.fi, self.lam)        \n        self.assertAlmostEqual(Q1_3, self.expected_Q, places=5)\n        \n    def test_DQfi_0(self):\n        DQfi_0 =  DQfi_v(self.item_history[0], self.fu, self.fi[0], self.sfu, self.lam)\n        for i in DQfi_0:\n            self.assertAlmostEqual(i, self.expected_DQfi_0, places=5) \n            \n    def test_DQfi_3(self):\n        DQfi_3 =  DQfi_v(self.item_history[3], self.fu, self.fi[3], self.sfu, self.lam)\n        for i in DQfi_3:\n            self.assertAlmostEqual(i, self.expected_DQfi_3, places=5)             \n            \n    def test_next_fi_0(self):\n        next_fi_0 = fi_next_v(np.array([0, 0.5, 0.5, 0.5]), self.item_history, self.fu, self.sfu, self.lam, self.a)\n        for i in next_fi_0:\n            self.assertAlmostEqual(i, self.expected_next_fi_0, places=5)\n            \n    def test_next_fi_3(self):\n        next_fi_3 = fi_next_v(np.array([3, .5, 0.5, 0.5]), self.item_history, self.fu, self.sfu, self.lam, self.a)\n        for i in next_fi_3:\n            self.assertAlmostEqual(i, self.expected_next_fi_3, places=5)\n            \n    def test_next_fu_5(self):\n        next_fu_5 = fu_next_v(np.array([5, 0.5, 0.5, 0.5]), self.user_history, self.fi, self.sfi, self.lam, self.a)\n        for i in next_fu_5:\n            self.assertAlmostEqual(i, self.expected_next_fu_5, places=5) \n            \n    def test_next_fi_apply_along(self):            \n        fi_next_1= np.apply_along_axis(fi_next_v, 1, np.c_[np.arange(self.fi.shape[0]), self.fi], self.item_history, \n                                        self.fu, self.sfu, self.lam, self.a)\n        for i in fi_next_1[0,:]:\n            self.assertAlmostEqual(i, self.expected_next_fi_0, places=5)\n        for i in fi_next_1[3,:]:\n            self.assertAlmostEqual(i, self.expected_next_fi_3, places=5)\n            \n    def test_steps_fi_0(self):\n        fi_0 = np.array([0.5, 0.5, 0.5])\n        next_fi_0 = fi_next_v(np.array([0, 0.5, 0.5, 0.5]), self.item_history, self.fu, self.sfu, self.lam, self.a)\n        DQfi_0 = DQfi_v(self.item_history[0], self.fu, self.fi[0], self.sfu, self.lam)\n        Qfi_0 = Qfi(0, self.indexes_of_ones, self.fu, self.fi, self.lam)\n        next_Qfi_0 = Qfi(0, self.indexes_of_ones, self.fu,  np.apply_along_axis(fi_next_v, 1, np.c_[np.arange(self.fi.shape[0]), self.fi], self.item_history, self.fu, self.sfu, self.lam, self.a), self.lam)\n        \nsuite = unittest.TestLoader().loadTestsFromTestCase(Tests)\nu = unittest.TextTestRunner().run(suite)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 6. Ocular fit on the example\n\nThe parameters that need to be set for the fit are:\n- `k`: number of co-clusters, \n- `max_it`: number of iterations of the for-loop,\n- `lam`: regularization parameter,\n- `a`: stepsize,\n- `fu` and `fi`: need to be initialized with random values,\n- `sfi` and `sfu`: sums of the initial `fi` and `fu`."}, {"metadata": {}, "cell_type": "code", "source": "rnd = np.random.RandomState(seed=123456789)\nk = 3\nmax_it = 20\nlam = 0.2\nfu = rnd.rand(len(users), k).astype('float64')\nfi = rnd.rand(len(items), k).astype('float64')\nsfu = np.sum(fu, axis=0)\nsfi = np.sum(fi, axis=0)\na = 0.05\niterrate_item = False\n\nfor i in range(max_it):\n    iterrate_item = not iterrate_item\n    if iterrate_item:\n        fi = np.apply_along_axis(fi_next_v, 1, np.c_[np.arange(fi.shape[0]), fi], item_history, fu, sfu, lam, a)\n        sfi = np.sum(fi, axis=0)\n    else:\n        fu = np.apply_along_axis(fu_next_v, 1, np.c_[np.arange(fu.shape[0]), fu], user_history, fi, sfi, lam, a)\n        sfu = np.sum(fu, axis=0)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The following code block prints out the probabilities for each user/item combination that a user would be interested in it, before plotting the co-clusters that were identified along with the probabilities. In the heatmap, you can see that the area where the co-clusters overlap has a very high probability while in the areas where no co-clusters were identified the values are very low."}, {"metadata": {}, "cell_type": "code", "source": "fig, ax, cax = plot_input(-r_c.toarray(), list(set(map(lambda l: users[l[0]], indexes_of_ones))), list(set(map(lambda l: items[l[1]], indexes_of_ones))), np.arange(0.5, 9.5, 1))\nfig, ax, cax = plot_input(1 - np.exp(-np.dot(fu, fi.T)), list(set(map(lambda l: users[l[0]], indexes_of_ones))), list(set(map(lambda l: items[l[1]], indexes_of_ones))), np.arange(0.5, 9.5, 1), \n                          cmap='coolwarm', title='Prob[r_{ui} = 1]')\ncbar = fig.colorbar(cax, ticks=[0.0001, 0.92])\ncbar.ax.set_yticklabels(['Low Probability', 'High Probability'])\nprint(1 - np.exp(-np.dot(fu, fi.T)))\n\nimport matplotlib.patches as mpatches\nrect = mpatches.Rectangle([1.5, -0.5], 4, 3, ec=\"black\", fc=\"none\",  ls = '--', lw=2, fill=False, alpha=1.0)\npp = ax.add_patch(rect)\nrect = mpatches.Rectangle([-0.5, 2.5], 4, 3, ec=\"black\", fc=\"none\",  ls = '--', lw=2, fill=False, alpha=1.0)\npp = ax.add_patch(rect)\nrect = mpatches.Rectangle([2.5, 4.5], 6, 4, ec=\"black\", fc=\"none\",  ls = '--', lw=2, fill=False, alpha=1.0)\npp = ax.add_patch(rect)\ndel pp", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Let's clean up by deleting some of the variables that are not needed anymore, before continuing with the second part of the notebook."}, {"metadata": {}, "cell_type": "code", "source": "delete_vars(['it', 'iterrate_item', 'suite', 'u', 'rect', 'pp', 'fig', 'ax', 'cax', 'mpatches', 'cbar'])\n%whos", "execution_count": null, "outputs": []}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "## Part B: Ocular with MovieLens\n\nLet's apply this to a real-world data set. The <a href= \"https://movielens.org\" target=\"_blank\" rel=\"noopener noreferrer\">MovieLens</a> project is run by <a href= \"http://grouplens.org/\" target=\"_blank\" rel=\"noopener noreferrer\">GroupLens</a> at the University of Minnesota and offers non-commercial, personalized movie recommendations. They also released several data sets to support people developing recommender algorithms. \n\nIn this notebook, you will work with the small dataset of 100,000 ratings of 9,000 movies that were provided by 700 users."}, {"metadata": {}, "cell_type": "markdown", "source": "## 1. Load the MovieLens data\n### 1.1 Download the data\n\nTo download the data:\n\n1. Download `ml-latest-small.zip` from <a href=\"https://grouplens.org/datasets/movielens/latest/\" target=\"_blank\" rel=\"noopener noreferrer\">MovieLens Latest DataSets</a>.\n1. Unzip the folder. The file you'll be using is `ratings.csv`."}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.2 Load the data-file to the notebook\n\nTo load the ratings.csv file into a Spark DataFrame:\n\n1. Click the **Find and Add Data** button on the notebook action bar (the 1001 button - second from the right in the navigation bar). Drag and drop the file into the box or click `browse` to select the file from your directory. The file is loaded to your object storage and also appears in the **Data Assets** section of your project.\n1. To load the data into your notebook, make sure you have the following code cell selected. In the files-section on the right-hand side, you can find **Insert to code** underneath the filename. If you click it, a dropdown list with different options to insert the data will appear. Select **Insert Pandas DataFrame** and it will automatically insert the code to create a Pandas DataFrame to the selected code cell. As the code contains credentials, the cell will be hidden when sharing the notebook.\n1. Change the name of the DataFrame to `ratings` in the last two lines."}, {"metadata": {}, "cell_type": "code", "source": "# SELECT this cell when inserting the dataframe!\n# RENAME the data frame to ratings\nimport os, types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_cdd1a4f11a7243449a739fd7851745b5 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='r0KQgAMKJDtX0el0dzIX0_4y7R0TiTFgzE7GXg_Q8GKf',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.private.us.cloud-object-storage.appdomain.cloud')\n\nbody = client_cdd1a4f11a7243449a739fd7851745b5.get_object(Bucket='py39-donotdelete-pr-awttqtqg1nzgv4',Key='ratings.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\nratings = pd.read_csv(body)\nratings.head()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2. Prepare the data\n\nIn this part, you will learn how to prepare the data so that it can be fed to the `OCuLaR` algorithm.\n\n### 2.1 Select positive ratings"}, {"metadata": {}, "cell_type": "markdown", "source": "`OCuLaR` was developed to get recommendations from data that only provides information on what items a user is interested in, e.g. a purchase history. To imitate the original input data, we will only select positive ratings with a score of 3 or above and treat them equally to a purchased item by a user. Negative ratings will be treated equally to movies that have not been rated by the user."}, {"metadata": {}, "cell_type": "code", "source": "positive_ratings_frame = ratings[ratings['rating'] >= 3]\npositive_ratings_frame.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.2 Remove \"bad\" users and items\n\nIn this case, \"bad\" refers to users who only did one rating and movies that only received one rating."}, {"metadata": {}, "cell_type": "code", "source": "# removing bad users\npositive_ratings_frame = positive_ratings_frame.groupby('userId').filter(lambda x: len(x) > 1)\n\n# removing bad items\npositive_ratings_frame = positive_ratings_frame.groupby('movieId').filter(lambda x: len(x) > 1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.3 Split data\n\nWe split the data using the `train_test_split` in the `sklearn` package. If you use a newer version of `sklearn` than 0.17, you will need to import the function from `sklearn.model_selection`. "}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.model_selection import train_test_split\n\ntrain_data_frame,test_data_frame = train_test_split(positive_ratings_frame, test_size=0.2, random_state=123456789)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The remaining data preparation part is similar to the preparation that you have already seen in part A. At first, only the active users/items are selected, before creating two dictionaries containing the history of each user respectively each item.\n\n### 2.4 Get list of active users/items"}, {"metadata": {}, "cell_type": "code", "source": "ratings_train = train_data_frame.values.tolist()\nratings_test = test_data_frame.values.tolist()\n\nactive_users = sorted(list(set(map(lambda x: x[0], ratings_train))))\nactive_items = sorted(list(set(map(lambda x: x[1], ratings_train))))\nactive_users_test = sorted(list(set(map(lambda x: x[0], ratings_test))))\nactive_items_test = sorted(list(set(map(lambda x: x[1], ratings_test))))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.5 Get user/item history"}, {"metadata": {}, "cell_type": "code", "source": "users_d = dict(zip(active_users, range(len(active_users))))\nitems_d = dict(zip(active_items, range(len(active_items))))\n\nrow = []\ncolumn = []\n\nfor r in ratings_train:\n    row.append(users_d[r[0]])\n    column.append(items_d[r[1]])\n    \ndata = [1] * len(row)\nr_c = csr_matrix((data, (row, column)), shape=(len(active_users), len(active_items)))\ndel row, column\n\nindexes_of_ones = r_c.nonzero()\nindexes_of_ones = set(zip(indexes_of_ones[0], indexes_of_ones[1]))\n\nuser_history, item_history = input_parser(indexes_of_ones)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We repeat this process for the test set, as we will need the user/item histories to compute the recall later."}, {"metadata": {}, "cell_type": "code", "source": "users_d_test = dict(zip(active_users_test, range(len(active_users_test))))\nitems_d_test = dict(zip(active_items_test, range(len(active_items_test))))\n\nrow = []\ncolumn = []\n\nfor r in ratings_test:\n    row.append(users_d_test[r[0]])\n    column.append(items_d_test[r[1]])\n    \ndata = [1] * len(row)    \nr_c_test = csr_matrix((data, (row, column)), shape=(len(active_users_test), len(active_items_test)))\ndel row, column\n\nindexes_of_ones_test = r_c_test.nonzero()\nindexes_of_ones_test = set(zip(indexes_of_ones_test[0], indexes_of_ones_test[1]))\n\nuser_history_test, item_history_test = input_parser(indexes_of_ones_test)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 3. The selection of the step size\nThe step size $\\alpha_k$ is selected using a backtracking line search, also referred to as the Armijo rule along the projection arc. \nSpecifically, $\\alpha_k = \\beta^{t_k}$, where $t_k$ is the smallest positive integer such that \n\n$$\nQ(\\vf^{k+1}_i) - Q(\\vf^k_i)\n\\leq\n\\sigma \\innerprod{\\nabla Q(\\vf^k_i)}{ \\vf^{k+1}_i  - \\vf^k_i }\n$$\n\nwhere $\\sigma,\\beta \\in (0,1)$ are user-set constants. "}, {"metadata": {}, "cell_type": "markdown", "source": "In addition to the parameters already used in part A, we need `sigma` and `lsParam` for the line search. `M_rec` is the maximum number of items that will be recommended to a user. This variable becomes relevant when computing the recall in the next step."}, {"metadata": {}, "cell_type": "code", "source": "# initialize the parameters and the fs\nrnd = np.random.RandomState(seed=123456789)\nk = 1000\nlam = 600\nmax_it = 20\nfu = rnd.rand(len(active_users), k)\nfi = rnd.rand(len(active_items), k)\nsfu = np.sum(fu, axis=0)\nsfi = np.sum(fi, axis=0)\nsigma = 0.01\nlsParam = 5e-3\nM_rec = 130", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The affiliation strength is computed individually for each user and item inside the while loop that iterates until the right step size has been found. In the last line, the probabilities are computed from the results."}, {"metadata": {}, "cell_type": "code", "source": "for it in range(max_it):\n    # items\n    for i in range(fi.shape[0]):\n        a = 1\n        active = True\n        d = DQfi_v(item_history[i], fu, fi[i], sfu, lam)\n        old_cost = Qfi_ones(item_history[i], fu, fi[i], sfu, lam)\n        old_fi = fi[i,:].copy()\n        \n        while active:     \n            fi_new = fi_next_v(np.append([i], old_fi), item_history, fu, sfu, lam, a)\n            fi[i,:] = fi_new\n            RHS = sigma * np.inner(d, (fi_new - old_fi))\n            new_cost = Qfi_ones(item_history[i], fu, fi[i], sfu, lam)\n            active = new_cost - old_cost > RHS + lsParam\n            a *= 0.1\n            \n    sfi = np.sum(fi, axis=0)\n\n    # users\n    for u in range(fu.shape[0]):\n        a = 1\n        active = True\n        d = DQfu_v(user_history[u], fu[u], fi, sfi, lam)\n        old_cost = Qfu_ones(user_history[u], fu[u], fi, sfi, lam)\n        old_fu = fu[u,:].copy()   \n        \n        while active:\n            fu_new = fu_next_v(np.append([u], old_fu), user_history, fi, sfi, lam, a)\n            fu[u,:] = fu_new\n            RHS = sigma * np.inner(d, (fu_new - old_fu))\n            new_cost = Qfu_ones(user_history[u], fu[u], fi, sfi, lam)\n            active = new_cost - old_cost > RHS + lsParam\n            a *= 0.1\n \n    sfu = np.sum(fu, axis=0)\n\n# Computes the probabilities\nprob = 1 - np.exp(-np.dot(fu, fi.T))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 4. Recall\n\nTo evaluate the performance of Ocular, we compute the recall at M with M being the number of recommended items. As `OCuLaR` was developed for a one-class setting, we treat negative ratings equally to movies without ratings. That means we do not know that a user is not interested in a movie just because he did not rate it. For these cases recall is the more suitable method to evaluate the recommendations than precision."}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "recall = dict()\n\n# get common users\nfor u in users_d_test:\n    if u in users_d:\n        test_user_index = users_d_test[u]\n        train_user_index = users_d[u]\n\n        # movies rated by user\n        rated_movie_ind = user_history_test[test_user_index]\n        rated_movie_IDs = [list(items_d_test.keys())[list(items_d_test.values()).index(i)] for i in rated_movie_ind]\n        all_positives = len(rated_movie_ind)\n\n        # recommendations for user\n        top_recs_ind = np.argsort(-prob)[train_user_index, :]\n\n        for m in range(10, M_rec + 10, 10):\n            exclude_zeros = min(m, sum(prob[train_user_index, :] > 0))\n            rec_ind = top_recs_ind[:exclude_zeros]\n            rec_movie_IDs = [list(items_d.keys())[list(items_d.values()).index(i)] for i in rec_ind]\n\n            # movies that were recommended and rated\n            true_pos = len([rmi for rmi in rec_movie_IDs if rmi in rated_movie_IDs])\n            \n            # recall for user\n            recall_user = float(true_pos)/float(all_positives)\n            \n            # append to dict to get recall for each M\n            if m not in recall:\n                recall[m] = [recall_user]\n            else:\n                recall[m] = recall[m] + [recall_user]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "To get an overview of the results, we plot the recall in a boxplot using Matplotlib."}, {"metadata": {}, "cell_type": "code", "source": "%matplotlib inline\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nlabels = sorted(recall.keys())\ndata = [recall[m] for m in labels]\n\nfig, ax1 = plt.subplots(figsize=(14, 8))\nplt.boxplot(data, showmeans=True)\nax1.set_xticklabels(labels)\nax1.yaxis.grid(True, linestyle='-', which='major', color='lightgrey', alpha=0.5)\n# Hide these grid behind plot objects\nax1.set_axisbelow(True)\nax1.set_title('Ocular with MovieLens')\nax1.set_xlabel('M')\nax1.set_ylabel('recall')\n\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 5. Summary\n\nIn this notebook, you learned how the `OCuLaR` algorithm works and how you can implement it. The second part showed you how to prepare your data so you can use the `OCuLaR` algorithms to create recommendations and evaluate its performance using recall. You can now apply this to your own data sets and get recommendations with the `OCuLaR` algorithm."}, {"metadata": {}, "cell_type": "markdown", "source": "### Authors\n\n**Vasileios Vasileiadis** is with the Cognitive Systems group at IBM Research - Zurich. He received his Ph.D. degree from Democritus University of Thrace, Greece. His research interests focus on statistical signal processing, data mining, and recommender systems.\n\n**Michail Vlachos** is a Research Staff Member in the Cognitive Systems group at the IBM Research - Zurich. He received his Ph.D. from University of California, Riverside, and his M.B.A. from University of Illinois at Urbana-Champaign. He has been the principal investigator of the ERC Grant on \"Exact Mining from InExact Data\". His research interests are in the areas of data mining, machine learning, and information retrieval.\n\n**Thomas Parnell** received his Ph.D. in mathematics from the University of Warwick. U.K. In 2007, he co-founded Siglead Europe, where he was involved in developing signal processing and coding algorithms for HDD and flash storage technologies. In 2013, he joined IBM Research - Zurich, where he is involved in developing advanced controller technology for the next-generation of IBM FlashSystem products. His research interests include signal processing, information theory, and machine learning.\n\n**Celestine D\u00fcnner** is a predoctoral researcher in the Analytics Infrastructure group at IBM Research - Zurich, and a Ph.D. candidate in Computer Science at the Swiss Federal Institute of Technology (ETH), where she is a member of the ETH data analytics laboratory led by professor Thomas Hofmann. Her current research interests focus on improving the interface between system and algorithm design for scalable and efficient machine learning applications.\n\n**Kathrin Wardatzky** is doing a master's degree with a focus on Information Science at the Hamburg University of Applied Sciences (HAW Hamburg), Germany. During her internship in the Cognitive Systems group at IBM Research - Zurich, she worked on porting the Ocular algorithm to the Watson Studio environment."}, {"metadata": {}, "cell_type": "markdown", "source": "### Data Citation\n\nF. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=<a href=\"http://dx.doi.org/10.1145/2827872\" target=\"_blank\" rel=\"noopener noreferrer\">10.1145/2827872</a>"}, {"metadata": {}, "cell_type": "markdown", "source": "\n<hr>\nCopyright &copy; IBM Corp. 2017-2024. This notebook and its source code are released under the terms of the MIT License."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}
