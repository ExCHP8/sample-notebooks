{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "\n# Analyze energy consumption in buildings\n\nThis Python notebook shows you how to use analytics to determine the factors that contibute to energy inefficiency in buildings and to help develop strategies to reduce energy consumption and greenhouse gas emissions. \n\nThis notebook runs on Python with Spark.\n\nYou can learn more about statistical modeling and optimization methods that can be used to optimize energy consumption in buildings in <a href=\"http://www.informs-sim.org/wsc11papers/082.pdf\" target=\"_blank\" rel=\"noopener noreferrer\" >this article</a>. The article was written by a group of researchers from the following institutions: \n-  IBM T.J. Watson Research Center \n-  CUNY Institute for Urban Systems\n-  McMaster University. \n\n\n## Table of contents\n\n- [Get data](#get_data)\n- [Load data](#load_data)\n- [Access data](#access_data)\n- [Load libraries](#load_libraries)\n- [Model energy usage of buildings in kWh](#Model-energy-usage-in-kWh-of-buildings)\n  - [Prepare the data](#data_preparation)\n  - [Fit a linear regression model to the observed data](#linear_regression)\n  - [Visualize model accuracy](#visualize_model_accuracy)\n- [Detect buildings that consume energy inefficiently](#buildings_inefficient)\n  - [Consider all metrics when measuring energy inefficiencies](#consider_all_metrics)\n- [Export data and models to RStudio](#Export-data-and-models-to-RStudio)\n- [Summary](#summary)\n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"get_data\"></a>\n## Get data\n\nThe notebook uses data sets created by BlocPower, a startup based in New York, which supports clean energy projects in the city. The data sets contain information on property names, property sizes, and their respective energy usage in kWh. By using  different metrics, such as plug-load consumption, air conditioning consumption, and usage of domestic gas and heating gas, you will learn how to use analytics to identify buildings that consume energy inefficiently.\n\nTo get the energy comsumption data sets:\n\n1. Go to the <a href=\"https://github.com/IBMDataScience/SparkSummitDemo/tree/master/data\" target=\"_blank\" rel=\"noopener noreferrer\">BlocPower data sets</a> and then download the following data sets in CSV format:\n    * BlocPower_T.csv\n    * HDD-Features.csv\n    * CDD-HDD-Features.csv \n2. Save the CSV files to your computer.\n\n<a id=\"load_data\"></a>\n## Load data \n\nAfter you have saved the files to your local file system, load the data files to use in the notebook:\n\n1. On the notebook action bar, click the **Data** icon.\n2. Click **Add Source**, select **From file**, and browse to and load the following CSV files:\n    * BlocPower_T.csv\n    * HDD-Features.csv\n    * CDD-HDD-Features.csv   \n\nThe data files are are stored in the Object Storage that is associated with your project."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"access_data\"></a>\n## Access data\n\nTo add the code to access the data files in Object Storage, click the **Code snippets** icon, select **Read data**, locate the data asset in your project, and then load it as a **sparkSessionDataFrame** in the empty cell below. Repeat these actions for each data file that you loaded.\n\nThis function inserts the setup code for the preconfigured `SparkSession`. Then the data is loaded into a `SparkSession DataFrame`. The credentials for accessing the CSV file are included in the generated code.  "}, {"metadata": {}, "cell_type": "markdown", "source": "Note: The `SparkSession DataFrames` that are created and loaded with data for you are given generic names. Give the DataFrame variables the following names:\n\n- `df2` for the BlocPower_t.csv file\n- `dfHDD` for the HDD-Features.csv file\n- `dfCH` for the CDD-HDD-Features.csv file\n\nThen, run each code cell to proceed."}, {"metadata": {}, "cell_type": "code", "source": "# Use Insert to code > SparkSession DataFrame to for BlocPower_t.csv file, and rename the data frame to df2.\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Use Insert to code > SparkSession DataFrame to for HDD-Features.csv file, and rename the data frame to dfHDD.\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Use Insert to code > SparkSession DataFrame to for CDD-HDD-Features.csv file, and rename the data frame to dfCH.\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"load_libraries\"></a>\n## Load libraries\n\nThe Spark and Python libraries that you need are preinstalled in the notebook environment and only need to be loaded.\n\nRun the following cell to load the libraries you will work with in this notebook:"}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np\nimport pandas as pd\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import DoubleType", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"Model-energy-usage-in-kWh-of-buildings\"></a>\n## Model energy usage of buildings in kWh\n\nNow that your data is in memory, you can begin to explore and manipulate the data. By examining this data, you can understand patterns of energy usage and heat transfer as well as characteristics of building structures,  operations,  and  occupant  behavior  that  influence  energy consumption. Begin by inspecting the first few rows of the BlocPower data. Each row has the following columns:\n\n| Column Name                | Column Description                                                                    |\n| -------------------------- |:-------------------------------------------------------------------------------------:| \n| UTSUM_Electricity_Usage    | Shows the annual energy usage of each building in kWh                                 |\n| INFO_Year of Construction  | Shows the year the building was constructed                                           |\n| INFO_Number of Stories     | Shows the number of stories of each building                                          |\n| INFO_Total Square Feet     | Shows the the square footage of each building                                         | \n| PLEI_1_Quantity            | Shows the number of plugged-in equipment in each building, for example, microwaves and computers                       |\n| PLEI_3_Quantity            | Shows the number of plugged-in equipment in each building, similar to column  PLEI_1_Quantity                        |\n\n\n<a id=\"data_preparation\"></a>\n### Prepare the data \n\nTo better consume the BlocPower data, it must be reformatted. The values in the first column, which show electricity usage, must be converted to actual numbers, for example, 117,870 kWh to 117870. In the fourth column, which shows the total square footage of each building, the commas must be removed from the values. You will notice too that there are some missing values.\n\nRun the next cell to show the first five rows of the BlocPower data:"}, {"metadata": {}, "cell_type": "code", "source": "df2.show(5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Run the following cell to define different functions that you can run to clean and prepare the data. For example, define functions to convert kWh values to actual numbers, to calculate the age of a building based on the year the building was constructed, to remove commas in returned values, and to interpret missing values in the PLEI columns as meaning that there is no plugged-in equipment."}, {"metadata": {}, "cell_type": "code", "source": "# define cleaning functions\ndef energy(v): # reformat the values to get an actual number (e.g., 117,870 kWh to 117870)\n    if (v=='') or v==None: return np.nan\n    v = v.split(' ')[0].replace(',','')\n    return np.nan if(v=='') else float(v)\ndef age(v): # computes the age of a buildings, given the year of construction\n    if (v=='') or v==None: return np.nan\n    v = v.encode('ascii','ignore')\n    return 2019.0-float(v) if(len(v)==4) else np.nan\ndef stories(v):\n    if (v=='') or v==None: return np.nan\n    return float(v)\ndef sqFeet(v): # get rid of commas \n    if (v=='') or v==None: return np.nan\n    v = v.replace(',','')\n    return np.nan if(v=='') else float(v) \ndef plei(v): # in the PLEI columns, missing values can be interpeted as 0 plugged equipment\n    try:\n        vv = float(v)\n    except:\n        vv = 0.0\n    return vv \n# Define udf's to apply the defined function to the Spark DataFrame\nudfEnergy = udf(energy, DoubleType())\nudfAge = udf(age, DoubleType())\nudfStories = udf(stories, DoubleType())\nudfSqFeet = udf(sqFeet, DoubleType())\nudfPlei = udf(plei, DoubleType())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Apply the data preparation functions, which you defined, to the data, and load the cleansed data to a `pandas` DataFrame:"}, {"metadata": {}, "cell_type": "code", "source": "dfN = df2.withColumn(\"UTSUM_Electricity_Usage\", udfEnergy(\"UTSUM_Electricity_Usage\")) \\\n         .withColumn(\"INFO_Year of Construction\", udfAge(\"INFO_Year of Construction\")) \\\n         .withColumn(\"INFO_Number of Stories\", udfStories(\"INFO_Number of Stories\")) \\\n         .withColumn(\"INFO_Total Square Feet\", udfSqFeet(\"INFO_Total Square Feet\")) \\\n         .withColumn(\"PLEI_1_Quantity\", udfPlei(\"PLEI_1_Quantity\")) \\\n         .withColumn(\"PLEI_3_Quantity\", udfPlei(\"PLEI_3_Quantity\")).cache()\ndfN = dfN.withColumnRenamed(\"UTSUM_Electricity_Usage\",\"energy\") \\\n           .withColumnRenamed(\"INFO_Year of Construction\",\"age\") \\\n           .withColumnRenamed(\"INFO_Number of Stories\",\"number_stories\") \\\n           .withColumnRenamed(\"INFO_Total Square Feet\",\"square_feet\") \\\n           .withColumnRenamed(\"PLEI_1_Quantity\",\"plei_1\") \\\n           .withColumnRenamed(\"PLEI_3_Quantity\",\"plei_3\")            ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Then compute the average of all the existing values in the energy consumption and buiding age columns, and insert these averages in the rows of the energy comsumption and building age columns that are missing values:"}, {"metadata": {}, "cell_type": "code", "source": "dfN.take(10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# compute average of non-missing energy and age\nenergy_mean = np.nanmean(np.asarray(dfN.select(\"energy\").rdd.map(lambda r: r[0]).collect()))\nage_mean = np.nanmean(np.asarray(dfN.select(\"age\").rdd.map(lambda r: r[0]).collect()))\n# fill missing values with the computed average\ndfN = dfN.na.fill({\"energy\": energy_mean, \"age\": age_mean})", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# define Spark DataFrame to be written to our object store\ndfOut = dfN.select('energy', 'age', 'number_stories','square_feet','plei_1','plei_3')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now run the following cell to form a feature matrix and scale the columns:"}, {"metadata": {}, "cell_type": "code", "source": "# use the .toPandas() function to map Spark DataFrames to pandas DataFrames\ndfNp = dfN.toPandas()\ndfHDDp = dfHDD.toPandas()\n# concatenate two pandas DataFrames\nfeat = pd.concat([dfNp, dfHDDp], axis=1)\n# get the column names of the concatenated DataFrame\ncols = feat.columns\n# scale data to prepare for regression model \nfrom sklearn import preprocessing\nscaler = preprocessing.MaxAbsScaler() \nfeat = scaler.fit_transform(feat)\n# define a new DataFrame with the scaled data\ndfScaled = pd.DataFrame(feat,columns=cols)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Plot the results and explore the correlations:"}, {"metadata": {}, "cell_type": "code", "source": "import matplotlib.pyplot as plt\n%matplotlib inline\n# import the scatter_matrix functionality\nfrom pandas.plotting import scatter_matrix\n\nplt.style.use('bmh')\nff = scatter_matrix(dfScaled, diagonal='hist',figsize=(12,12))\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"linear_regression\"></a>\n### Fit a linear regression model to the observed data\n\nLinear regression attempts to model the relationship between two variables by fitting a linear equation to the observed data.\nThe energy usage (kWh) in buildings can be explained by considering the following building characteristics: \n+ Age of the building \n+ Square feet \n+ Number of stories \n+ Total number of plugged equipment\n\nThe regression modeler takes those building characteristics, which you prepared and scaled in an earlier step, and calculates the association between the proposed predicted value and the observed energy usage of the building."}, {"metadata": {}, "cell_type": "code", "source": "# get a list of the features used to explain energy\nfeatures = dfScaled.columns.tolist()\nresponse = ['energy']\nfeatures.remove(response[0])\n# import regression solver\nfrom sklearn import linear_model\n# declare a linear regression model \nlr = linear_model.LinearRegression(fit_intercept=True)\n# define response variable: energy usage\ny = np.asarray(dfScaled[response]) \n# define features\nX = dfScaled[features]\n# fit regression model to the data\nregr = lr.fit(X,y)\ncoefs = regr.coef_[0]\n# collect regression coefficients\ndataRegQ = []\ndataRegQ.append(('Intercept', regr.intercept_[0]))\nfor i in range(len(features)):\n    dataRegQ.append((features[i],coefs[i]))\n# compute energy predictions using our fitted model     \nyh = regr.predict(X)\n# import package to compute the R-squared quality metric\nfrom sklearn.metrics import r2_score\n# print results\nprint ('R-Squared: ', r2_score(y,yh))\npd.DataFrame(dataRegQ,columns=['feature_name','coefficient']) #.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"visualize_model_accuracy\"></a>\n### Visualize model accuracy\n\nA scatter plot can be a helpful tool in determining the strength of the relationship between two variables.\n\nRun the following cell to create a scatter plot.\n\nThe blue dots represent the observed energy usage versus the energy usage predicted by the trained model. The black dotted line is at 45 degrees and represents the perfect model. The closer the blue dots are to the black dotted line, the better the model fits the data.\n"}, {"metadata": {}, "cell_type": "code", "source": "fig, ax = plt.subplots()\nax.scatter(y, yh)\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\nax.set_xlabel('Energy usage observed',fontsize=20)\nax.set_ylabel('Energy usage predicted',fontsize=20)\nax.axis([-0.1, 1.1, -0.1, 1.1])\nplt.gcf().set_size_inches( (6, 6) )\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"buildings_inefficient\"></a>\n## Detect buildings that consume energy inefficiently\n\nIn this section, you will learn how to use engineered characteristics of heating and cooling systems in buildings to detect those buildings that consume energy inefficiently. Factors that influence high energy consumption include:\n+ Air conditioning\n+ High usage of plugged-in equipment, for example, microwaves, computers, refrigerators, and freezers\n+ Use of gas for domestic usage\n+ Use of gas for heating purposes\n\nRun the following cell to prepare the DataFrames to enable detecting and extracting those factors from the data set:"}, {"metadata": {}, "cell_type": "code", "source": "dfCH = dfCH.toPandas()\n# get the numerical features\ndfR = dfCH[['plug_load_consumption','ac_consumption','domestic_gas','heating_gas']]\n# scale features with the max value of each column\ndfR=dfR.apply(pd.to_numeric)\ndfN = dfR/dfR.max()\n# concatenate scaled features and buildings name\ndfCH_n = pd.concat((dfN, dfCH['Property Name']),1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now run the following cell to only identify the buildings with inefficient air conditioning systems:"}, {"metadata": {}, "cell_type": "code", "source": "cname = 'ac_consumption' #'heating_gas'\ndfCH_n[dfCH_n[cname]>dfCH_n.quantile(0.95)[0]][['Property Name',cname]]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"consider_all_metrics\"></a>\n### Consider all metrics when measuring energy inefficiencies\n\nUp until this point in the notebook, you have used the air conditioning, plugged-in equipment, domestic gas, and heating gas metrics for analytics processing in isolation. By using k-means, you can train your model to consider all of the metrics combined. k-means clustering helps you identify groups of buildings that consume energy inefficiently.  \n\nRun the following cell to import the k-means libraries, prepare the input data set, and define clusters of data in the input data set (groups of buildings) that are similar to one another:\n"}, {"metadata": {}, "cell_type": "code", "source": "# import K-means and PCA library\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n# declare PCA model with 4 components\npca = PCA(n_components=2)\npca.fit(dfN)\nfeatReduced = pca.fit_transform(dfN)\n# declare a K-means model with 4 clusters \nkmeans = KMeans(n_clusters=4, tol=0.00001, random_state=1)\n# run K-means with our data\nkmeans.fit(featReduced)\n# get the label for each building using the K-means model\nlabels = kmeans.predict(featReduced)\nbuildings = np.asarray(dfCH['Property Name'].values.tolist())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The clustering model helps you to identify inefficient groups of buildings. Visualize the clusters by using two of the four dimensions and the k-means labels to color the observations:"}, {"metadata": {}, "cell_type": "code", "source": "# inefficiency dimensions: ['plug_load_consumption','ac_consumption','domestic_gas','heating_gas']\nx = dfCH_n['plug_load_consumption'] \ny = dfCH_n['heating_gas'] \nplt.scatter(x, y, s=225, c=labels, alpha=0.5)\nplt.gcf().set_size_inches( (7, 7) )\nplt.xlabel('Plugged-in equipment inefficiency',fontsize=15)\nplt.ylabel('Heating gas inefficiency',fontsize=15)\ntt = 'Buildings colored by cluster labels' \nplt.title(tt,fontsize=15)\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The resulting scatter plot shows that most buildings are part of the purple cluster. The buildings with a brown, yellow, or light blue label are more energy inefficient than the ones that are part of the purple cluster.  "}, {"metadata": {}, "cell_type": "markdown", "source": "Now recode the color labels into a binary variable where 1 is inefficient (the brown, yellow or light blue clusters) and 0 otherwise (the purple cluster):"}, {"metadata": {}, "cell_type": "code", "source": "# define binary variable to identify inefficient buildings\nlabel_binary = []\nfor v in labels:\n    label_binary.append(0 if (v == 0) else 1)\nlabel_binary = np.asarray(label_binary)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now we split the data into training nd testing:"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.model_selection import train_test_split\nfeat_tr, feat_te, lab_tr, lab_te = train_test_split(featReduced, label_binary, train_size=0.6, random_state=1337)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Then run the following cell to train a classifier as well as show its accuracy of identifying inefficient buildings:"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.ensemble import RandomForestClassifier\n# declare a random forest model \nrfc = RandomForestClassifier(n_estimators=100)\n# fit model with our data\nrfc.fit(feat_tr, lab_tr)\n# compute accuracy of the trained model\naccuracy = rfc.score(feat_te, lab_te)\n# compute predictions using trained model\ny_pred = rfc.predict(feat_te)\nprint (\"Model Accuracy: \", accuracy)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Use a confusion matrix to analyze, summarize, and plot the results of the logistic regression model. A confusion metrix  reports the actual and predicted results of the regression model and thus allows more detailed analysis than calculating the model accuracy based on predictions only.\n\nRun the following cell to define the parameters of the confusion matrix:"}, {"metadata": {}, "cell_type": "code", "source": "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(2)\n    plt.xticks(tick_marks, ['efficient','inefficient'], rotation=0)\n    plt.yticks(tick_marks, ['efficient','inefficient'])\n    plt.tight_layout()\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Then create the confusion matrix. In the first row of the confusion matrix, you will notice that the logistic regression model is very good at predicting efficient buildings; however the second row shows that the model is not very accurate at identifying inefficient buildings. The model predicted that only 8 of the 22 buildings were inefficient."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "# import confusion matrix to explore model accuracy\nfrom sklearn.metrics import confusion_matrix\n# compute confusion matrix\ncm = confusion_matrix(lab_te, y_pred)\nnp.set_printoptions(precision=2)\nprint('Confusion matrix, without normalization')\nprint(cm)\nplt.figure()\nplot_confusion_matrix(cm)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"export_rstudio\"></a>\n## Export data and models to RStudio\n\nIf you want to create an interactive web application (a Shiny app) to visualize and share the results of your data analysis in RStudio, perform the following steps:\n\n1. In Watson Studio, open RStudio from the left navigation bar.\n2. Create a new R Markdown document. You might need to download some R packages.\n3. Copy the <a href=\"https://raw.githubusercontent.com/IBMDataScience/SparkSummitDemo/master/shinyDemo.Rmd\" target=\"_blank\" rel=\"noopener noreferrer\" >raw R code</a>, and replace the default content in the new R Markdown document by pasting the code into the file.\n5. Select lines 21 - 65, and run them to install all necessary packages.\n6. Click **Knit**, which is the button with the ball of yarn next to it.\n7. Click **Open in Browser** to interact with the app, and share the link with other users.\n"}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "<a id=\"summary\"></a>\n## Summary\n\nIn this notebook, you learned how to prepare and cleanse data to use in a linear regression model and how to visualize the accuracy of this model. You learned how to detect building energy inefficiencies by using individual metrics as well as how to use k-means to combine building metrics. You learned the difference between visualizing the results by using a scatter plot and a confusion matrix. In addition, you learned how to create an interactive dashboard in RStudio to interact with and share analysis results. "}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "### Author\n**Jorge Casta\u00f1\u00f3n** is a data scientist at IBM Analytics, specializing in machine learning and text analytics using Apache Spark and Hadoop."}, {"metadata": {}, "cell_type": "markdown", "source": "Copyright &copy; IBM Corp. 2017-2024. This notebook and its source code are released under the terms of the MIT License."}, {"metadata": {}, "cell_type": "markdown", "source": "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://www.ibm.com/products/watson-studio\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n</div>"}], "metadata": {"kernelspec": {"display_name": "Python 3.10 with Spark", "language": "python3", "name": "python310"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.10"}}, "nbformat": 4, "nbformat_minor": 1}
