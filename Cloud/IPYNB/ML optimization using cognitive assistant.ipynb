{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#F5F7FA; height:100px; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;color:#152935;\">Want to do more?</span><span style=\"border: 1px solid #3d70b2;padding: 15px;float:right;margin-right:40px; color:#3d70b2; \"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n",
    "<span style=\"color:#5A6872;\"> Try out this notebook with your free trial of IBM Watson Studio.</span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning optimization using the Cognitive Assistant component of spark-service\n",
    "\n",
    "This notebook demonstrates the use of the Cognitive Assistant imported into a Python kernel and accessed via a Jupyter notebook. In this sample, you use the Cognitive Assistant package to optimize machine learning piplelines for a dataframe you create from a data set. Cognitive Assistant  helps you:\n",
    "- Select the highest-performing pipelines for your dataframe, \n",
    "- Tune your chosen pipelines' hyperparameters for further optimization\n",
    "- Generate python code for the optimized pipelines.  \n",
    "\n",
    "The generated code assists you with subsequent training of models with your full data set (and subsequent updates), and with scoring (generating predictions for unlabeled data).\n",
    "\n",
    "Cognitive assistant currently employs Apache Spark (PySpark) and SKLearn (Python scikit-learn) analytics on Spark 2.1 and Python 2.7, optionally with feature selection, for classification and regression.\n",
    "\n",
    "In the first phase, when you invoke the `startOptimization()` method on your dataframe, Cognitive Assistant takes your data frame, samples it, and runs all pipelines with their default parameters, progressively allocating more data to the pipelines that cognitive assistant projects will have the best performance.  This approach allows Cognitive Assistant to quickly identify better-performing pipelines among many, therefore the user does not necessarily need to wait for the entire optimization to complete to select a preferred pipeline.\n",
    "\n",
    "In the second phase, you invoke the `hyperParamOptimizer()` method on the chosen pipeline's ID.  A radial basis function black-box optimization is performed on the adjustable parameters of the pipeline, using your data frame.  The result is a new pipeline with a new pipeline ID.\n",
    "\n",
    "In the third phase, you can generate code for your optimized pipeline and populate a notebook cell with it.  Currently, code generation is supported for sklearn classifiers.\n",
    "\n",
    "The notebook remains interactive during the optimizations. The execution counter and the kernel activity indicator reflect the activity of both the notebook user and the Cognitive Assistant.\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook has the following main sections:\n",
    "1. [One-time setup: Load or update packages](#update)\n",
    "1. [Download the data](#download)\n",
    "1. [Format the data for PySpark ML](#format)\n",
    "1. [Run the cognitive assistant to rank ML pipelines using your dataset, and monitor its progress](#cads)\n",
    "1. [Use cognitive assistant to optimize hyperparameters of a chosen pipeline for your dataset](#hpo)\n",
    "1. [Obtain readable/executable code for your chosen pipeline](#readableCode)\n",
    "1. [Stop cognitive assistant](#shutDown)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"update\"></a>\n",
    "## 1. One-time setup: update sklearn >0.17 and xgboost >= 0.6\n",
    "This need only be done one time per user.  The packages are stored in your client's home directory under ~/.local\n",
    "\n",
    "You should restart your kernel (**Kernel** > **Restart**) if any of these packages are installed or updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --user -U sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --user -U xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --user -U numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: restart your kernel (**Kernel** > **Restart**) if **any** of these packages were not already up-to-date, then continue with the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"download\"></a>\n",
    "## 2. Download the data (one time per user, per dataset)\n",
    "\n",
    "Because Cognitive Assistant is especially suited to large data sets, in the following step, you'll be retrieving a *portion of a*  large (2.6 GB) .csv file that contains data on HIGGS boson particles. You can modify the cell to get more of or all of the file.\n",
    "\n",
    "From the description of the data set on the UCI Web site: \"The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. There is an interest in using deep learning methods to obviate the need for physicists to manually develop such features. Benchmark results using Bayesian Decision Trees from a standard physics package and 5-layer neural networks are presented in the original paper. The last 500,000 examples are used as a test set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# optionally you can limit the download rate to wget via --limit-rate=2m\n",
      "# limiting the rate provides better control of the total amount of data that is downloaded\n",
      "wget -nv  -O limited.csv.gz --limit-rate=2m https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz &\n",
      "WGET_PID=$!\n",
      "while [ `ps $WGET_PID | wc -l ` -eq 2 ] ; do\n",
      "    sleep 1\n",
      "    SIZE=`du -k --apparent-size limited.csv.gz | awk '{print \\$1}'`\n",
      "    echo $SIZE kilobytes downloaded\n",
      "    #modify the target kilobyte download size after -ge below, or comment out the line below to get the whole file\n",
      "    if [ -e limited.csv.gz ] ; then if [ $SIZE -ge 3500  ] ; then kill -15 $WGET_PID; fi; fi\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Download a portion of a popular compressed csv dataset.  The target kilobytes of data are specified after '-ge' nine or so lines below\n",
    "cat << 'EOF' > limited.sh\n",
    "# optionally you can limit the download rate to wget via --limit-rate=2m\n",
    "# limiting the rate provides better control of the total amount of data that is downloaded\n",
    "wget -nv  -O limited.csv.gz --limit-rate=2m https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz &\n",
    "WGET_PID=$!\n",
    "while [ `ps $WGET_PID | wc -l ` -eq 2 ] ; do\n",
    "    sleep 1\n",
    "    SIZE=`du -k --apparent-size limited.csv.gz | awk '{print \\$1}'`\n",
    "    echo $SIZE kilobytes downloaded\n",
    "    #modify the target kilobyte download size after -ge below, or comment out the line below to get the whole file\n",
    "    if [ -e limited.csv.gz ] ; then if [ $SIZE -ge 3500  ] ; then kill -15 $WGET_PID; fi; fi\n",
    "done\n",
    "EOF\n",
    "cat limited.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expect an unexpected-end-of-file-message, that is ok if you specified a limited download size\n",
      "1544 kilobytes downloaded\n",
      "3656 kilobytes downloaded\n",
      "limited.sh: line 12:  3748 Terminated              wget -nv -O limited.csv.gz --limit-rate=2m https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\n",
      "\n",
      "gzip: limited.csv.gz: unexpected end of file\n",
      "14614 limited.csv  complete lines of csv retrieved\n"
     ]
    }
   ],
   "source": [
    "!echo \"expect an unexpected-end-of-file-message, that is ok if you specified a limited download size\"\n",
    "!bash limited.sh\n",
    "#an incomplete last line will likely occur in the case of a partial download, and must be discarded\n",
    "!gunzip -c -q limited.csv.gz | head -n -2 > limited.csv\n",
    "!echo `wc -l limited.csv` ' complete lines of csv retrieved'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"format\"></a>\n",
    "## 3. Format the data\n",
    "\n",
    "Format the data into a Pyspark SQL data frame consisting of a numeric `label` column and a vector `features` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_list = sc.version.split('.')\n",
    "major_version = version_list[0]\n",
    "if int(major_version) == 1:\n",
    "    from pyspark.mllib.linalg import Vectors\n",
    "else:\n",
    "    from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "if int(major_version) == 1:\n",
    "    raise Warning(\"Spark versions before 2.0 may no longer function with this notebook. Use Kernel... Change Kernel... to select Spark 2.0 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "  .options(header='false', inferschema='true')\\\n",
    "  .load(\"limited.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelName _c0\n",
      "featureNames=['_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7', '_c8', '_c9', '_c10', '_c11', '_c12', '_c13', '_c14', '_c15', '_c16', '_c17', '_c18', '_c19', '_c20', '_c21', '_c22', '_c23', '_c24', '_c25', '_c26', '_c27', '_c28']\n"
     ]
    }
   ],
   "source": [
    "allNames = [f.name for f in df.schema.fields]\n",
    "#FIRST column is the label (unlike many csv where it is the last)\n",
    "labelName = allNames[0]\n",
    "print 'labelName ' + labelName\n",
    "featureNames = [ f.name for f in df.schema.fields if f.name != labelName ]\n",
    "print 'featureNames=' + str(featureNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureAssembler = VectorAssembler(\n",
    "    inputCols=featureNames,\n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create features (vector) column then select label and features column, with renaming.\n",
    "inDF=featureAssembler.transform(df).selectExpr(labelName + \" as label\",\"features as features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[label: double, features: vector]\n"
     ]
    }
   ],
   "source": [
    "print inDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Remove compressed file\n",
    "\n",
    "You can clear up space on your filesystem by removing the compressed *limited.csv.gz* file that you downloaded. Because of the deferred computation in Spark, the uncompressed file is accessed later and cannot be erased at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f limited.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cads\"></a>\n",
    "## 4. Run the Cognitive Assistant optimization to rank machine learning pipelines for your data\n",
    "\n",
    "Before you can use the Cognitive Assistant, you must import the `cognitive_assistant` package and then start the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cognitive_assistant has instantiated 'assistant'\n"
     ]
    }
   ],
   "source": [
    "import cognitive_assistant as cognitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait...Cognitive Assistant is starting\n",
      "WARNING your arguments to startOptimization must be compatible with the cadsArgs you have supplied here.\n",
      "mario is available and responding\n",
      "mario is available and responding\n",
      "cads is available and responding\n",
      "ok to proceed\n",
      "Assistant is started.\n",
      "If you have directly invoked startAssistant() or have just imported cognitive_assistant, you may now invoke startOptimization(<dataFrame>).\n"
     ]
    }
   ],
   "source": [
    "#override CADS.conf hyperparameter optimization defaults for demonstration purposes\n",
    "cadsArgs='-DBBO_MAX_WALL_CLOCK_TIME=300'\n",
    "#better values for hyperparameter optimization, but will HPO will take too long for a demonstration\n",
    "#cadsArgs='-DBBO_MAX_WALL_CLOCK_TIME=10000 -DBBO_MAX_EVALUATIONS=350'\n",
    "#or use CADS.conf defaults:\n",
    "#cadsArgs=''\n",
    "cognitive.assistant.startAssistant(cadsArgs=cadsArgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Start the Cognitive Assistant optimization\n",
    "\n",
    "You invoke the Cognitive Assistant function by providing it the following information:\n",
    "\n",
    "-  The name of the data frame\n",
    "-  The prediction type (default is 'CLASSIFICATION')\n",
    "    -  Alternatively, specify data frame name and predictionType='REGRESSION'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant is determining the optimum pipeline for dataFrame NoNameSpecified with row count= 14614 in background.\n",
      "To view progress, invoke visualizeProgress().  To stop the optimization and erase the results, invoke stopAssistant().\n"
     ]
    }
   ],
   "source": [
    "cognitive.assistant.startOptimization(inDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Monitor the ranking of pipelines as optimization progresses\n",
    "\n",
    "This code defines a dynamically-updated leaderboard to monitor the progress of optimization in your notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a id=\"leaderboard\"></a><table width=\"100%\"><tr><th style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f5f5f5;\">ID</th><th style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f5f5f5;\">Approach</th><th style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f5f5f5;\">FSCORE [Loss]</th><th style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f5f5f5;\">Data Allocation Pct</th></tr><tr><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">P26</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">[TH Pending] Data -> FS: SklearnJupyterPercentile -> Sampler -> CL: JupyterXgboost</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">0.6626 [33.64%]</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">6.84</td></tr><tr><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">P3</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">[TH Pending] Data -> FS: SklearnJupyterPercentile -> Sampler -> CL: SklearnJupyterGradientBoostedTree</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">0.6574 [34.15%]</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">6.84</td></tr><tr><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">P30</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">[TH Pending] Data -> FS: SklearnJupyterPercentile -> Sampler -> CL: PySpark JupyterRandom Forest</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">0.6459 [35.26%]</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">6.84</td></tr><tr><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">P31</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">[TH Pending] Data -> FS: SklearnJupyterPercentile -> Sampler -> CL: SklearnJupyterDecision Tree  [Explainable]</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">0.6083 [38.44%]</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">6.84</td></tr><tr><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">P4</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">[TH Pending] Data -> FS: SklearnJupyterPercentile -> Sampler -> CL: SklearnJupyterLogistic Regression L2 [Explainable]</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">0.6001 [39.33%]</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">6.84</td></tr><tr><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">P6</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">[TH Pending] Data -> FS:  -> Sampler -> CL: SklearnJupyterLogistic Regression L2 [Explainable]</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">0.5916 [40.29%]</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">6.84</td></tr><tr><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">P9</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">[TH Pending] Data -> FS: SklearnJupyterPercentile -> Sampler -> CL: PySpark JupyterLogistic Regression [Explainable]</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">0.5884 [39.45%]</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">6.84</td></tr><tr><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">P29</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">[TH Pending] Data -> FS: SklearnJupyterPercentile -> Sampler -> CL: SklearnJupyterKNearestNeighbors</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">0.5714 [42.73%]</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">6.84</td></tr><tr><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">P8</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">[TH Pending] Data -> FS: SklearnJupyterPercentile -> Sampler -> CL: SklearnJupyterNaive Bayes</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">0.5571 [41.2%]</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #ffffff;\">6.84</td></tr><tr><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">P28</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">[TH Pending] Data -> FS: SklearnJupyterPercentile -> Sampler -> CL: SklearnJupyterPerceptron</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">0.4852 [42.74%]</td><td style=\"text-align: left; padding-right:15px; border:#f8f8ff 1px solid; background: #f0f8ff;\">6.84</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cognitive.assistant.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instruct the leaderboard to stop before the next automatic query/update.  It can be restarted at any time (after the query interval).\n",
    "#cognitive.assistant.leaderboard(stop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"leaderboard\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "Warning",
     "evalue": "Stopping 'Run All' execution at this point to allow time for optimization to progress.  Execute subsequent cells one-at-a-time as needed. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mWarning\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6a6867d769b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# raise an exception so that in-order all-cell execution is halted at this cell.  Instruct the user to proceed on a cell-by-cell basis.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mWarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Stopping 'Run All' execution at this point to allow time for optimization to progress.  Execute subsequent cells one-at-a-time as needed. \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mWarning\u001b[0m: Stopping 'Run All' execution at this point to allow time for optimization to progress.  Execute subsequent cells one-at-a-time as needed. "
     ]
    }
   ],
   "source": [
    "# raise an exception so that in-order all-cell execution is halted at this cell.  Instruct the user to proceed on a cell-by-cell basis.\n",
    "raise Warning(\"Stopping 'Run All' execution at this point to allow time for optimization to progress.  Execute subsequent cells one-at-a-time as needed. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate performance curves for all pipelines.  Re-invoke this as many times as you want to update and view current results. An error message can be expected while the optimization is starting up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The ranked list is empty. Please try again soon.\n"
     ]
    }
   ],
   "source": [
    "cognitive.assistant.visualizeProgress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Do a quick check of CPU - Optional\n",
    "\n",
    "To further check on the progress of your optimization, you can can run the following command to display CPU activity and the number of processes that are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1h\u001b=\u001b[H\u001b[2J\u001b[mtop - 07:28:03 up 26 days, 10:08,  0 users,  load average: 7.71, 6.58, 6.33\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "Tasks:\u001b[m\u001b[m\u001b[1m 948 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m   2 \u001b[m\u001b[mrunning,\u001b[m\u001b[m\u001b[1m 946 \u001b[m\u001b[msleeping,\u001b[m\u001b[m\u001b[1m   0 \u001b[m\u001b[mstopped,\u001b[m\u001b[m\u001b[1m   0 \u001b[m\u001b[mzombie\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "%Cpu(s):\u001b[m\u001b[m\u001b[1m 10.5 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m  4.5 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 84.9 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.1 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "KiB Mem :\u001b[m\u001b[m\u001b[1m 39597036+\u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m 25603824+\u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m 66286080 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m 73646048 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "KiB Swap:\u001b[m\u001b[m\u001b[1m  1048572 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m  1048572 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 32667980+\u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "\u001b[K\n",
      "\u001b[7m  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND     \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 5289 sf9b-79+  20   0 10.878g 523112  13064 S  96.4  0.1   1:40.14 java        \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 2146 sf9b-79+  20   0 13.985g 859076  13316 S  64.3  0.2   2:14.45 java        \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 1823 sf9b-79+  20   0 3915624 264128  18756 S  57.1  0.1   0:36.10 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 5367 sf9b-79+  20   0 11.432g 335832  13208 S  21.4  0.1   0:44.16 java        \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m\u001b[1m10672 sf9b-79+  20   0  160624   3048   1532 R  10.7  0.0   0:00.08 top         \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m10660 sf9b-79+  20   0  425060  18656   5644 S   7.1  0.0   0:00.23 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m39084 sf9b-79+  20   0 7342756 204884   7620 S   7.1  0.1   4:44.10 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 4417 sf9b-79+  20   0 2595556  51536   9132 S   0.0  0.0   0:04.57 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 4602 sf9b-79+  20   0 11.512g 184040  13176 S   0.0  0.0   0:53.60 java        \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 5362 sf9b-79+  20   0 2104484 108680  13952 S   0.0  0.0   0:03.28 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 6043 sf9b-79+  20   0 2187580 103012  14664 S   0.0  0.0   0:02.97 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 7790 sf9b-79+  20   0    4316    348    276 S   0.0  0.0   0:00.00 sleep       \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 8858 sf9b-79+  20   0   12512    832    676 S   0.0  0.0   0:00.00 msh         \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 8864 sf9b-79+  20   0  117368   1576   1272 S   0.0  0.0   0:00.00 sh          \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 8866 sf9b-79+  20   0   12512    180      0 S   0.0  0.0   0:00.00 msh         \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 8877 sf9b-79+  20   0  117508   1732   1392 S   0.0  0.0   0:00.00 c261fe67-6+ \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 9491 sf9b-79+  20   0   12512    836    676 S   0.0  0.0   0:00.00 msh         \u001b[m\u001b[m\u001b[K\u001b[?1l\u001b>\u001b[25;1H\n",
      "\u001b[K"
     ]
    }
   ],
   "source": [
    "!top -u $USER -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Continue your work!\n",
    "\n",
    "Cognitive Assistant is working in the background so you can continue to use your notebook interactively as the optimization progresses. This can include additional optimization and code generation as detailed below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hpo\"></a>\n",
    "## 5 Optimize hyperparameters for your chosen pipeline\n",
    "\n",
    "If you like the evolving performance evaluation of a pipeline, you do not need to wait for CADS ranking of ML algorithms to complete. You can ask Cognitive Assistant to optimize the hyperparameters for your chosen pipeline (identified by pipeline ID `P<nn>`) at any time. Black-Box Optimization can take considerable time, which can be controlled by the startup parameters first given to the `startAssistant()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "cognitive.assistant.hyperParamOptimizer(pipelineID='P2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can periodically return to the `visualizeProgress()` method to see the status of the HPO.  The original pipeline will have the additional label \"Optimizing\" while the optimization is in progress and the new pipeline will be labeled \"Optimized\" when HPO is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"readableCode\"></a>\n",
    "## 6 Obtain \"readable code\" for your chosen pipeline\n",
    "\n",
    "To deploy the pipeline outside of Cognitive Assistant, you can request a \"readable code\" representation of the pipeline identified by ID.  Remember that the hyperparameter-optimized pipeline is given its own pipeline ID, should you want the generated code to match its parameterization. This code is emitted directly into the notebook cell for you, the user, to use as you please. The query is done asynchronously in a two-step process in separate notebook cells to avoid deadlocking the kernel.\n",
    "\n",
    "The emitted code is intended to run with minimum necessary user modifications (you will need to provide the data frame names you wish to operate upon). Other minor fixups may be necessary.  Currently in cognitive-assistant, \"readable code\" output is limited to pipelines with sklearn classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixups for generated code to run as-is in this notebook.  Modify to suit.\n",
    "gini='gini'\n",
    "trainDF=inDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no json returned for pipeline (AlgorithmTrajectoryUUID): P41\n"
     ]
    }
   ],
   "source": [
    "cell=cognitive.assistant.generateCode('P41')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no json returned for pipeline (AlgorithmTrajectoryUUID): P41\n"
     ]
    }
   ],
   "source": [
    "#if the code was not emitted above, invoke the following:\n",
    "cell.generateCell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=1.0, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for pipeline P41\n",
    "from numpy import array as nparray\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "def np2df(npY,npX):\n",
    "       rddResult = sc.parallelize(zip(npY, npX))\n",
    "       dfResult = sqlContext.createDataFrame(rddResult.map(lambda r:Row(label=float(r[0]), features=Vectors.dense(r[1]))))     \n",
    "       return dfResult;\n",
    "\n",
    "def df2npF(df, featuresCol=\"features\"):\n",
    "        features = df.select(featuresCol)\n",
    "        features2 = features.withColumnRenamed(featuresCol, \"my_column\")\n",
    "        X = nparray([f.my_column.toArray() for f in features2.collect()])\n",
    "        return X;\n",
    "\n",
    "def df2npL(df, labelCol=\"label\"):\n",
    "        label = df.select(labelCol)\n",
    "        Y = nparray([l.label for l in label.collect()])\n",
    "        return Y;\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf =  GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, init=None, random_state=None, max_features=1.0, verbose=0, max_leaf_nodes=None, warm_start=False)\n",
    "clf.fit(df2npF(df=trainDF), df2npL(df=trainDF))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no json returned for pipeline (AlgorithmTrajectoryUUID): P2\n"
     ]
    }
   ],
   "source": [
    "cell2=cognitive.assistant.generateCode('P2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no json returned for pipeline (AlgorithmTrajectoryUUID): P2\n"
     ]
    }
   ],
   "source": [
    "#if the code was not emitted above, invoke the following:\n",
    "cell2.generateCell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=1.0, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for pipeline P2\n",
    "from numpy import array as nparray\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "def np2df(npY,npX):\n",
    "       rddResult = sc.parallelize(zip(npY, npX))\n",
    "       dfResult = sqlContext.createDataFrame(rddResult.map(lambda r:Row(label=float(r[0]), features=Vectors.dense(r[1]))))     \n",
    "       return dfResult;\n",
    "\n",
    "def df2npF(df, featuresCol=\"features\"):\n",
    "        features = df.select(featuresCol)\n",
    "        features2 = features.withColumnRenamed(featuresCol, \"my_column\")\n",
    "        X = nparray([f.my_column.toArray() for f in features2.collect()])\n",
    "        return X;\n",
    "\n",
    "def df2npL(df, labelCol=\"label\"):\n",
    "        label = df.select(labelCol)\n",
    "        Y = nparray([l.label for l in label.collect()])\n",
    "        return Y;\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf =  GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, init=None, random_state=None, max_features=1.0, verbose=0, max_leaf_nodes=None, warm_start=False)\n",
    "clf.fit(df2npF(df=trainDF), df2npL(df=trainDF))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=1.0, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for pipeline P2\n",
    "from numpy import array as nparray\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "def np2df(npY,npX):\n",
    "       rddResult = sc.parallelize(zip(npY, npX))\n",
    "       dfResult = sqlContext.createDataFrame(rddResult.map(lambda r:Row(label=float(r[0]), features=Vectors.dense(r[1]))))     \n",
    "       return dfResult;\n",
    "\n",
    "def df2npF(df, featuresCol=\"features\"):\n",
    "        features = df.select(featuresCol)\n",
    "        features2 = features.withColumnRenamed(featuresCol, \"my_column\")\n",
    "        X = nparray([f.my_column.toArray() for f in features2.collect()])\n",
    "        return X;\n",
    "\n",
    "def df2npL(df, labelCol=\"label\"):\n",
    "        label = df.select(labelCol)\n",
    "        Y = nparray([l.label for l in label.collect()])\n",
    "        return Y;\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf =  GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, init=None, random_state=None, max_features=1.0, verbose=0, max_leaf_nodes=None, warm_start=False)\n",
    "clf.fit(df2npF(df=trainDF), df2npL(df=trainDF))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=1.0, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for pipeline P2\n",
    "from numpy import array as nparray\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "def np2df(npY,npX):\n",
    "       rddResult = sc.parallelize(zip(npY, npX))\n",
    "       dfResult = sqlContext.createDataFrame(rddResult.map(lambda r:Row(label=float(r[0]), features=Vectors.dense(r[1]))))     \n",
    "       return dfResult;\n",
    "\n",
    "def df2npF(df, featuresCol=\"features\"):\n",
    "        features = df.select(featuresCol)\n",
    "        features2 = features.withColumnRenamed(featuresCol, \"my_column\")\n",
    "        X = nparray([f.my_column.toArray() for f in features2.collect()])\n",
    "        return X;\n",
    "\n",
    "def df2npL(df, labelCol=\"label\"):\n",
    "        label = df.select(labelCol)\n",
    "        Y = nparray([l.label for l in label.collect()])\n",
    "        return Y;\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf =  GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, init=None, random_state=None, max_features=1.0, verbose=0, max_leaf_nodes=None, warm_start=False)\n",
    "clf.fit(df2npF(df=trainDF), df2npL(df=trainDF))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=1.0, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for pipeline P2\n",
    "from numpy import array as nparray\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "def np2df(npY,npX):\n",
    "       rddResult = sc.parallelize(zip(npY, npX))\n",
    "       dfResult = sqlContext.createDataFrame(rddResult.map(lambda r:Row(label=float(r[0]), features=Vectors.dense(r[1]))))     \n",
    "       return dfResult;\n",
    "\n",
    "def df2npF(df, featuresCol=\"features\"):\n",
    "        features = df.select(featuresCol)\n",
    "        features2 = features.withColumnRenamed(featuresCol, \"my_column\")\n",
    "        X = nparray([f.my_column.toArray() for f in features2.collect()])\n",
    "        return X;\n",
    "\n",
    "def df2npL(df, labelCol=\"label\"):\n",
    "        label = df.select(labelCol)\n",
    "        Y = nparray([l.label for l in label.collect()])\n",
    "        return Y;\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf =  GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, init=None, random_state=None, max_features=1.0, verbose=0, max_leaf_nodes=None, warm_start=False)\n",
    "clf.fit(df2npF(df=trainDF), df2npL(df=trainDF))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"shutdown\"></a>\n",
    "## 6. Stop the cognitive assistant process\n",
    "\n",
    "When optimization has completed or progressed to an acceptable point, the cognitive assistant can be stopped by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## begin stub\n",
      "lf_filename = '/gpfs/global_fs01/sym_shared/YPProdSpark/user/sf9b-795b2b888c32b6-772f4e1cd93d/notebook/cads-72bc0314-3bc0-4c98-9114-1a193248e823/mario.output/script/meta/sfa1968aa04004a6c4ab7827076db72ed.json'\n",
      "lf_input_test_df_id = 's7fa24ea67d79743be40255f3eeed3471_test_df'\n",
      "lf_input_train_df_id = 's7fa24ea67d79743be40255f3eeed3471_train_df'\n",
      "lf_problem_type = 'binary-classification'\n",
      "lf_thread_id = 'sfa1968aa04004a6c4ab7827076db72ed_thread'\n",
      "from spark_ml_wrapper.evaluate_analytics_performance import aperf_go\n",
      "aperf_go(train_input_dfi=lf_input_train_df_id,\n",
      "         test_input_dfi=lf_input_test_df_id,\n",
      "         thi=lf_thread_id,\n",
      "         output_filename=lf_filename,\n",
      "         problem_type=lf_problem_type)\n",
      "## end stub\n",
      "\n",
      "Instance CADS services successfully terminated by stopAssistant().  Therefore terminating skulker:\n",
      "Assistant is stopped.  To start anew, invoke startAssistant(). Invoking import again will not re-start cognitive_assistant.\n"
     ]
    }
   ],
   "source": [
    "cognitive.assistant.stopAssistant()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You downloaded and formatted a publicly available data set and then used the cognitive assistant to identify a high-performance pipeline and adjust its hyperparameters for your dataset.  You then used cognitive-assistant to generate code (this cognitive-assistant feature is currently limited to sklearn classifiers) so that you can invoke these pipelines independently of cognitive-assistant. Congratulations!\n",
    "\n",
    "## Authors\n",
    "\n",
    "**Peter D. Kirchner**, PhD (Electrical Engineering), is a Research Scientist persuing computer science research in machine learning and cloud computing at the IBM Thomas J. Watson Research Center. He is presently engaged in cognitive automation of data science workflow to assist data scientists, focused on cloud-based deployments and scalability.\n",
    "\n",
    "**Gregory Bramble** is a Research Software Engineer at the IBM T.J. Watson Research Center. Greg has 10+ years development experience in industry and research, and has built several applications which have achieved widespread use. Greg studied Computer Science and Mathematics at Columbia University where his advanced coursework included AI, Graph Theory, and Algorithms.\n",
    "\n",
    "**Mike Sochka** is a content designer focusing on Watson Studio and Watson Machine Learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "### References\n",
    "\n",
    "Ashish Sabharwal, Horst Samulowitz, Gerald Tesauro, \"Selecting Near-Optimal Learners via Incremental Data Allocation\". Proc. 30th AAAI pp. 2007-15 (2016) http://dl.acm.org/citation.cfm?id=3016179\n",
    "\n",
    "Biem, A., Butrico, M., Feblowitz, M., Klinger, T., Malitsky, Y., Ng, K., Perer, A., and Reddy, C., Riabov, A., Samulowitz, H., Sow, D., Tesauro, G., Turaga, D. \"Towards Cognitive Automation of Data Science\".  Proc. 29th AAAI pp. 4268-9 (2015) http://dl.acm.org/citation.cfm?id=2888116.2888360\n",
    "\n",
    "Costa, A., Nannicini, G. \"RBFOpt: an open-source library for black-box optimization with costly function evaluations\" Optimization Online https://pdfs.semanticscholar.org/6bdb/d2de4988fab315bc7e6ec0cd14ed3ae8c018.pdf (2014)\n",
    "\n",
    "Baldi, P., P. Sadowski, and D. Whiteson. “Searching for Exotic Particles in High-energy Physics with Deep Learning.” Nature Communications 5 (July 2, 2014).\n",
    "\n",
    "Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2017, 2018 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [If you landed here from 'Run All' cells, you can navigate to the leaderboard](#leaderboard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.1",
   "language": "python",
   "name": "python2-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
