{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Parquet Modular Encryption Example\n## Key Management by Application"}, {"metadata": {}, "cell_type": "markdown", "source": "Configure IBM Analytics Engine accoring to https://cloud.ibm.com/docs/AnalyticsEngine?topic=AnalyticsEngine-parquet-encryption and choose it as the engine for the notebook."}, {"metadata": {}, "cell_type": "markdown", "source": "Define path to the folder with encrypted parquet files:"}, {"metadata": {}, "cell_type": "code", "source": "val encryptedParquetFullName = \"/tmp/bloodtests.parquet.encrypted\"", "execution_count": 7, "outputs": [{"data": {"text/plain": "encryptedParquetFullName = /tmp/bloodtests.parquet.encrypted\n"}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/plain": "/tmp/bloodtests.parquet.encrypted"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Setup keys for Parquet Encryption:"}, {"metadata": {}, "cell_type": "code", "source": "sc.hadoopConfiguration.set(\"encryption.key.list\",\n      \"key1: AAECAwQFBgcICQoLDA0ODw==, key2: AAECAAECAAECAAECAAECAA==\")", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Generate data:"}, {"metadata": {}, "cell_type": "code", "source": "val dataRange = (1 to 40).toList\nval bloodTestList = dataRange.map(i => (i, (i * 2)))", "execution_count": 9, "outputs": [{"data": {"text/plain": "dataRange = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40)\nbloodTestList = List((1,2), (2,4), (3,6), (4,8), (5,10), (6,12), (7,14), (8,16), (9,18), (10,20), (11,22), (12,24), (13,26), (14,28), (15,30), (16,32), (17,34), (18,36), (19,38), (20,40), (21,42), (22,44), (23,46), (24,48), (25,50), (26,52), (27,54), (28,56), (29,58), (30,60), (31,62), (32,64), (33,66), (34,68), (35,70), (36,72), (37,74), (38,76), (39,78), (40,80))\n"}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/plain": "List((1,2), (2,4), (3,6), (4,8), (5,10), (6,12), (7,14), (8,16), (9,18), (10,20), (11,22), (12,24), (13,26), (14,28), (15,30), (16,32), (17,34), (18,36), (19,38), (20,40), (21,42), (22,44), (23,46), (24,48), (25,50), (26,52), (27,54), (28,56), (29,58), (30,60), (31,62), (32,64), (33,66), (34,68), (35,70), (36,72), (37,74), (38,76), (39,78), (40,80))"}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Write encrypted data:"}, {"metadata": {}, "cell_type": "code", "source": "bloodTestList.toDF(\"id\", \"value\").write\n        // Configure which columns to encrypt with which keys\n      .option(\"encryption.column.keys\", \"key1: id\")\n      .option(\"encryption.footer.key\", \"key2\")\n      .mode(\"overwrite\").parquet(encryptedParquetFullName)", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Read encrypted data:"}, {"metadata": {}, "cell_type": "code", "source": "val encrypedDataDF = spark.read.parquet(encryptedParquetFullName)\nencrypedDataDF.createOrReplaceTempView(\"bloodtests\")\nval queryResult = spark.sql(\"SELECT id, value FROM bloodtests\")\nqueryResult.show(10, false)", "execution_count": 11, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-----+\n|id |value|\n+---+-----+\n|1  |2    |\n|2  |4    |\n|3  |6    |\n|4  |8    |\n|5  |10   |\n|6  |12   |\n|7  |14   |\n|8  |16   |\n|9  |18   |\n|10 |20   |\n+---+-----+\nonly showing top 10 rows\n\n"}, {"data": {"text/plain": "encrypedDataDF = [id: int, value: int]\nqueryResult = [id: int, value: int]\n"}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/plain": "[id: int, value: int]"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Make sure files were written with parquet encryption (in encrypted footer mode):"}, {"metadata": {}, "cell_type": "code", "source": "import scala.sys.process._\nval parquetPartitionFile = Seq(\"hdfs\", \"dfs\", \"-ls\", \"-S\", \"-C\", encryptedParquetFullName).!!.split(\"\\\\s+\")(0)\nSeq(\"hdfs\", \"dfs\", \"-tail\", parquetPartitionFile) #| \"tail -c 4\" !", "execution_count": 12, "outputs": [{"name": "stdout", "output_type": "stream", "text": "PARE"}, {"data": {"text/plain": "parquetPartitionFile = /tmp/bloodtests.parquet.encrypted/part-00000-87815d7c-fb42-45c2-a568-398aba00b608-c000.snappy.parquet\n"}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/plain": "warning: there was one feature warning; re-run with -feature for details\n"}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/plain": "0"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}]}, {"metadata": {}, "cell_type": "markdown", "source": "The output of the cell above should be <em>\"PARE\"</em>"}], "metadata": {"kernelspec": {"name": "scala-spark23", "display_name": "Scala 2.11 with Spark 2.3 (YARN Client Mode)", "language": "scala"}, "language_info": {"codemirror_mode": "text/x-scala", "file_extension": ".scala", "mimetype": "text/x-scala", "name": "scala", "pygments_lexer": "scala", "version": "2.11.8"}}, "nbformat": 4, "nbformat_minor": 1}