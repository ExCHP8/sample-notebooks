{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Use scikit-learn and custom library to predict temperature with `ibm-watson-machine-learning`"}, {"metadata": {}, "cell_type": "markdown", "source": "This notebook contains steps and code to train a Scikit-Learn model that uses a custom defined transformer and use it with Watson Machine Learning service. Once the model is trained, this notebook contains steps to persist the model and custom defined transformer to Watson Machine Learning Repository, deploy and score it using Watson Machine Learning python client.\n\nIn this notebook, we use GNFUV dataset that contains mobile sensor readings data about humidity and temperature from Unmanned Surface Vehicles in a test-bed in Athens, to train a Scikit-Learn model for predicting the temperature. \n\nSome familiarity with Python is helpful. This notebook uses Python & scikit-learn."}, {"metadata": {}, "cell_type": "markdown", "source": "## Learning goals\n\nThe learning goals of this notebook are:\n\n- Train a model with custom defined transformer\n- Persist the custom defined transformer and the model in Watson Machine Learning repository.\n- Deploy the model using Watson Machine Learning Service\n- Perform predictions using the deployed model\n\n## Contents\n1. [Set up the environment](#setup)\n2. [Install python library containing custom transformer implementation](#install_lib)\n3. [Prepare training data](#load)\n4. [Train the scikit-learn model](#train)\n5. [Save the model and library to WML Repository](#upload)\n6. [Deploy and score data in the IBM Cloud](#deploy)\n7. [Clean up](#cleanup)\n8. [Summary and next steps](#summary)\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"setup\"></a>\n## 1. Set up the environment\n\nBefore you use the sample code in this notebook, you must perform the following setup tasks:\n\n-  Create a <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/admin/create-services.html?context=cpdaas&audience=wdp\" target=\"_blank\" rel=\"noopener no referrer\">here</a>).\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### Connection to WML\n\nAuthenticate the Watson Machine Learning service on IBM Cloud. You need to provide platform `api_key` and instance `location`.\n\nYou can use [IBM Cloud CLI](https://cloud.ibm.com/docs/cli/index.html) to retrieve platform API Key and instance location.\n\nAPI Key can be generated in the following way:\n```\nibmcloud login\nibmcloud iam api-key-create API_KEY_NAME\n```\n\nIn result, get the value of `api_key` from the output.\n\n\nLocation of your WML instance can be retrieved in the following way:\n```\nibmcloud login --apikey API_KEY -a https://cloud.ibm.com\nibmcloud resource service-instance WML_INSTANCE_NAME\n```\n\nIn result, get the value of `location` from the output."}, {"metadata": {}, "cell_type": "markdown", "source": "**Tip**: Your `Cloud API key` can be generated by going to the [**Users** section of the Cloud console](https://cloud.ibm.com/iam#/users). From that page, click your name, scroll down to the **API Keys** section, and click **Create an IBM Cloud API key**. Give your key a name and click **Create**, then copy the created key and paste it below. You can also get a service specific url by going to the [**Endpoint URLs** section of the Watson Machine Learning docs](https://cloud.ibm.com/apidocs/machine-learning).  You can check your instance location in your  <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance details.\n\nYou can also get service specific apikey by going to the [**Service IDs** section of the Cloud Console](https://cloud.ibm.com/iam/serviceids).  From that page, click **Create**, then copy the created key and paste it below.\n\n**Action**: Enter your `api_key` and `location` in the following cell."}, {"metadata": {}, "cell_type": "code", "source": "api_key = 'PASTE YOUR PLATFORM API KEY HERE'\nlocation = 'PASTE YOUR INSTANCE LOCATION HERE'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_credentials = {\n    \"apikey\": api_key,\n    \"url\": 'https://' + location + '.ml.cloud.ibm.com'\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Install and import the `ibm-watson-machine-learning` package\n**Note:** `ibm-watson-machine-learning` documentation can be found <a href=\"http://ibm-wml-api-pyclient.mybluemix.net/\" target=\"_blank\" rel=\"noopener no referrer\">here</a>.\n"}, {"metadata": {}, "cell_type": "code", "source": "!pip install -U ibm-watson-machine-learning", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient\n\nclient = APIClient(wml_credentials)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Working with spaces\n\nFirst of all, you need to create a space that will be used for your work. If you do not have space already created, you can use [Deployment Spaces Dashboard](https://dataplatform.cloud.ibm.com/ml-runtime/spaces) to create one.\n\n- Click New Deployment Space\n- Create an empty space\n- Select Cloud Object Storage\n- Select Watson Machine Learning instance and press Create\n- Copy `space_id` and paste it below\n\n**Tip**: You can also use SDK to prepare the space for your work. More information can be found [here](https://github.com/IBM/watson-machine-learning-samples/blob/master/cloud/notebooks/python_sdk/instance-management/Space%20management.ipynb).\n\n**Action**: Assign space ID below"}, {"metadata": {}, "cell_type": "code", "source": "space_id = 'PASTE YOUR SPACE ID HERE'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You can use `list` method to print all existing spaces."}, {"metadata": {}, "cell_type": "code", "source": "client.spaces.list(limit=10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "To be able to interact with all resources available in Watson Machine Learning, you need to set **space** which you will be using."}, {"metadata": {}, "cell_type": "code", "source": "client.set.default_space(space_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"install_lib\"></a>\n\n## 2. Install the library containing custom transformer"}, {"metadata": {}, "cell_type": "markdown", "source": "Library - `linalgnorm-0.1` is a python distributable package that contains the implementation of a user defined Scikit-Learn transformer - `LNormalizer` . <br>\nAny 3rd party libraries that are required for the custom transformer must be defined as the dependency for the corresponding library that contains implementation of the transformer. \n\n\nIn this section, we will create the library and install it in the current notebook environment. "}, {"metadata": {}, "cell_type": "code", "source": "!mkdir -p linalgnorm-0.1/linalg_norm", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Define a custom scikit transformer."}, {"metadata": {}, "cell_type": "code", "source": "%%writefile linalgnorm-0.1/linalg_norm/sklearn_transformers.py\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport numpy as np\n\n\nclass LNormalizer(BaseEstimator, TransformerMixin):\n    def __init__(self, norm_ord=2):\n        self.norm_ord = norm_ord\n        self.row_norm_vals = None\n\n    def fit(self, X, y=None):\n        self.row_norm_vals = np.linalg.norm(X, ord=self.norm_ord, axis=0)\n\n    def transform(self, X, y=None):\n        return X / self.row_norm_vals\n\n    def fit_transform(self, X, y=None):\n        self.fit(X, y)\n        return self.transform(X, y)\n\n    def get_norm_vals(self):\n        return self.row_norm_vals", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Wrap created code into Python source distribution package."}, {"metadata": {}, "cell_type": "code", "source": "%%writefile linalgnorm-0.1/linalg_norm/__init__.py\n\n__version__ = \"0.1\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "%%writefile linalgnorm-0.1/README.md\n\nA simple library containing a simple custom scikit estimator.", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "%%writefile linalgnorm-0.1/setup.py\n\nfrom setuptools import setup\n\nVERSION='0.1'\nsetup(name='linalgnorm',\n      version=VERSION,\n      url='https://github.ibm.com/NGP-TWC/repository/',\n      author='IBM',\n      author_email='ibm@ibm.com',\n      license='IBM',\n      packages=[\n            'linalg_norm'\n      ],\n      zip_safe=False\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "%%bash\n\ncd linalgnorm-0.1\npython setup.py sdist --formats=zip\ncd ..\nmv linalgnorm-0.1/dist/linalgnorm-0.1.zip .\nrm -rf linalgnorm-0.1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Install the downloaded library using `pip` command"}, {"metadata": {}, "cell_type": "code", "source": "!pip install linalgnorm-0.1.zip", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"load\"></a>\n\n## 3. Download training dataset and prepare training data"}, {"metadata": {}, "cell_type": "markdown", "source": "Download the data from UCI repository - https://archive.ics.uci.edu/ml/machine-learning-databases/00452/GNFUV%20USV%20Dataset.zip"}, {"metadata": {}, "cell_type": "code", "source": "!rm -rf dataset\n!mkdir dataset", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00452/GNFUV%20USV%20Dataset.zip --output-document=dataset/gnfuv_dataset.zip", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!unzip dataset/gnfuv_dataset.zip -d dataset", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Create pandas datafame based on the downloaded dataset"}, {"metadata": {}, "cell_type": "code", "source": "import json\nimport pandas as pd\nimport numpy as np\nimport os\nfrom datetime import datetime\nfrom json import JSONDecodeError", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "home_dir = './dataset'\npi_dirs = os.listdir(home_dir)\n\ndata_list = []\nbase_time = None\ncolumns = None\n\nfor pi_dir in pi_dirs:\n    if 'pi' not in pi_dir:\n        continue\n    curr_dir = os.path.join(home_dir, pi_dir)\n    data_file = os.path.join(curr_dir, os.listdir(curr_dir)[0])\n    with open(data_file, 'r') as f:\n        line = f.readline().strip().replace(\"'\", '\"')\n        while line != '':\n            try:\n                input_json = json.loads(line)\n                sensor_datetime = datetime.fromtimestamp(input_json['time'])\n                if base_time is None:\n                    base_time = datetime(sensor_datetime.year, sensor_datetime.month, sensor_datetime.day, 0, 0, 0, 0)\n                input_json['time'] = (sensor_datetime - base_time).seconds\n                data_list.append(list(input_json.values()))\n                if columns is None:\n                    columns = list(input_json.keys())\n            except JSONDecodeError as je:\n                pass\n            line = f.readline().strip().replace(\"'\", '\"')\n\ndata_df = pd.DataFrame(data_list, columns=columns)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "data_df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Create training and test datasets from the downloaded GNFUV-USV dataset."}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nY = data_df['temperature']\nX = data_df.drop('temperature', axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=143)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"train\"></a>\n\n## 4. Train a model\n\nIn this section, you will use the custom transformer as a stage in the Scikit-Learn `Pipeline` and train a model."}, {"metadata": {}, "cell_type": "markdown", "source": "#### Import the custom transformer \nHere, import the custom transformer that has been defined in `linalgnorm-0.1.zip` and create an instance of it that will inturn be used as stage in `sklearn.Pipeline`"}, {"metadata": {}, "cell_type": "code", "source": "from linalg_norm.sklearn_transformers import LNormalizer", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "lnorm_transf = LNormalizer()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Import other objects required to train a model"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now, you can create a `Pipeline` with user defined transformer as one of the stages and train the model"}, {"metadata": {}, "cell_type": "code", "source": "skl_pipeline = Pipeline(steps=[('normalizer', lnorm_transf), ('regression_estimator', LinearRegression())])\nskl_pipeline.fit(X_train.loc[:, ['time', 'humidity']].values, y_train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "y_pred = skl_pipeline.predict(X_test.loc[:, ['time', 'humidity']].values)\nrmse = np.mean((np.round(y_pred) - y_test.values)**2)**0.5\nprint('RMSE: {}'.format(rmse))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"upload\"></a>\n\n## 5. Persist the model and custom library\n\nIn this section, using `ibm-watson_machine_learning` SDK, you will ...\n- save the library `linalgnorm-0.1.zip` in WML Repository by creating a package extension resource\n- create a Software Specification resource and bind the package resource to it. This Software Specification resource will be used to configure the online deployment runtime environment for a model \n- bind Software Specification resource to the model and save the model to WML Repository"}, {"metadata": {}, "cell_type": "markdown", "source": "### Create package extension"}, {"metadata": {}, "cell_type": "markdown", "source": "Define the meta data required to create package extension resource. <br>\n\nThe value for `file_path` in `client.package_extensions.LibraryMetaNames.store()` contains the library file name that must be uploaded to the WML.\n\n**Note:** You can also use conda environment configuration file `yaml` as package extension input. In such case set the `TYPE` to `conda_yml` and `file_path` to yaml file.\n```\nclient.package_extensions.ConfigurationMetaNames.TYPE = \"conda_yml\"\n```"}, {"metadata": {}, "cell_type": "code", "source": "meta_prop_pkg_extn = {\n    client.package_extensions.ConfigurationMetaNames.NAME: \"K_Linag_norm_skl\",\n    client.package_extensions.ConfigurationMetaNames.DESCRIPTION: \"Pkg extension for custom lib\",\n    client.package_extensions.ConfigurationMetaNames.TYPE: \"pip_zip\"\n}\n\npkg_extn_details = client.package_extensions.store(meta_props=meta_prop_pkg_extn, file_path=\"linalgnorm-0.1.zip\")\npkg_extn_uid = client.package_extensions.get_uid(pkg_extn_details)\npkg_extn_url = client.package_extensions.get_href(pkg_extn_details)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Display the details of the package extension resource that was created in the above cell."}, {"metadata": {}, "cell_type": "code", "source": "details = client.package_extensions.get_details(pkg_extn_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Create software specification and add custom library"}, {"metadata": {}, "cell_type": "markdown", "source": "Define the meta data required to create software spec resource and bind the package. This software spec resource will be used to configure the online deployment runtime environment for a model."}, {"metadata": {}, "cell_type": "code", "source": "client.software_specifications.ConfigurationMetaNames.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### List base software specifications"}, {"metadata": {}, "cell_type": "code", "source": "client.software_specifications.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Select base software specification to extend"}, {"metadata": {}, "cell_type": "code", "source": "base_sw_spec_uid = client.software_specifications.get_uid_by_name(\"runtime-23.1-py3.10\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Define new software specification based on base one and custom library"}, {"metadata": {}, "cell_type": "code", "source": "meta_prop_sw_spec = {\n    client.software_specifications.ConfigurationMetaNames.NAME: \"linalgnorm-0.1\",\n    client.software_specifications.ConfigurationMetaNames.DESCRIPTION: \"Software specification for linalgnorm-0.1\",\n    client.software_specifications.ConfigurationMetaNames.BASE_SOFTWARE_SPECIFICATION: {\"guid\": base_sw_spec_uid}\n}\n\nsw_spec_details = client.software_specifications.store(meta_props=meta_prop_sw_spec)\nsw_spec_uid = client.software_specifications.get_uid(sw_spec_details)\n\n\nclient.software_specifications.add_package_extension(sw_spec_uid, pkg_extn_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Save the model"}, {"metadata": {}, "cell_type": "markdown", "source": "Define the metadata to save the trained model to WML Repository along with the information about the software spec resource required for the model. \n\nThe `client.repository.ModelMetaNames.SOFTWARE_SPEC_UID` metadata property is used to specify the GUID of the software spec resource that needs to be associated with the model."}, {"metadata": {}, "cell_type": "code", "source": "model_props = {\n    client.repository.ModelMetaNames.NAME: \"Temp prediction model with custom lib\",\n    client.repository.ModelMetaNames.TYPE: 'scikit-learn_1.1',\n    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sw_spec_uid\n    \n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Save the model to the WML Repository and display its saved metadata. "}, {"metadata": {}, "cell_type": "code", "source": "published_model = client.repository.store_model(model=skl_pipeline, meta_props=model_props)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "published_model_uid = client.repository.get_model_id(published_model)\nmodel_details = client.repository.get_details(published_model_uid)\nprint(json.dumps(model_details, indent=2))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"deploy\"></a>\n\n## 6 Deploy and Score\n\nIn this section, you will deploy the saved model that uses the custom transformer and perform predictions. You will use WML client to perform these tasks."}, {"metadata": {}, "cell_type": "markdown", "source": "### Deploy the model"}, {"metadata": {}, "cell_type": "code", "source": "metadata = {\n    client.deployments.ConfigurationMetaNames.NAME: \"Deployment of custom lib model\",\n    client.deployments.ConfigurationMetaNames.ONLINE: {}\n}\n\ncreated_deployment = client.deployments.create(published_model_uid, meta_props=metadata)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Predict using the deployed model"}, {"metadata": {}, "cell_type": "markdown", "source": "**Note**: Here we use deployment `uid` saved in published_model object. In next section, we show how to retrive deployment url from Watson Machine Learning instance."}, {"metadata": {}, "cell_type": "code", "source": "deployment_uid = client.deployments.get_uid(created_deployment)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now you can print an online scoring endpoint. "}, {"metadata": {}, "cell_type": "code", "source": "scoring_endpoint = client.deployments.get_scoring_href(created_deployment)\nprint(scoring_endpoint)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"score\"></a>\nPrepare the payload for prediction. The payload contains the input records for which predictions has to be performed."}, {"metadata": {}, "cell_type": "code", "source": "scoring_payload = {\n    \"input_data\": [{\n        'fields': [\"time\", \"humidity\"],\n        'values': [[79863, 47]]}]\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Execute the method to perform online predictions and display the prediction results"}, {"metadata": {}, "cell_type": "code", "source": "predictions = client.deployments.score(deployment_uid, scoring_payload)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(json.dumps(predictions, indent=2))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"cleanup\"></a>\n## 7. Clean up"}, {"metadata": {}, "cell_type": "markdown", "source": "If you want to clean up all created assets:\n- experiments\n- trainings\n- pipelines\n- model definitions\n- models\n- functions\n- deployments\n\nplease follow up this sample [notebook](https://github.com/IBM/watson-machine-learning-samples/blob/master/cloud/notebooks/python_sdk/instance-management/Machine%20Learning%20artifacts%20management.ipynb)."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"summary\"></a>\n\n## 8. Summary\n\nYou successfully completed this notebook! \n \nYou learned how to use a scikit-learn model with custom transformer in Watson Machine Learning service to deploy and score.\n\nCheck out our [Online Documentation](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-service-instance.html) for more samples, tutorials, documentation, how-tos, and blog posts. "}, {"metadata": {}, "cell_type": "markdown", "source": "## Author\n\n**Krishnamurthy Arthanarisamy**, is a senior technical lead in IBM Watson Machine Learning team. Krishna works on developing cloud services that caters to different stages of machine learning and deep learning modeling life cycle.\n\n**Lukasz Cmielowski**, PhD, is a Software Architect and Data Scientist at IBM."}, {"metadata": {}, "cell_type": "markdown", "source": "Copyright \u00a9 2020, 2021, 2022 IBM. This notebook and its source code are released under the terms of the MIT License."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}
