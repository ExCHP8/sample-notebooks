{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Vector database agnostic integration with LangChain\n\nThis notebook teaches you how to:\n1. Apply a RAG framework by connecting a vector database to one of the watsonx foundation models and utility functions from the Watson Machine Learning service within watsonx.ai and Langchain,\n2. Build up a knowledge base,\n3. Create an embedding function to generate a Q&A resource for users"}, {"metadata": {}, "cell_type": "markdown", "source": "# Set up the environment "}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "## Install and import the dependencies"}, {"metadata": {}, "cell_type": "code", "source": "!pip install \"langchain==0.1.7\" | tail -n 1\n!pip install langchain_ibm | tail -n 1\n!pip install wget | tail -n 1\n!pip install sentence-transformers | tail -n 1\n!pip install \"chromadb==0.3.26\" | tail -n 1\n!pip install \"ibm-watsonx-ai>=0.1.7\" | tail -n 1\n!pip install \"pydantic==1.10.0\" | tail -n 1\n!pip install \"sqlalchemy==2.0.1\" | tail -n 1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Install vector database's Python library"}, {"metadata": {}, "cell_type": "markdown", "source": "You must choose your own vector database and update `\"vector store\"`."}, {"metadata": {}, "cell_type": "code", "source": "!pip install [\"vector store\"] | tail -n 1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Foundation models on watsonx.ai"}, {"metadata": {}, "cell_type": "markdown", "source": "## Import IBM Foundation Models and utility functions from WML and Langchain, build up knowledge base, create embedding fuction"}, {"metadata": {}, "cell_type": "code", "source": "from langchain.embeddings import HuggingFaceEmbeddings\n\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\n\nfrom langchain.chains import RetrievalQA\n\nimport os \n\nimport getpass\n\nimport wget", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Import vector database"}, {"metadata": {}, "cell_type": "markdown", "source": "You must choose your own vector database and update `\"VECTOR STORE\"`."}, {"metadata": {}, "cell_type": "code", "source": "from langchain.vectorstores import [\"VECTOR STORE\"]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## watsonx API connection (insert WML api key) \n\nThis cell defines the credentials required to work with watsonx API for Foundation Model inferencing.\n\nAction: Provide the IBM Cloud user API key. For details, see documentation: https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\n\nThe API requires project id that provides the context for the call. We will obtain the id from the project in which this notebook runs. Otherwise, please provide the project id.\n"}, {"metadata": {}, "cell_type": "code", "source": "credentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n    \"apikey\": getpass.getpass(\"Please enter your WML api key (hit enter): \")\n}\n\n\ntry:\n    project_id = os.environ[\"PROJECT_ID\"] \nexcept KeyError:\n    project_id = input(\"Please enter your project_id (hit enter): \")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Define model, define model parameteres, LangChain CustomLLM wrapper for watsonx model"}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n\nmodel_id = ModelTypes.GRANITE_13B_CHAT_V2\n\n### Define model behavior parameters\n\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n\nparameters = {\n    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n    GenParams.MIN_NEW_TOKENS: 1,\n    GenParams.MAX_NEW_TOKENS: 100,\n    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\"]\n}\n\n### Create an instance of your watsonx LLM\nfrom langchain_ibm import WatsonxLLM\n\nwatsonx_granite = WatsonxLLM(\n    model_id=model_id.value,\n    url=credentials.get(\"url\"),\n    apikey=credentials.get(\"apikey\"),\n    project_id=project_id,\n    params=parameters\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Document data loading"}, {"metadata": {}, "cell_type": "code", "source": "filename = 'state_of_the_union.txt'\nurl = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\nif not os.path.isfile(filename):\n    wget.download(url, out=filename)\nloader = TextLoader(filename)\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.split_documents(documents)\n\n\nembeddings = HuggingFaceEmbeddings()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Connecting to vector database from Langchain \n\nCheck Langchain documentation for each vector db (https://python.langchain.com/docs/integrations/vectorstores) for the specific formatting. Check whether it is an in memory db or cloud based db. If cloud based, user will need to create an account and obtain api key. See example below: "}, {"metadata": {}, "cell_type": "code", "source": "url = [\"URL\"]\napi_key = [\"API KEY\"]\ndocsearch = \"VECTOR STORE\".from_documents(\n    docs,\n    embeddings,\n    url=url,\n    prefer_grpc=True,\n    api_key=api_key,\n    collection_name=[\"COLLECTION NAME\"],\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Generate a retrieval-augmented response to a question\n\n## Create RAG chain\n"}, {"metadata": {}, "cell_type": "code", "source": "qa = RetrievalQA.from_chain_type(llm=watsonx_granite, chain_type=\"stuff\", retriever=docsearch.as_retriever())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Select questions "}, {"metadata": {}, "cell_type": "code", "source": "query = \"What did the president say about Ketanji Brown Jackson\"\nqa.invoke(query)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Copyright \u00a9 2024 IBM. This notebook and its source code are released under the terms of the MIT License."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}
