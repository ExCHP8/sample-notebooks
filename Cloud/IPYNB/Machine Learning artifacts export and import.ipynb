{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "\n# Export/Import assets with `ibm-watson-machine-learning`\n\nThis notebook demonstrates an example for exporting/importing assets using Watson Machine Learning service. It contains steps and code to work with [ibm-watson-machine-learning](https://pypi.python.org/pypi/ibm-watson-machine-learning) library available in PyPI repository."}, {"metadata": {}, "cell_type": "markdown", "source": "## Learning goals\n\nThe learning goals of this notebook are:\n\n-  Download an externally trained Keras model.\n-  Persist an external model in Watson Machine Learning repository.\n-  Export the model from the space\n-  Import the model to another space and deploy\n\n\n\n## Contents\n\nThis notebook contains the following parts:\n\n1. [Setup](#setup)\n2. [Download externally created Keras model](#download)\n3. [Persist externally created Keras model](#persistence)\n4. [Export the model](#export)\n5. [Import the model](#import)\n6. [Deploy and score the imported model](#scoring)\n7. [Clean up](#cleanup)\n8. [Summary and next steps](#summary)"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"setup\"></a>\n## 1. Set up the environment\n\nBefore you use the sample code in this notebook, you must perform the following setup tasks:\n\n-  Create a <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/admin/create-services.html?context=cpdaas&audience=wdp\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)."}, {"metadata": {}, "cell_type": "markdown", "source": "### Connection to WML\n\nAuthenticate the Watson Machine Learning service on IBM Cloud. You need to provide platform `api_key` and instance `location`.\n\nYou can use [IBM Cloud CLI](https://cloud.ibm.com/docs/cli/index.html) to retrieve platform API Key and instance location.\n\nAPI Key can be generated in the following way:\n```\nibmcloud login\nibmcloud iam api-key-create API_KEY_NAME\n```\n\nIn result, get the value of `api_key` from the output.\n\n\nLocation of your WML instance can be retrieved in the following way:\n```\nibmcloud login --apikey API_KEY -a https://cloud.ibm.com\nibmcloud resource service-instance WML_INSTANCE_NAME\n```\n\nIn result, get the value of `location` from the output."}, {"metadata": {}, "cell_type": "markdown", "source": "**Tip**: Your `Cloud API key` can be generated by going to the [**Users** section of the Cloud console](https://cloud.ibm.com/iam#/users). From that page, click your name, scroll down to the **API Keys** section, and click **Create an IBM Cloud API key**. Give your key a name and click **Create**, then copy the created key and paste it below. You can also get a service specific url by going to the [**Endpoint URLs** section of the Watson Machine Learning docs](https://cloud.ibm.com/apidocs/machine-learning).  You can check your instance location in your  <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance details.\n\nYou can also get service specific apikey by going to the [**Service IDs** section of the Cloud Console](https://cloud.ibm.com/iam/serviceids).  From that page, click **Create**, then copy the created key and paste it below.\n\n**Action**: Enter your `api_key` and `location` in the following cell."}, {"metadata": {}, "cell_type": "code", "source": "#api_key = 'PASTE YOUR PLATFORM API KEY HERE'\n# location = 'PASTE YOUR INSTANCE LOCATION HERE'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_credentials = {\n    \"apikey\": api_key,\n    \"url\": 'https://' + location + '.ml.cloud.ibm.com'\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Install and import the latest `ibm-watson-machine-learning` package\n**Note:** `ibm-watson-machine-learning` documentation can be found <a href=\"http://ibm-wml-api-pyclient.mybluemix.net/\" target=\"_blank\" rel=\"noopener no referrer\">here</a>. Latest client can be found <a href=\"https://pypi.org/project/ibm-watson-machine-learning/\" rel=\"noopener no referrer\">here.</a>"}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient\n\nclient = APIClient(wml_credentials)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Create two spaces. One for export and one for import\n\n**Tip**: You can refer to details and example for space management apis [here](https://github.com/IBM/watson-machine-learning-samples/blob/master/cloud/notebooks/python_sdk/instance-management/Space%20management.ipynb)\n"}, {"metadata": {}, "cell_type": "code", "source": "# Refer to the link in above Tip how to find the cos and instance crns\ncos_resource_crn = \"CRN_OF_YOUR_COS\"\ninstance_crn = \"CRN_OF_YOUR_WML_INSTANCE\"\nname = \"NAME_OF_YOUR_WML_INSTANCE\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import uuid\n\nspace_name = str(uuid.uuid4())\n\n\nexport_space_metadata = {\n              client.spaces.ConfigurationMetaNames.NAME: \"client_space_export_\" + space_name,\n              client.spaces.ConfigurationMetaNames.DESCRIPTION: space_name + \" description\",\n              client.spaces.ConfigurationMetaNames.STORAGE: { \"resource_crn\": cos_resource_crn},\n              client.spaces.ConfigurationMetaNames.COMPUTE: {\n                         \"name\": name,\n                         \"crn\": instance_crn\n              }\n}\n\nspace = client.spaces.store(meta_props=export_space_metadata)\n\nexport_space_id = client.spaces.get_id(space)\nprint(\"{}export space_id: {}{}\".format('\\n', export_space_id, '\\n'))\n\n\nimport_space_metadata = {\n              client.spaces.ConfigurationMetaNames.NAME: \"client_space_import_\" + space_name,\n              client.spaces.ConfigurationMetaNames.DESCRIPTION: space_name +  \"description\",\n              client.spaces.ConfigurationMetaNames.STORAGE: {\"resource_crn\": cos_resource_crn},\n              client.spaces.ConfigurationMetaNames.COMPUTE: {\n                         \"name\": name,\n                         \"crn\": instance_crn\n              }\n}\n\nspace = client.spaces.store(meta_props=import_space_metadata)\n\nimport_space_id = client.spaces.get_id(space)\nprint(\"{}import space_id: {}\".format('\\n', import_space_id))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "client.spaces.get_details(\"YOUR_SPACE_ID\")['entity']['status']", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"download\"></a>\n## 2. Download externally created Keras model and data\nIn this section, you will download externally created Keras models and data used for training it."}, {"metadata": {}, "cell_type": "code", "source": "! pip install wget", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"is_executing": false, "name": "#%%\n"}}, "cell_type": "code", "source": "import os\nimport wget\nimport ssl\n\ndata_dir = 'MNIST_DATA'\nif not os.path.isdir(data_dir):\n    os.mkdir(data_dir)\n    \nmodel_path = os.path.join(data_dir, 'mnist_keras.h5.tgz')\nif not os.path.isfile(model_path):\n    wget.download(\"https://github.com/IBM/watson-machine-learning-samples/raw/master/cloud/models/keras/mnist_keras.h5.tgz\", out=data_dir)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import os\nimport wget\n\ndata_dir = 'MNIST_DATA'\nif not os.path.isdir(data_dir):\n    os.mkdir(data_dir)\n    \nfilename = os.path.join(data_dir, 'mnist.npz')\nif not os.path.isfile(filename):\n    wget.download('https://s3.amazonaws.com/img-datasets/mnist.npz', out=data_dir)  ", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"is_executing": false, "name": "#%%\n"}}, "cell_type": "code", "source": "import numpy as np\n\ndataset = np.load(filename)\nx_test = dataset['x_test']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"persistence\"></a>\n## 3. Persist externally created Keras model"}, {"metadata": {}, "cell_type": "markdown", "source": "In this section, you will learn how to store your model in Watson Machine Learning repository by using the Watson Machine Learning Client."}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.1: Publish model"}, {"metadata": {}, "cell_type": "markdown", "source": "Define model name, type and software specification needed to deploy model later."}, {"metadata": {"pycharm": {"is_executing": false, "name": "#%%\n"}}, "cell_type": "code", "source": "sofware_spec_uid = client.software_specifications.get_id_by_name(\"runtime-23.1-py3.10\")", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"is_executing": false, "name": "#%%\n"}}, "cell_type": "code", "source": "client.set.default_space(export_space_id)\n\nmetadata = {\n            client.repository.ModelMetaNames.NAME: 'External Keras model',\n            client.repository.ModelMetaNames.TYPE: 'tensorflow_2.12',\n            client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sofware_spec_uid\n}\n\npublished_model = client.repository.store_model(\n    model=model_path,\n    meta_props=metadata)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.2: Get model details"}, {"metadata": {"pycharm": {"is_executing": false, "name": "#%%\n"}}, "cell_type": "code", "source": "import json\n\npublished_model_uid = client.repository.get_model_id(published_model)\nmodel_details = client.repository.get_details(published_model_uid)\nprint(json.dumps(model_details, indent=2))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.3 Get all models in the space"}, {"metadata": {}, "cell_type": "markdown", "source": "space_id is automatically picked up from client.set.default_space() api call before"}, {"metadata": {"pycharm": {"is_executing": false, "name": "#%%\n"}}, "cell_type": "code", "source": "models_details = client.repository.list_models()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"export\"></a>\n## 4. Export"}, {"metadata": {}, "cell_type": "code", "source": "help(client.export_assets.start)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "client.export_assets has these apis. For any help on these apis, type 'help(api_name)' in your notebook \nExample: help(client.export_assets.start), help(client.export_assets.get_details)\n\n1. client.export_assets.start: This starts the export job. export job is asynchronously executed\n2. client.export_assets.get_details: Given export_id and corresponding space_id/project_id, this gives the export job details. Usually used for monitoring the export job submitted with start api\n3. client.export_assets.list: Prints summary of all the export jobs\n4. client.export_assets.get_exported_content: Downloads the exported content. This information will be used by the import process\n5. client.export_assets.delete: Deletes the given export job\n6. client.export_assets.cancel: Cancels the given export job if running"}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.1: Start the export process"}, {"metadata": {}, "cell_type": "markdown", "source": "Start the export process for the model created. Either ASSET_IDS or ASSET_TYPES or ALL_ASSETS can be provided.\nIf you have more than one model ids, you need to provide them as array like client.export_assets.ConfigurationMetaNames.ASSET_IDS: [model_id1, model_id2]\nRefer to the help api above to see different usages and details"}, {"metadata": {}, "cell_type": "code", "source": "metadata = { client.export_assets.ConfigurationMetaNames.NAME: \"export_model\",\n             client.export_assets.ConfigurationMetaNames.ASSET_IDS: [published_model_uid]                                                                         \n           }\n\ndetails = client.export_assets.start(meta_props=metadata, space_id=export_space_id)\nprint(json.dumps(details, indent=2))\n\nexport_job_id = details[u'metadata'][u'id']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.2: Monitor the export process"}, {"metadata": {}, "cell_type": "code", "source": "import time\n\nstart_time = time.time()\ndiff_time = start_time - start_time\nwhile True and diff_time < 10 * 60:\n    time.sleep(3)\n    response = client.export_assets.get_details(export_job_id, space_id=export_space_id)\n    state = response[u'entity'][u'status'][u'state']\n    print(state)\n    if state == 'completed' or state == 'error' or state == 'failed':\n        break\n    diff_time = time.time() - start_time\n\nprint(json.dumps(response, indent=2))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.3: Get the exported content"}, {"metadata": {}, "cell_type": "code", "source": "export_dir = 'EXPORT_DATA'\n\nif not os.path.isdir(export_dir):\n    os.mkdir(export_dir)\n    \nexport_file_name = 'exported_content_' + str(uuid.uuid4()) + '.zip'\n    \nexport_file_path = os.path.join(export_dir, export_file_name)\n\ndetails = client.export_assets.get_exported_content(export_job_id, \n                                                    space_id = export_space_id, \n                                                    file_path = export_file_path)\n\nprint(details)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"import\"></a>\n## 5. Import"}, {"metadata": {}, "cell_type": "markdown", "source": "client.import_assets has these apis. For any help on these apis, type 'help(api_name)' in your notebook \nExample: help(client.import_assets.start), help(client.import_assets.get_details)\n\n1. client.import_assets.start: This starts the import job. import job is asynchronously executed\n2. client.import_assets.get_details: Given import_id and corresponding space_id/project_id, this gives the import job details. Usually used for monitoring the import job submitted with start api\n3. client.import_assets.list: Prints summary of all the import jobs\n4. client.import_assets.delete: Deletes the given import job\n5. client.import_assets.cancel: Cancels the given import job if running"}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.1: Start the import process"}, {"metadata": {}, "cell_type": "code", "source": "details = client.import_assets.start(file_path=export_file_path,\n                                     space_id=import_space_id)\nprint(json.dumps(details, indent=2))\n\nimport_job_id = details[u'metadata'][u'id']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.2: Monitor the import process"}, {"metadata": {}, "cell_type": "code", "source": "import time\n\nstart_time = time.time()\ndiff_time = start_time - start_time\nwhile True and diff_time < 10 * 60:\n    time.sleep(3)\n    response = client.import_assets.get_details(import_job_id,\n                                                space_id=import_space_id)\n    state = response[u'entity'][u'status'][u'state']\n    print(state)\n    if state == 'completed' or state == 'error' or state == 'failed':\n         break\n    diff_time = time.time() - start_time\n\nprint(json.dumps(response, indent=2))\n\nclient.set.default_space(import_space_id)\n\nprint(\"{}List of models: {}\".format('\\n', '\\n'))\nclient.repository.list_models()\ndetails = client.repository.get_model_details()\n\nfor obj in details[u'resources']:\n    if obj[u'metadata'][u'name'] == \"External Keras model\":\n        model_id_for_deployment = obj[u'metadata'][u'id']\n\nprint(\"{}model id for deployment: {}\".format('\\n', model_id_for_deployment))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "List the import and export jobs"}, {"metadata": {}, "cell_type": "code", "source": "print(\"Export jobs: \\n\")\nclient.export_assets.list(space_id=export_space_id)\nprint(\"\\nImport jobs:\")\nclient.import_assets.list(space_id=import_space_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"scoring\"></a>\n## 6. Deploy and score the imported model"}, {"metadata": {}, "cell_type": "markdown", "source": "### 6.1: Create model deployment"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Create online deployment for published model"}, {"metadata": {"pycharm": {"is_executing": false, "name": "#%%\n"}}, "cell_type": "code", "source": "metadata = {\n    client.deployments.ConfigurationMetaNames.NAME: \"Deployment of external Keras model\",\n    client.deployments.ConfigurationMetaNames.ONLINE: {}\n}\n\ncreated_deployment = client.deployments.create(model_id_for_deployment, meta_props=metadata)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "deployment_uid = client.deployments.get_uid(created_deployment)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now you can print an online scoring endpoint. "}, {"metadata": {"pycharm": {"is_executing": false, "name": "#%%\n"}}, "cell_type": "code", "source": "scoring_endpoint = client.deployments.get_scoring_href(created_deployment)\nprint(scoring_endpoint)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You can also list existing deployments."}, {"metadata": {}, "cell_type": "code", "source": "client.deployments.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 6.2: Get deployment details"}, {"metadata": {"pycharm": {"is_executing": false, "name": "#%%\n"}}, "cell_type": "code", "source": "details = client.deployments.get_details(deployment_uid)\nprint(json.dumps(details, indent=2))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 6.3: Score"}, {"metadata": {}, "cell_type": "markdown", "source": "You can use below method to do test scoring request against deployed model."}, {"metadata": {}, "cell_type": "markdown", "source": "Let's first visualize two samples from dataset, we'll use for scoring. You must have matplotlib package installed"}, {"metadata": {}, "cell_type": "code", "source": "%matplotlib inline\nimport matplotlib.pyplot as plt", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "for i, image in enumerate([x_test[0], x_test[1]]):\n    plt.subplot(2, 2, i + 1)\n    plt.axis('off')\n    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Prepare scoring payload with records to score."}, {"metadata": {}, "cell_type": "code", "source": "score_0 = (x_test[0].ravel() / 255).tolist()\nscore_1 = (x_test[1].ravel() / 255).tolist()", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"is_executing": false, "name": "#%%\n"}}, "cell_type": "code", "source": "scoring_payload = {\"input_data\": [{\"values\": [score_0, score_1]}]}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Use ``client.deployments.score()`` method to run scoring."}, {"metadata": {"pycharm": {"is_executing": false, "name": "#%%\n"}}, "cell_type": "code", "source": "predictions = client.deployments.score(deployment_uid, scoring_payload)", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"is_executing": false, "name": "#%%\n"}}, "cell_type": "code", "source": "print(json.dumps(predictions, indent=2))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"cleanup\"></a>\n## 7. Clean up "}, {"metadata": {}, "cell_type": "code", "source": "client.export_assets.delete(export_job_id, space_id=export_space_id)\nclient.import_assets.delete(import_job_id, space_id=import_space_id)\n\nclient.spaces.delete(export_space_id)\nclient.spaces.delete(import_space_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "If you want to clean up all created assets:\n- experiments\n- trainings\n- pipelines\n- model definitions\n- models\n- functions\n- deployments\n\nplease follow up this sample [notebook](https://github.com/IBM/watson-machine-learning-samples/blob/master/cloud/notebooks/python_sdk/instance-management/Machine%20Learning%20artifacts%20management.ipynb)."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"summary\"></a>\n## 8. Summary and next steps"}, {"metadata": {}, "cell_type": "markdown", "source": " You successfully completed this notebook! You learned how to use export/import assets client apis. Check out our _[Online Documentation](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/welcome-main.html)_ for more samples, tutorials, documentation, how-tos, and blog posts. "}, {"metadata": {}, "cell_type": "markdown", "source": "### Authors\n\n*Mithun - *vbmithun@in.ibm.com**, Software Engineer"}, {"metadata": {}, "cell_type": "markdown", "source": "Copyright \u00a9 2020, 2021, 2022 IBM. This notebook and its source code are released under the terms of the MIT License."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "pycharm": {"stem_cell": {"cell_type": "raw", "metadata": {"collapsed": false}, "source": []}}}, "nbformat": 4, "nbformat_minor": 4}
