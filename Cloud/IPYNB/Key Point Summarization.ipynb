{"cells": [{"metadata": {"id": "ecc90601-6af3-42d2-891c-f6e375feff5f"}, "cell_type": "markdown", "source": "# Key Point Summarization\n## Extracting meaningful insights from reviews, surveys, and customer feedback\n\nWhen dealing with large collections of text representing people's opinions, such as product reviews, survey responses, customer feedback, or social media posts, understanding the key issues within the data can be challenging. Manually reviewing thousands of comments is time-consuming and cost-prohibitive.\nExisting automated approaches are typically limited to identifying recurring phrases or concepts and gauging overall sentiment. Such methods often fail to provide fine-grained, actionable insights.\nKey Point Summarization maps the input texts to a set of automatically-extracted short sentences and phrases, termed Key Points, which provide a concise plain-text summary of the data. The prevalence of each key point is quantified as the number of its matching sentences.\n\nIn this tutorial, you will gain hands-on experience using Key Point Summarization (KPS) to analyze and derive insights from free-text feedback.\nThe data we will use is [a community survey conducted in the city of Austin](https://data.austintexas.gov/dataset/Community-Survey/s2py-ceb7). In this survey, the citizens of Austin were asked \"If there was ONE thing you could share with the Mayor regarding the City of Austin (any comment, suggestion, etc.), what would it be?\".\n\nKPS utilizes fine-tuned language models for data analysis. Therefore, an environment based on runtime 24.1 and with a GPU is required to run this tutorial effectively.\n"}, {"metadata": {}, "cell_type": "markdown", "source": "## Learning goal\n\nThe goal of this notebook is to demonstrate how Key Points Summarization can be used for extracting meaningful insights from reviews, surveys and customer feedback"}, {"metadata": {}, "cell_type": "markdown", "source": "## Contents\n\nThis notebook contains the following parts:\n\n- [Initialization and data loading for analysis](#setup)\n- [Creating a summary](#summary)\n- [Examining and saving results](#exam)\n"}, {"metadata": {"id": "601c5872-b567-495f-b492-7a48db2a19d9"}, "cell_type": "markdown", "source": "<a id=\"setup\"></a>\n## Initialization and data loading for analysis\nFirst thing we need to do is run the KPS backend. This service runs in the background and performs the analysis."}, {"metadata": {"id": "4c5b65cf-e0a6-4086-8d0f-7111b0892576"}, "cell_type": "code", "source": "from keypoint_matching.BackendRunnerWatsonStudio import BackendRunnerWatsonStudio\nrunner = BackendRunnerWatsonStudio()\nrunner.start_backend()", "execution_count": null, "outputs": []}, {"metadata": {"id": "96d20d28-cf9d-498f-a192-e08fcb59b90b"}, "cell_type": "markdown", "source": "Now we can create a client that connects to the backend and uses it."}, {"metadata": {"id": "32e75917-1e40-4b72-b303-ae51af9ed93a"}, "cell_type": "code", "source": "from key_points_summarization_api.api.clients.keypoints_client import KpsClientWatsonStudio\nclient = KpsClientWatsonStudio()", "execution_count": null, "outputs": []}, {"metadata": {"id": "815ff146-2b7b-4102-b32d-ebe356ee6743"}, "cell_type": "markdown", "source": "Let's run self_check and make sure all is configured correctly and working.\nself_check outputs\n{'status': 'UP'} when all is working or {'status': 'DOWN'} if a problem is detected. We can also see if GPUs are used or not."}, {"metadata": {"id": "0957814b-b52f-4232-a5e5-97b35e9b2fcf"}, "cell_type": "code", "source": "client.run_self_check()", "execution_count": null, "outputs": []}, {"metadata": {"id": "84f20062-ad75-440e-8209-f5c9b2370c09"}, "cell_type": "markdown", "source": "Next, we read the Austin survey dataset from the dataset_austin.csv file."}, {"metadata": {"id": "2cc0da0d-34bc-4004-8ceb-c244063b43c0"}, "cell_type": "code", "source": "import pandas as pd\n\ndef get_comments_texts():\n    url = \"https://raw.githubusercontent.com/IBMDataScience/sample-notebooks/master/Files/Data/dataset_austin.csv\"\n    df = pd.read_csv(url)\n    texts = [str(text) for text in df['text'].tolist()]\n    texts = [text for text in texts if len(text)<3000]\n    return texts", "execution_count": null, "outputs": []}, {"metadata": {"id": "144a740b-903d-4d17-9267-363a5b70cb05"}, "cell_type": "markdown", "source": "We load the data into a list of strings and limit the number of comments for quick analysis:"}, {"metadata": {"id": "ececd78a-01d9-4b69-b7da-68600bd174f7"}, "cell_type": "code", "source": "texts = get_comments_texts()\nprint(f'There are {len(texts)} comments in the dataset')\n\nlimit_comments = 500\ntexts = texts[:limit_comments]\nprint(f'Analysing {len(texts)} comments')", "execution_count": null, "outputs": []}, {"metadata": {"id": "9936bacf-6791-4c20-a250-817bd36bf7b7"}, "cell_type": "markdown", "source": "<a id=\"summary\"></a>\n## Creating a summary\nWe will now analyze the comments using the `client.run_full_kps_flow` method. This may take a little while. Run time depends on the input size.\n\nEach dataset is temporarily stored in a domain, to which the analysis is applied."}, {"metadata": {"id": "b8181239-1589-44d3-aef3-082f653e93a3", "scrolled": true}, "cell_type": "code", "source": "domain = f'austin_test'  # describes the dataset\nkps_result = client.run_full_kps_flow(domain, texts)", "execution_count": null, "outputs": []}, {"metadata": {"id": "f9942ebc-8232-45ac-a9a3-b029c99cd4db"}, "cell_type": "markdown", "source": "<a id=\"exam\"></a>\n## Examining and saving results\nResults are now available. Let\u2019s print a summary of the analysis. For example, we can print the top 40 key points in the dataset, along with the top 3 matching sentences for each key point. The total number of matches for each key point is also indicated."}, {"metadata": {"id": "5f270a18-b8c6-45dd-a84b-8a9da1bf50e0"}, "cell_type": "code", "source": "kps_result.print_result(n_sentences_per_kp = 3, title = \"Austin sample\", n_top_kps = 40)", "execution_count": null, "outputs": []}, {"metadata": {"id": "13adfef5-dcc4-4395-9a0d-94b2467f9d73"}, "cell_type": "markdown", "source": "We can also export the results into files, including summary and full analysis csv files, as well as a user-friendly Word Document report."}, {"metadata": {"id": "b269e8c9-0570-4362-b8db-bc694a3dcd42"}, "cell_type": "code", "source": "import os\noutput_dir = f'./kps_results/{domain}/'\nos.makedirs(output_dir, exist_ok = True)\nkps_result.export_to_all_outputs(output_dir=output_dir, result_name=domain)\n!ls -al {output_dir}", "execution_count": null, "outputs": []}, {"metadata": {"id": "eba54180-37f8-4287-8459-8037b2702679"}, "cell_type": "markdown", "source": "When we're done, we can stop the backend for a clean termination."}, {"metadata": {"id": "790e6216-7300-445b-b0ee-1d950c1f4df1", "scrolled": true}, "cell_type": "code", "source": "runner.stop_backend()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Authors:\n**Yoav Katz**\n**Roy Bar-Haim**\n**Yoav Kantor**\n**Lilach Edelstein**"}, {"metadata": {}, "cell_type": "markdown", "source": "Copyright \u00a9 2024 IBM. This notebook and its source code are released under the terms of the MIT License."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}
