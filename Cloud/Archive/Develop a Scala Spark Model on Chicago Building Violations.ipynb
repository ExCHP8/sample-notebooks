{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Develop a Scala Spark Model on Chicago Building Violations\n",
    "\n",
    "\n",
    "This notebook models building violations in Chicago.\n",
    "___________\n",
    "\n",
    "The data are <a href=\"https://data.cityofchicago.org/Buildings/Building-Violations/22u3-xenr\"  target=\"_blank\" rel=\"noopener noreferrer\">Violations issued by the Chicago Department of Buildings</a>\n",
    " over the period from 2006 until present. The dataset contains instances of `violations`. Each violation is associated with an `inspection` and an `inspection status`. \n",
    "\n",
    "Using Spark Machine Learning, we're going to develop a model for the data from 2006-2016 which provides a score in the interval $[0,1]$ for how likely we believe an individual building is to `Pass` or `Fail` an inspection. \n",
    "\n",
    "This notebook runs on Scala 2.11 with Spark.\n",
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Table of contents\n",
    "1. [Wrangle data](#wrangle)\n",
    "2. [Build a pipeline](#build)\n",
    "3. [Train the model](#train)\n",
    "4. [Save the model](#save)\n",
    "5. [Deploy the model](#deploy)\n",
    "6. [Summary](#summary)\n",
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"wrangle\"></a>\n",
    "## 1. Wrangle data \n",
    "\n",
    "Read the data into a Spark DataFrame. \n",
    "\n",
    "1. [Import the libraries](#libraries)\n",
    "2. [Load the credentials](#credentials)\n",
    "3. [Load the data set](#dataset)\n",
    "4. [Modify the data](#modify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.1 Import the libraries<a id=\"libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import top level\n",
    "import scala.sys.process._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.util._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.catalyst.expressions.DateFormatClass\n",
    "import com.ibm.ibmos2spark.bluemix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1.2 Load the credentials<a id='credentials'></a>\n",
    "We need our credentials to work with Cloud Object Storage (COS) to read the data into a Spark DataFrame. \n",
    "To load our credentials and the data set:\n",
    "1. Go to the <a href=\"https://data.cityofchicago.org/Buildings/Building-Violations/22u3-xenr\" target=\"_blank\" rel=\"noopener noreferrer\">Violations issued by the Chicago Department of Buildings</a>. \n",
    "2. Click export, then download and save the data set `Building_Violations` as a.csv file to your computer.  \n",
    "3. Load the .csv file into your notebook. Click the **Data** icon on the notebook action bar. Drop the file into the box or browse to select the file. The file is loaded to your COS and appears in the Data Assets section of the project. For more information, see <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/load-and-access-data.html\" target=\"_blank\" rel=\"noopener noreferrer\">Load and access data</a>.\n",
    "4. Load the credentials for the `Building_Violations.csv` file. Click in the next code cell and select **Insert to code > Insert credentials**.\n",
    "6. Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Insert to code > Insert credentials.\n",
    "// @hidden_cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load the data set<a id='dataset'></a>\n",
    "\n",
    "1. Load the data from the `Building_Violations.csv` file into a SparkSession DataFrame. Click in the next code cell and select **Insert to code > Insert SparkSession DataFrame** under the file name.\n",
    "2. Replace `dfData1` with `violations`.\n",
    "3. Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// @hidden_cell\n",
    "// SparkSession DataFrame goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Modify the data <a id='modify'></a>\n",
    "\n",
    "We’re not going to build a model on all of the data. We need to separate 2006–2016 from 2017. We will use a decade of data to train the model, and we will test the performance of our model on the 2017 data. \n",
    "\n",
    "In the above Schema, `VIOLATION DATE` is string type. This means we need to do some wrangling before we can filter by the dates in an intuitive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dated = [ID: int, VIOLATION LAST MODIFIED DATE: string ... 31 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ID: int, VIOLATION LAST MODIFIED DATE: string ... 31 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create datetime column\n",
    "val dated = violations.withColumn(\"timeStamp\", to_date(unix_timestamp(\n",
    "  $\"VIOLATION DATE\", \"MM/dd/yyyy\"\n",
    ").cast(\"timestamp\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s make some more modifications:\n",
    "\n",
    "- Rename all of the columns so that we can reference them more easily later \n",
    "- Remove the space between the names and replace it with an underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleanDf = [ID: int, VIOLATION_LAST_MODIFIED_DATE: string ... 31 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ID: int, VIOLATION_LAST_MODIFIED_DATE: string ... 31 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// sub whitespace for `_`\n",
    "var cleanDf = dated\n",
    "for(col <- dated.columns){\n",
    "    cleanDf = cleanDf.withColumnRenamed(col,col.replaceAll(\"\\\\s\", \"_\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re modeling `INSPECTION_STATUS`, but there are a small number of records where the status has not been resolved into `PASSED` or `FAILED`. Now we will:\n",
    "- Select only those records that meet our criteria with `SQL Transformer`\n",
    "- Change the datatype of ``LATITUDE`` and ``LONGITUDE`` from string to Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [ID: int, VIOLATION_LAST_MODIFIED_DATE: string ... 31 more fields]\n",
       "preppedFrame = [ID: int, VIOLATION_LAST_MODIFIED_DATE: string ... 31 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ID: int, VIOLATION_LAST_MODIFIED_DATE: string ... 31 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.SQLTransformer\n",
    "val df = new SQLTransformer().setStatement(\"SELECT * FROM __THIS__ WHERE INSPECTION_STATUS IN ('FAILED', 'PASSED')\").transform(cleanDf)\n",
    "val preppedFrame = df.withColumn(\"LATITUDE\", df(\"LATITUDE\").cast(DoubleType)).\n",
    "                    withColumn(\"LONGITUDE\", df(\"LONGITUDE\").cast(DoubleType))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, separate the data by year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingData2016 = [ID: int, VIOLATION_LAST_MODIFIED_DATE: string ... 31 more fields]\n",
       "testingData2017 = [ID: int, VIOLATION_LAST_MODIFIED_DATE: string ... 31 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ID: int, VIOLATION_LAST_MODIFIED_DATE: string ... 31 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Filter by date. Train on  year < 2017, test on 2017 data\n",
    "val trainingData2016 = preppedFrame.filter(year($\"timestamp\").leq(lit(2016))) \n",
    "val testingData2017 = preppedFrame.filter(year($\"timestamp\").gt(lit(2016))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** `leq` is `less-than-or-equal-to` . `gt` follows the same logic.\n",
    "\n",
    "Now, we’ve represented the DataFrame with a new field, `timeStamp`. We can use this to filter the timestamp data intuitively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|VIOLATION_DATE|\n",
      "+--------------+\n",
      "|    06/12/2019|\n",
      "|    06/12/2019|\n",
      "|    06/12/2019|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Take a peek\n",
    "testingData2017.select(\"VIOLATION_DATE\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|VIOLATION_DATE|\n",
      "+--------------+\n",
      "|    12/30/2016|\n",
      "|    12/30/2016|\n",
      "|    12/30/2016|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// at the training data too\n",
    "trainingData2016.select(\"VIOLATION_DATE\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we choose only a subset of the fields to use for modeling. \n",
    "Many of the other fields have missing values, which is beyond the scope of this notebook. \n",
    "\n",
    "- Specify a subset of the columns\n",
    "- Drop those rows which contain nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keepCols = Array(VIOLATION_CODE, VIOLATION_DESCRIPTION, INSPECTION_STATUS, INSPECTOR_ID, INSPECTION_CATEGORY, DEPARTMENT_BUREAU, LATITUDE, LONGITUDE)\n",
       "dfTrain = [VIOLATION_CODE: string, VIOLATION_DESCRIPTION: string ... 6 more fields]\n",
       "dfTest = [VIOLATION_CODE: string, VIOLATION_DESCRIPTION: string ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[VIOLATION_CODE: string, VIOLATION_DESCRIPTION: string ... 6 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val keepCols = Array(\"VIOLATION_CODE\", \"VIOLATION_DESCRIPTION\", \n",
    "                   \"INSPECTION_STATUS\", \"INSPECTOR_ID\", \n",
    "                   \"INSPECTION_CATEGORY\", \"DEPARTMENT_BUREAU\", \n",
    "                   \"LATITUDE\", \"LONGITUDE\")\n",
    "val dfTrain = trainingData2016.select(keepCols.head, keepCols.tail: _*).na.drop\n",
    "val dfTest = testingData2017.select(keepCols.head, keepCols.tail: _*).na.drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VIOLATION_CODE: string (nullable = true)\n",
      " |-- VIOLATION_DESCRIPTION: string (nullable = true)\n",
      " |-- INSPECTION_STATUS: string (nullable = true)\n",
      " |-- INSPECTOR_ID: string (nullable = true)\n",
      " |-- INSPECTION_CATEGORY: string (nullable = true)\n",
      " |-- DEPARTMENT_BUREAU: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTrain.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"build\"></a>\n",
    "## 2. Build a pipeline\n",
    "When you deploy a model to Watson Machine Learning, you need to provide a `Spark Machine Learning Pipeline`, which indicates how to transform raw data into the representation required by our model. \n",
    "Pipelines typically include a series of transformers and terminate with a model or, especially in classification tasks, some transformers which will convert model predictions into string labels.\n",
    "\n",
    "1. [Import transformers](#transformer)\n",
    "2. [Instantiate model object and pipeline](#instantiate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import transformers<a id=\"transformer\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* Import transformers to build a pipeline  */ \n",
    "import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorAssembler}\n",
    "import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF}\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use the `StringIndexer` to convert strings into a numeric representation for the machine. \n",
    "\n",
    "You can read about many transformations in the <a href=\"https://spark.apache.org/docs/2.0.2/ml-features.html\"  target=\"_blank\" rel=\"noopener noreferrer\">Spark documentation</a>. \n",
    "\n",
    "We assign each transformation a value because we’ll need to reference them later in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, notice that after creating a new instance of `StringIndexer` , we use `setInputCol` and `setOutputCol` . The output column goes into the `VectorAssembler`. We include all of those features we use for modeling in `VectorAssembler`.\n",
    "But what about string data that is not categorical? Sure, we can index all of the `INSPECTOR_ID` data, but does that make sense for the `VIOLATION_DESCRIPTION`, where almost every field is unique? \n",
    "<br>\n",
    "\n",
    "For text data like this, Scala and Spark provide other handy transformations. For `RegexTokenizer` and `HashingTF` the  idea is simple. The tokenizer takes the text and breaks it into individual words, called `tokens`. Map the tokens contained in each violation description to their frequencies. This allows us to accept unseen data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelCol = strIdx_8bf88cee6cfd\n",
       "interCodeCol = strIdx_b11fc7033811\n",
       "interSpector = strIdx_6f8f64a112ab\n",
       "interCatSpector = strIdx_6a5e0b76aa91\n",
       "interBureau = strIdx_1efc1c3bf7c4\n",
       "regexTokenizer = regexTok_a69f069bd9ac\n",
       "hashingTF = hashingTF_05c11333fe97\n",
       "vecAssembler = vecAssembler_db4f136b8425\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vecAssembler_db4f136b8425"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Label colum\n",
    "val labelCol = new StringIndexer().setInputCol(\"INSPECTION_STATUS\").setOutputCol(\"STATUS_LABEL\").fit(df)\n",
    "\n",
    "// Feature cols with String Indexer => Vector Assembler //\n",
    "\n",
    "//* VIOLATION CODE * //\n",
    "val interCodeCol = new StringIndexer().setInputCol(\"VIOLATION_CODE\").setOutputCol(\"CODE_X\").setHandleInvalid(\"skip\")\n",
    "\n",
    "\n",
    "//* INSPECTOR ID * //\n",
    "val interSpector = new StringIndexer().setInputCol(\"INSPECTOR_ID\").setOutputCol(\"INSP_X\").setHandleInvalid(\"skip\")\n",
    "\n",
    "\n",
    "//* INSPECTION CATEGORY * //\n",
    "val interCatSpector = new StringIndexer().setInputCol(\"INSPECTION_CATEGORY\").setOutputCol(\"INCAT_X\").setHandleInvalid(\"skip\")\n",
    "\n",
    "\n",
    "//* DEPARTMENT BUREAU * //\n",
    "val interBureau = new StringIndexer().setInputCol(\"DEPARTMENT_BUREAU\").setHandleInvalid(\"skip\").setOutputCol(\"BUR_X\")\n",
    "\n",
    "\n",
    "//** DEALING WITH TEXT **//\n",
    "val regexTokenizer = new RegexTokenizer().setInputCol(\"VIOLATION_DESCRIPTION\").setOutputCol(\"WORD_X\").setPattern(\"\\\\W\")\n",
    "val hashingTF = new HashingTF().setInputCol(\"WORD_X\").setOutputCol(\"DESCRIPTION\").setNumFeatures(150) // experiment with numFeatures + regularization params\n",
    "\n",
    "// LAT AND LONG ARE NUMERIC //\n",
    "\n",
    "\n",
    "//** VECTOR ASSEMBLER **//\n",
    "\n",
    "val vecAssembler = new VectorAssembler().setInputCols(Array(\"BUR_X\", \"INCAT_X\", \"CODE_X\", \"INSP_X\", \"DESCRIPTION\", \"LATITUDE\", \"LONGITUDE\")).setOutputCol(\"FEATURES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Instantiate the model object and pipeline<a id=\"instantiate\"></a>\n",
    "Time to instantiate a new untrained model object and pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logitModel = logreg_ab0b684fb5e6\n",
       "labelConverter = idxToStr_b4faaf46f318\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "idxToStr_b4faaf46f318"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Model, Pipeline, PipelineStage, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "//** Logistic Regression **//\n",
    "val logitModel = new LogisticRegression().setLabelCol(\"STATUS_LABEL\").setFeaturesCol(\"FEATURES\").setRegParam(0.1)\n",
    "\n",
    "\n",
    "//** Convert index prediction back to string **//\n",
    "val labelConverter = new IndexToString().setInputCol(\"prediction\").setOutputCol(\"PREDICTED_LABEL\").setLabels(labelCol.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the modeling pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logisticPipe = pipeline_2c6504b03396\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_2c6504b03396"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* Logitic Regression Pipeline */ \n",
    "val logisticPipe = new Pipeline().setStages(\n",
    "                                    Array(\n",
    "                                        labelCol, \n",
    "                                        interCodeCol, \n",
    "                                        interSpector, \n",
    "                                        interCatSpector,\n",
    "                                        interBureau,\n",
    "                                        regexTokenizer, hashingTF,\n",
    "                                        vecAssembler,\n",
    "                                        logitModel                                                                  \n",
    "                                    )\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## 3. Train the models\n",
    "Call `.fit()` on the pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainedLogit = pipeline_2c6504b03396\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_2c6504b03396"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainedLogit = logisticPipe.fit(dfTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make predictions and get metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionsLogisitc = [VIOLATION_CODE: string, VIOLATION_DESCRIPTION: string ... 17 more fields]\n",
       "predictionAndLabels = MapPartitionsRDD[746] at map at <console>:90\n",
       "metrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@33039935\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@33039935"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// predict\n",
    "val predictionsLogisitc = trainedLogit.transform(dfTest)\n",
    "\n",
    "\n",
    "// Prepare for metrics\n",
    "val predictionAndLabels = predictionsLogisitc.select(\"STATUS_LABEL\", \"prediction\").rdd.map(row => \n",
    "            (row.getAs[Double](\"prediction\"), row.getAs[Double](\"STATUS_LABEL\")))\n",
    "\n",
    "val metrics = new BinaryClassificationMetrics(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a new object `metrics`, which contains a lot of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6090319971611869"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// AUC\n",
    "metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An area of .5 under the <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\"  target=\"_blank\" rel=\"noopener noreferrer\">ROC Curve</a> indicates that the model performs as well as random guessing, so we’ve beaten that enough to continue for purposes of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "## 4. Save the model\n",
    "You’ll need an instance of [Watson Machine Learning](https://developer.ibm.com/clouddataservices/docs/ibm-watson-machine-learning/) to save your model. \n",
    "\n",
    "You can create a new instance directly from within Watson Studio, but you’ll need to log in to IBM Cloud for your credentials. IBM Cloud offers many powerful services and several are available at little to no cost, including Watson Machine Learning (WML). With WML, you have a repository to store and deploy your models. Consult <a href=\"https://developer.ibm.com/clouddataservices/docs/ibm-watson-machine-learning/get-started/\"  target=\"_blank\" rel=\"noopener noreferrer\">IBM Developer resources</a> to help you get started if you haven't already. You can also check the companion <a href=\"https://medium.com/p/91b580450c5b/edit\"  target=\"_blank\" rel=\"noopener noreferrer\">blog</a> for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the <a href=\"https://watson-ml-libs.mybluemix.net/repository-scalaV3/#com.ibm.analytics.ngp.repository_v3.package\"  target=\"_blank\" rel=\"noopener noreferrer\">IBM Scala Repository API Client for Watson Machine Learning</a> and other helpful libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.ibm.analytics.ngp.repository_v3._\n",
    "\n",
    "// Helper libraries\n",
    "\n",
    "import scalaj.http.{Http, HttpOptions}\n",
    "import scala.util.{Success, Failure}\n",
    "import java.util.Base64\n",
    "import java.nio.charset.StandardCharsets\n",
    "import play.api.libs.json._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fetch your credentials. The WML credentials enable you to communicate with your repository via the internet. \n",
    "Learn more about <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-setup.html?audience=wdp&context=wdp&linkInPage=true\"  target=\"_blank\" rel=\"noopener noreferrer\">retrieving service credentials</a> in the Watson Studio documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.collection.mutable.HashMap\n",
    "val wmlCredentials: HashMap[String, String] = HashMap(\n",
    "    \"url\"->\"***\",\n",
    "    \"username\"->\"***\",\n",
    "    \"password\"->\"***\",\n",
    "    \"instance_id\"->\"***\",\n",
    "    \"apikey\"->\"***\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val service_path = wmlCredentials(\"url\")\n",
    "val username = wmlCredentials(\"username\")\n",
    "val password = wmlCredentials(\"password\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s make a connection and authorize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Authorize\n",
    "val client = MLRepositoryClient(service_path)\n",
    "client.authorize(username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use `MLRepositoryArtifact` to create a model artifact for the repository. We must pass:\n",
    "- A Spark ML pipeline: `trainedLogit`\n",
    "- The training data used: `dfTrain`\n",
    "- A name for the model:`VIOLATIONS_SCALA211_SPARK20`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// model artifact\n",
    "val model_artifact = MLRepositoryArtifact(trainedLogit, dfTrain, \"VIOLATIONS_SCALA211_SPARK20\")\n",
    "val saved = client.models.save(model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`saved` is the model artifact. Check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelType: Some(standard)\n",
      "trainingDataSchema: Some({\"type\":\"struct\",\"fields\":[{\"name\":\"VIOLATION_CODE\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"VIOLATION_DESCRIPTION\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"INSPECTION_STATUS\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"modeling_role\":\"target\"}},{\"name\":\"INSPECTOR_ID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"INSPECTION_CATEGORY\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DEPARTMENT_BUREAU\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"LATITUDE\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"LONGITUDE\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]})\n",
      "creationTime: Some(2019-06-14T15:01:17.729Z)\n",
      "modelVersionHref: None\n",
      "label: Some(INSPECTION_STATUS)\n",
      "runtime: Some(spark-2.3)\n"
     ]
    }
   ],
   "source": [
    "println(\"modelType: \" + saved.get.meta.prop(\"modelType\"))\n",
    "println(\"trainingDataSchema: \" + saved.get.meta.prop(\"trainingDataSchema\"))\n",
    "println(\"creationTime: \" + saved.get.meta.prop(\"creationTime\"))\n",
    "println(\"modelVersionHref: \" + saved.get.meta.prop(\"modelVersionHref\"))\n",
    "println(\"label: \" + saved.get.meta.prop(\"label\"))\n",
    "println(\"runtime: \"+ saved.get.meta.prop(\"runtime\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deploy\"></a>\n",
    "## 5. Deploy the model from within your Watson Studio project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve saved the model, we can deploy and create an API endpoint to score records. \n",
    "\n",
    "\n",
    "1. Go to **Assets** in your Project, then find your model under **Models** and click on it. We named ours `VIOLATIONS_SCALA211_SPARK20`.\n",
    "2. Select **Deployment > Add Deployment**.\n",
    "3. Enter a name and description for the deployment, click **Save**.\n",
    "4. Select the model, click **Test** to test the API. \n",
    "\n",
    "See the Watson Studio <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-deploy_new.html\" target=\"_blank\" rel=\"noopener noreferrer\">documentation</a> for more information about how to deploy a model within a project.\n",
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary<a id=\"summary\"></a>\n",
    "That's awesome! We built a model with Scala and Spark on Watson Studio Cloud. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "\n",
    "City of Chicago (2017). Building Violations <a href=https://data.cityofchicago.org/Buildings/Building-Violations/22u3-xenr target=\"_blank\" rel=\"noopener noreferrer\">https://data.cityofchicago.org/Buildings/Building-Violations/22u3-xenr</a>  Chicago, IL: Chicago City Data Portal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author\n",
    "**Adam Massachi** is a Data Scientist with the Watson Studio and IBM Watson teams at IBM. Before IBM, he worked on political campaigns, building and managing large volunteer operations and organizing campaign finance initiatives. Say hello <a href=\"https://twitter.com/adammassach?lang=en\"  target=\"_blank\" rel=\"noopener noreferrer\">@adammassach</a>!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corp. 2018, 2019. This notebook and its source code are released under the terms of the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n",
    "<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n",
    "<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n",
    "<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11 with Spark",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
