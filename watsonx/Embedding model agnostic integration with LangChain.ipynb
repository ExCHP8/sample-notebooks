{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Embedding model agnostic integration with LangChain\n\nThis notebook teaches you how to:\n\n1. Apply a RAG framework by connecting a vector database to one of the watsonx foundation models and utility functions from the Watson Machine Learning service within watsonx.ai and Langchain,\n2. Build up a knowledge base,\n3. Create an embedding function with an embedding model to generate a Q&A resource for users\n"}, {"metadata": {}, "cell_type": "markdown", "source": "# Set up the environment \n\n## Install and import the dependencies "}, {"metadata": {}, "cell_type": "code", "source": "!pip install \"langchain==0.1.10\" | tail -n 1\n!pip install langchain_ibm | tail -n 1\n!pip install wget | tail -n 1\n!pip install sentence-transformers | tail -n 1\n!pip install \"chromadb==0.3.26\" | tail -n 1\n!pip install \"pydantic==1.10.0\" | tail -n 1\n!pip install \"sqlalchemy==2.0.1\" | tail -n 1\n!pip install \"ibm-watsonx-ai>=0.2.5\" | tail -n 1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Install embedding model's Python library\n\nYou must choose your own embedding model and update \"EMBEDDING MODEL\". See here https://python.langchain.com/v0.1/docs/integrations/text_embedding/ to find the correct parameters to load the class for your chosen embeddings model. "}, {"metadata": {}, "cell_type": "code", "source": "!pip install langchain-\"EMBEDDING MODEL\" | tail -n 1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Foundation models on watsonx.ai\n\n## Import IBM Foundation Models and utility functions from WML and Langchain, build up knowledge base, import vector database"}, {"metadata": {}, "cell_type": "code", "source": "from langchain.vectorstores import Chroma\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\n\nfrom langchain.chains import RetrievalQA\n\nimport os \n\nimport getpass\n\nimport wget", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# watsonx API connection (insert WML api key)\n\nThis cell defines the credentials required to work with watsonx API for Foundation Model inferencing.\n\nAction: Provide the IBM Cloud user API key. For details, see documentation: https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\n\nThe API requires project id that provides the context for the call. We will obtain the id from the project in which this notebook runs. Otherwise, please provide the project id."}, {"metadata": {}, "cell_type": "code", "source": "credentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n    \"apikey\": getpass.getpass(\"Please enter your WML api key (hit enter): \")\n}\n\n\ntry:\n    project_id = os.environ[\"PROJECT_ID\"] \nexcept KeyError:\n    project_id = input(\"Please enter your project_id (hit enter): \")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Define model, define model parameteres, LangChain CustomLLM wrapper for watsonx model"}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n\nmodel_id = ModelTypes.GRANITE_13B_CHAT_V2\n\n### Define model behavior parameters\n\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n\nparameters = {\n    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n    GenParams.MIN_NEW_TOKENS: 1,\n    GenParams.MAX_NEW_TOKENS: 100,\n    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\"]\n}\n\n### Create an instance of your watsonx LLM\nfrom langchain_ibm import WatsonxLLM\n\nwatsonx_granite = WatsonxLLM(\n    model_id=model_id.value,\n    url=credentials.get(\"url\"),\n    apikey=credentials.get(\"apikey\"),\n    project_id=project_id,\n    params=parameters\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Document data loading"}, {"metadata": {}, "cell_type": "code", "source": "filename = 'state_of_the_union.txt'\nurl = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\n\nif not os.path.isfile(filename):\n    wget.download(url, out=filename)\n    \nloader = TextLoader(filename)\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ntexts = text_splitter.split_documents(documents)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Import embedding model \n\nYou must choose your own embedding model and update \"EMBEDDING MODEL\". Insert API key for \"EMBEDDING MODEL\" if one is required. See https://python.langchain.com/v0.1/docs/integrations/text_embedding/ for specific parameters. "}, {"metadata": {}, "cell_type": "code", "source": "from langchain_\"EMBEDDING PROVIDER NAME\" import \"EMBEDDING MODEL\"\nos.environ[\"EMBEDDING MODEL_API_KEY\"] = \"API KEY\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Connecting to embedding model from Langchain and vector database\n\nCheck Langchain documentation for each embedding model (https://python.langchain.com/v0.1/docs/integrations/text_embedding/) for the specific formatting. Provide API key and model name if required. "}, {"metadata": {}, "cell_type": "code", "source": "embeddings = \"EMBEDDING MODEL\"(\n    \"EMBEDDING MODEL API KEY\"=\"API KEY\", model=\"MODEL NAME\"\n)\n\ndocsearch = Chroma.from_documents(texts, embeddings)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Generate a retrieval-augmented response to a question\n\n## Create RAG chain"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "qa = RetrievalQA.from_chain_type(llm=watsonx_granite, chain_type=\"stuff\", retriever=docsearch.as_retriever())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Select questions"}, {"metadata": {}, "cell_type": "code", "source": "query = \"What did the president say about Ketanji Brown Jackson\"\nqa.invoke(query)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Summary and next steps\n\nYou successfully completed this notebook!.\n\nYou learned how Create a generalized embedding function with an embedding model\nto generate a Q&A resource for users on watsonx.\n\nCheck out our Online Documentation for more samples, tutorials, documentation, how-tos, and\nblog posts.\n\nAuthors:\nJennifer Son, Nikki Brown, Technology Business Development at IBM watsonx.\n\nCopyright \u00a9 2024 IBM. This notebook and its source code are released under the terms of the\nMIT License.\n\n\n"}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}